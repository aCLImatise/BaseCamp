!Command
command:
- snakemake
positional:
- !Positional
  optional: false
  position: 0
  name: target
  description: "Targets to build. May be rules or files. (default:\nNone)"
named:
- !Flag
  optional: true
  synonyms:
  - --dry-run
  - --dryrun
  - -n
  description: "Do not execute anything, and display what would be\ndone. If you have\
    \ a very large workflow, use --dry-run\n--quiet to just print a summary of the\
    \ DAG of jobs.\n(default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --profile
  description: "Name of profile to use for configuring Snakemake.\nSnakemake will\
    \ search for a corresponding folder in\n/etc/xdg/snakemake and /root/.config/snakemake.\n\
    Alternatively, this can be an absolute or relative\npath. The profile folder has\
    \ to contain a file\n'config.yaml'. This file can be used to set default\nvalues\
    \ for command line options in YAML format. For\nexample, '--cluster qsub' becomes\
    \ 'cluster: qsub' in\nthe YAML file. Profiles can be obtained from\nhttps://github.com/snakemake-profiles.\
    \ (default: None)"
  args: !SimpleFlagArg
    name: PROFILE
- !Flag
  optional: true
  synonyms:
  - --cache
  description: "[RULE ...]    Store output files of given rules in a central cache\n\
    given by the environment variable\n$SNAKEMAKE_OUTPUT_CACHE. Likewise, retrieve\
    \ output\nfiles of the given rules from this cache if they have\nbeen created\
    \ before (by anybody writing to the same\ncache), instead of actually executing\
    \ the rules.\nOutput files are identified by hashing all steps,\nparameters and\
    \ software stack (conda envs or\ncontainers) needed to create them. (default:\
    \ None)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --snakefile
  - -s
  description: "The workflow definition in form of a\nsnakefile.Usually, you should\
    \ not need to specify\nthis. By default, Snakemake will search for\n'Snakefile',\
    \ 'snakefile', 'workflow/Snakefile',\n'workflow/snakefile' beneath the current\
    \ working\ndirectory, in this order. Only if you definitely want\na different\
    \ layout, you need to use this parameter.\n(default: None)"
  args: !SimpleFlagArg
    name: FILE
- !Flag
  optional: true
  synonyms:
  - --cores
  description: "[N], --jobs [N], -j [N]\nUse at most N CPU cores/jobs in parallel.\
    \ If N is\nomitted or 'all', the limit is set to the number of\navailable CPU\
    \ cores. (default: None)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --local-cores
  description: "In cluster mode, use at most N cores of the host\nmachine in parallel\
    \ (default: number of CPU cores of\nthe host). The cores are used to execute local\
    \ rules.\nThis option is ignored when not in cluster mode.\n(default: 2)"
  args: !SimpleFlagArg
    name: N
- !Flag
  optional: true
  synonyms:
  - --resources
  description: "[NAME=INT ...], --res [NAME=INT ...]\nDefine additional resources\
    \ that shall constrain the\nscheduling analogously to threads (see above). A\n\
    resource is defined as a name and an integer value.\nE.g. --resources mem_mb=1000.\
    \ Rules can use resources\nby defining the resource keyword, e.g. resources:\n\
    mem_mb=600. If now two rules require 600 of the\nresource 'mem_mb' they won't\
    \ be run in parallel by the\nscheduler. (default: None)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --set-threads
  description: "=THREADS [RULE=THREADS ...]\nOverwrite thread usage of rules. This\
    \ allows to fine-\ntune workflow parallelization. In particular, this is\nhelpful\
    \ to target certain cluster nodes by e.g.\nshifting a rule to use more, or less\
    \ threads than\ndefined in the workflow. Thereby, THREADS has to be a\npositive\
    \ integer, and RULE has to be the name of the\nrule. (default: None)"
  args: !SimpleFlagArg
    name: RULE
- !Flag
  optional: true
  synonyms:
  - --set-scatter
  description: "=SCATTERITEMS [NAME=SCATTERITEMS ...]\nOverwrite number of scatter\
    \ items of scattergather\nprocesses. This allows to fine-tune workflow\nparallelization.\
    \ Thereby, SCATTERITEMS has to be a\npositive integer, and NAME has to be the\
    \ name of the\nscattergather process defined via a scattergather\ndirective in\
    \ the workflow. (default: None)"
  args: !SimpleFlagArg
    name: NAME
- !Flag
  optional: true
  synonyms:
  - --default-resources
  description: "[NAME=INT ...], --default-res [NAME=INT ...]\nDefine default values\
    \ of resources for rules that do\nnot define their own values. In addition to\
    \ plain\nintegers, python expressions over inputsize are\nallowed (e.g. '2*input.size_mb').When\
    \ specifying this\nwithout any arguments (--default-resources), it\ndefines 'mem_mb=max(2*input.size_mb,\
    \ 1000)'\n'disk_mb=max(2*input.size_mb, 1000)', i.e., default\ndisk and mem usage\
    \ is twice the input file size but at\nleast 1GB. (default: None)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --preemption-default
  description: "A preemptible instance can be requested when using the\nGoogle Life\
    \ Sciences API. If you set a --preemption-\ndefault,all rules will be subject\
    \ to the default.\nSpecifically, this integer is the number of restart\nattempts\
    \ that will be made given that the instance is\nkilled unexpectedly. Note that\
    \ preemptible instances\nhave a maximum running time of 24 hours. If you want\n\
    to set preemptible instances for only a subset of\nrules, use --preemptible-rules\
    \ instead. (default:\nNone)"
  args: !SimpleFlagArg
    name: PREEMPTION_DEFAULT
- !Flag
  optional: true
  synonyms:
  - --preemptible-rules
  description: "A preemptible instance can be requested when using the\nGoogle Life\
    \ Sciences API. If you want to use these\ninstances for a subset of your rules,\
    \ you can use\n--preemptible-rules and then specify a list of rule\nand integer\
    \ pairs, where each integer indicates the\nnumber of restarts to use for the rule's\
    \ instance in\nthe case that the instance is terminated unexpectedly.\n--preemptible-rules\
    \ can be used in combination with\n--preemption-default, and will take priority.\
    \ Note\nthat preemptible instances have a maximum running time\nof 24. If you\
    \ want to apply a consistent number of\nretries across all your rules, use --premption-default\n\
    instead. Example: snakemake --preemption-default 10\n--preemptible-rules map_reads=3\
    \ call_variants=0\n(default: None)"
  args: !RepeatFlagArg
    name: PREEMPTIBLE_RULES
- !Flag
  optional: true
  synonyms:
  - --config
  description: "[KEY=VALUE ...], -C [KEY=VALUE ...]\nSet or overwrite values in the\
    \ workflow config object.\nThe workflow config object is accessible as variable\n\
    config inside the workflow. Default values can be set\nby providing a JSON file\
    \ (see Documentation).\n(default: None)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --configfile
  - --configfiles
  description: "Specify or overwrite the config file of the workflow\n(see the docs).\
    \ Values specified in JSON or YAML\nformat are available in the global config\
    \ dictionary\ninside the workflow. Multiple files overwrite each\nother in the\
    \ given order. (default: None)"
  args: !RepeatFlagArg
    name: FILE
- !Flag
  optional: true
  synonyms:
  - --envvars
  description: "Environment variables to pass to cloud jobs. (default:\nNone)"
  args: !RepeatFlagArg
    name: VARNAME
- !Flag
  optional: true
  synonyms:
  - --directory
  - -d
  description: "Specify working directory (relative paths in the\nsnakefile will use\
    \ this as their origin). (default:\nNone)"
  args: !SimpleFlagArg
    name: DIR
- !Flag
  optional: true
  synonyms:
  - --touch
  - -t
  description: "Touch output files (mark them up to date without\nreally changing\
    \ them) instead of running their\ncommands. This is used to pretend that the rules\
    \ were\nexecuted, in order to fool future invocations of\nsnakemake. Fails if\
    \ a file does not yet exist. Note\nthat this will only touch files that would\
    \ otherwise\nbe recreated by Snakemake (e.g. because their input\nfiles are newer).\
    \ For enforcing a touch, combine this\nwith --force, --forceall, or --forcerun.\
    \ Note however\nthat you loose the provenance information when the\nfiles have\
    \ been created in realitiy. Hence, this\nshould be used only as a last resort.\
    \ (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --keep-going
  - -k
  description: "Go on with independent jobs if a job fails. (default:\nFalse)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --force
  - -f
  description: "Force the execution of the selected target or the\nfirst rule regardless\
    \ of already created output.\n(default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --forceall
  - -F
  description: "Force the execution of the selected (or the first)\nrule and all rules\
    \ it is dependent on regardless of\nalready created output. (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --forcerun
  description: "[TARGET ...], -R [TARGET ...]\nForce the re-execution or creation\
    \ of the given rules\nor files. Use this option if you changed a rule and\nwant\
    \ to have all its output in your workflow updated.\n(default: None)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --prioritize
  - -P
  description: "Tell the scheduler to assign creation of given targets\n(and all their\
    \ dependencies) highest priority.\n(EXPERIMENTAL) (default: None)"
  args: !RepeatFlagArg
    name: TARGET
- !Flag
  optional: true
  synonyms:
  - --batch
  description: "=BATCH/BATCHES\nOnly create the given BATCH of the input files of\
    \ the\ngiven RULE. This can be used to iteratively run parts\nof very large workflows.\
    \ Only the execution plan of\nthe relevant part of the workflow has to be\ncalculated,\
    \ thereby speeding up DAG computation. It is\nrecommended to provide the most\
    \ suitable rule for\nbatching when documenting a workflow. It should be\nsome\
    \ aggregating rule that would be executed only\nonce, and has a large number of\
    \ input files. For\nexample, it can be a rule that aggregates over\nsamples. (default:\
    \ None)"
  args: !SimpleFlagArg
    name: RULE
- !Flag
  optional: true
  synonyms:
  - --until
  - -U
  description: "Runs the pipeline until it reaches the specified rules\nor files.\
    \ Only runs jobs that are dependencies of the\nspecified rule or files, does not\
    \ run sibling DAGs.\n(default: None)"
  args: !RepeatFlagArg
    name: TARGET
- !Flag
  optional: true
  synonyms:
  - --omit-from
  - -O
  description: "Prevent the execution or creation of the given rules\nor files as\
    \ well as any rules or files that are\ndownstream of these targets in the DAG.\
    \ Also runs jobs\nin sibling DAGs that are independent of the rules or\nfiles\
    \ specified here. (default: None)"
  args: !RepeatFlagArg
    name: TARGET
- !Flag
  optional: true
  synonyms:
  - --rerun-incomplete
  - --ri
  description: "Re-run all jobs the output of which is recognized as\nincomplete.\
    \ (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --shadow-prefix
  description: "Specify a directory in which the 'shadow' directory is\ncreated. If\
    \ not supplied, the value is set to the\n'.snakemake' directory relative to the\
    \ working\ndirectory. (default: None)"
  args: !SimpleFlagArg
    name: DIR
- !Flag
  optional: true
  synonyms:
  - --scheduler
  description: "[{ilp,greedy}]\nSpecifies if jobs are selected by a greedy algorithm\n\
    or by solving an ilp. The ilp scheduler aims to reduce\nruntime and hdd usage\
    \ by best possible use of\nresources. (default: ilp)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --wms-monitor
  description: "[WMS_MONITOR]\nIP and port of workflow management system to monitor\n\
    the execution of snakemake (e.g.\nhttp://127.0.0.1:5000) Note that if your service\n\
    requires an authorization token, you must export\nWMS_MONITOR_TOKEN in the environment.\
    \ (default: None)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --wms-monitor-arg
  description: "[NAME=VALUE ...]\nIf the workflow management service accepts extra\n\
    arguments, provide. them in key value pairs with\n--wms-monitor-arg. For example,\
    \ to run an existing\nworkflow using a wms monitor, you can provide the pair\n\
    id=12345 and the arguments will be provided to the\nendpoint to first interact\
    \ with the workflow (default:\nNone)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --scheduler-ilp-solver
  description: "Specifies solver to be utilized when selecting ilp-\nscheduler. (default:\
    \ COIN_CMD)"
  args: !ChoiceFlagArg
    choices: !!set
      COIN_CMD:
- !Flag
  optional: true
  synonyms:
  - --no-subworkflows
  - --nosw
  description: "Do not evaluate or execute subworkflows. (default:\nFalse)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --groups
  description: "Assign rules to groups (this overwrites any group\ndefinitions from\
    \ the workflow). (default: None)"
  args: !RepeatFlagArg
    name: GROUPS
- !Flag
  optional: true
  synonyms:
  - --group-components
  description: "Set the number of connected components a group is\nallowed to span.\
    \ By default, this is 1, but this flag\nallows to extend this. This can be used\
    \ to run e.g. 3\njobs of the same rule in the same group, although they\nare not\
    \ connected. It can be helpful for putting\ntogether many small jobs or benefitting\
    \ of shared\nmemory setups. (default: None)"
  args: !RepeatFlagArg
    name: GROUP_COMPONENTS
- !Flag
  optional: true
  synonyms:
  - --report
  description: "[FILE]       Create an HTML report with results and statistics.\n\
    This can be either a .html file or a .zip file. In the\nformer case, all results\
    \ are embedded into the .html\n(this only works for small data). In the latter\
    \ case,\nresults are stored along with a file report.html in\nthe zip archive.\
    \ If no filename is given, an embedded\nreport.html is the default. (default:\
    \ None)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --report-stylesheet
  description: "Custom stylesheet to use for report. In particular,\nthis can be used\
    \ for branding the report with e.g. a\ncustom logo, see docs. (default: None)"
  args: !SimpleFlagArg
    name: CSSFILE
- !Flag
  optional: true
  synonyms:
  - --edit-notebook
  description: "Interactively edit the notebook associated with the\nrule used to\
    \ generate the given target file. This will\nstart a local jupyter notebook server.\
    \ Any changes to\nthe notebook should be saved, and the server has to be\nstopped\
    \ by closing the notebook and hitting the 'Quit'\nbutton on the jupyter dashboard.\
    \ Afterwards, the\nupdated notebook will be automatically stored in the\npath\
    \ defined in the rule. If the notebook is not yet\npresent, this will create an\
    \ empty draft. (default:\nNone)"
  args: !SimpleFlagArg
    name: TARGET
- !Flag
  optional: true
  synonyms:
  - --notebook-listen
  description: ":PORT\nThe IP address and PORT the notebook server used for\nediting\
    \ the notebook (--edit-notebook) will listen on.\n(default: localhost:8888)"
  args: !SimpleFlagArg
    name: IP
- !Flag
  optional: true
  synonyms:
  - --lint
  description: "[{text,json}]  Perform linting on the given workflow. This will print\n\
    snakemake specific suggestions to improve code quality\n(work in progress, more\
    \ lints to be added in the\nfuture). If no argument is provided, plain text output\n\
    is used. (default: None)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --generate-unit-tests
  description: "[TESTPATH]\nAutomatically generate unit tests for each workflow\n\
    rule. This assumes that all input files of each job\nare already present. Rules\
    \ without a job with present\ninput files will be skipped (a warning will be\n\
    issued). For each rule, one test case will be created\nin the specified test folder\
    \ (.tests/unit by default).\nAfter successfull execution, tests can be run with\n\
    'pytest TESTPATH'. (default: None)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --containerize
  description: "Print a Dockerfile that provides an execution\nenvironment for the\
    \ workflow, including all conda\nenvironments. (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --export-cwl
  description: "Compile workflow to CWL and store it in given FILE.\n(default: None)"
  args: !SimpleFlagArg
    name: FILE
- !Flag
  optional: true
  synonyms:
  - --list
  - -l
  description: "Show available rules in given Snakefile. (default:\nFalse)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --list-target-rules
  - --lt
  description: "Show available target rules in given Snakefile.\n(default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --dag
  description: "Do not execute anything and print the directed acyclic\ngraph of jobs\
    \ in the dot language. Recommended use on\nUnix systems: snakemake --dag | dot\
    \ | displayNote\nprint statements in your Snakefile may interfere with\nvisualization.\
    \ (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --rulegraph
  description: "Do not execute anything and print the dependency graph\nof rules in\
    \ the dot language. This will be less\ncrowded than above DAG of jobs, but also\
    \ show less\ninformation. Note that each rule is displayed once,\nhence the displayed\
    \ graph will be cyclic if a rule\nappears in several steps of the workflow. Use\
    \ this if\nabove option leads to a DAG that is too large.\nRecommended use on\
    \ Unix systems: snakemake --rulegraph\n| dot | displayNote print statements in\
    \ your Snakefile\nmay interfere with visualization. (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --filegraph
  description: "Do not execute anything and print the dependency graph\nof rules with\
    \ their input and output files in the dot\nlanguage. This is an intermediate solution\
    \ between\nabove DAG of jobs and the rule graph. Note that each\nrule is displayed\
    \ once, hence the displayed graph will\nbe cyclic if a rule appears in several\
    \ steps of the\nworkflow. Use this if above option leads to a DAG that\nis too\
    \ large. Recommended use on Unix systems:\nsnakemake --filegraph | dot | displayNote\
    \ print\nstatements in your Snakefile may interfere with\nvisualization. (default:\
    \ False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --d3dag
  description: "Print the DAG in D3.js compatible JSON format.\n(default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --summary
  - -S
  description: "Print a summary of all files created by the workflow.\nThe has the\
    \ following columns: filename, modification\ntime, rule version, status, plan.\
    \ Thereby rule version\ncontains the versionthe file was created with (see the\n\
    version keyword of rules), and status denotes whether\nthe file is missing, its\
    \ input files are newer or if\nversion or implementation of the rule changed since\n\
    file creation. Finally the last column denotes whether\nthe file will be updated\
    \ or created during the next\nworkflow execution. (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --detailed-summary
  - -D
  description: "Print a summary of all files created by the workflow.\nThe has the\
    \ following columns: filename, modification\ntime, rule version, input file(s),\
    \ shell command,\nstatus, plan. Thereby rule version contains the\nversion the\
    \ file was created with (see the version\nkeyword of rules), and status denotes\
    \ whether the file\nis missing, its input files are newer or if version or\nimplementation\
    \ of the rule changed since file\ncreation. The input file and shell command columns\
    \ are\nself explanatory. Finally the last column denotes\nwhether the file will\
    \ be updated or created during the\nnext workflow execution. (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --archive
  description: "Archive the workflow into the given tar archive FILE.\nThe archive\
    \ will be created such that the workflow can\nbe re-executed on a vanilla system.\
    \ The function needs\nconda and git to be installed. It will archive every\nfile\
    \ that is under git version control. Note that it\nis best practice to have the\
    \ Snakefile, config files,\nand scripts under version control. Hence, they will\
    \ be\nincluded in the archive. Further, it will add input\nfiles that are not\
    \ generated by by the workflow itself\nand conda environments. Note that symlinks\
    \ are\ndereferenced. Supported formats are .tar, .tar.gz,\n.tar.bz2 and .tar.xz.\
    \ (default: None)"
  args: !SimpleFlagArg
    name: FILE
- !Flag
  optional: true
  synonyms:
  - --cleanup-metadata
  - --cm
  description: "Cleanup the metadata of given files. That means that\nsnakemake removes\
    \ any tracked version info, and any\nmarks that files are incomplete. (default:\
    \ None)"
  args: !RepeatFlagArg
    name: FILE
- !Flag
  optional: true
  synonyms:
  - --cleanup-shadow
  description: "Cleanup old shadow directories which have not been\ndeleted due to\
    \ failures or power loss. (default:\nFalse)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --skip-script-cleanup
  description: "Don't delete wrapper scripts used for execution\n(default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --unlock
  description: "Remove a lock on the working directory. (default:\nFalse)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --list-version-changes
  - --lv
  description: "List all output files that have been created with a\ndifferent version\
    \ (as determined by the version\nkeyword). (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --list-code-changes
  - --lc
  description: "List all output files for which the rule body (run or\nshell) have\
    \ changed in the Snakefile. (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --list-input-changes
  - --li
  description: "List all output files for which the defined input\nfiles have changed\
    \ in the Snakefile (e.g. new input\nfiles were added in the rule definition or\
    \ files were\nrenamed). For listing input file modification in the\nfilesystem,\
    \ use --summary. (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --list-params-changes
  - --lp
  description: "List all output files for which the defined params\nhave changed in\
    \ the Snakefile. (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --list-untracked
  - --lu
  description: "List all files in the working directory that are not\nused in the\
    \ workflow. This can be used e.g. for\nidentifying leftover files. Hidden files\
    \ and\ndirectories are ignored. (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --delete-all-output
  description: "Remove all files generated by the workflow. Use\ntogether with --dry-run\
    \ to list files without actually\ndeleting anything. Note that this will not recurse\n\
    into subworkflows. Write-protected files are not\nremoved. Nevertheless, use with\
    \ care! (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --delete-temp-output
  description: "Remove all temporary files generated by the workflow.\nUse together\
    \ with --dry-run to list files without\nactually deleting anything. Note that\
    \ this will not\nrecurse into subworkflows. (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --bash-completion
  description: "Output code to register bash completion for snakemake.\nPut the following\
    \ in your .bashrc (including the\naccents): `snakemake --bash-completion` or issue\
    \ it in\nan open terminal session. (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --keep-incomplete
  description: "Do not remove incomplete output files by failed jobs.\n(default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --drop-metadata
  description: "Drop metadata file tracking information after job\nfinishes. Provenance-information\
    \ based reports (e.g.\n--report and the --list_x_changes functions) will be\n\
    empty or incomplete. (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --reason
  - -r
  description: "Print the reason for each executed rule. (default:\nFalse)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --gui
  description: "[PORT]          Serve an HTML based user interface to the given\n\
    network and port e.g. 168.129.10.15:8000. By default\nSnakemake is only available\
    \ in the local network\n(default port: 8000). To make Snakemake listen to all\n\
    ip addresses add the special host address 0.0.0.0 to\nthe url (0.0.0.0:8000).\
    \ This is important if Snakemake\nis used in a virtualised environment like Docker.\
    \ If\npossible, a browser window is opened. (default: None)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --printshellcmds
  - -p
  description: "Print out the shell commands that will be executed.\n(default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --debug-dag
  description: "Print candidate and selected jobs (including their\nwildcards) while\
    \ inferring DAG. This can help to debug\nunexpected DAG topology or errors. (default:\
    \ False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --stats
  description: "Write stats about Snakefile execution in JSON format\nto the given\
    \ file. (default: None)"
  args: !SimpleFlagArg
    name: FILE
- !Flag
  optional: true
  synonyms:
  - --nocolor
  description: 'Do not use a colored output. (default: False)'
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --quiet
  - -q
  description: "Do not output any progress or rule information.\n(default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --print-compilation
  description: "Print the python representation of the workflow.\n(default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --verbose
  description: 'Print debugging output. (default: False)'
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --force-use-threads
  description: "Force threads rather than processes. Helpful if shared\nmemory (/dev/shm)\
    \ is full or unavailable. (default:\nFalse)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --allow-ambiguity
  - -a
  description: "Don't check for ambiguous rules and simply use the\nfirst if several\
    \ can produce the same file. This\nallows the user to prioritize rules by their\
    \ order in\nthe snakefile. (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --nolock
  description: 'Do not lock the working directory (default: False)'
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --ignore-incomplete
  - --ii
  description: "Do not check for incomplete output files. (default:\nFalse)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --max-inventory-time
  description: "Spend at most SECONDS seconds to create a file\ninventory for the\
    \ working directory. The inventory\nvastly speeds up file modification and existence\n\
    checks when computing which jobs need to be executed.\nHowever, creating the inventory\
    \ itself can be slow,\ne.g. on network file systems. Hence, we do not spend\n\
    more than a given amount of time and fall back to\nindividual checks for the rest.\
    \ (default: 20)"
  args: !SimpleFlagArg
    name: SECONDS
- !Flag
  optional: true
  synonyms:
  - --latency-wait
  - --output-wait
  - -w
  description: "Wait given seconds if an output file of a job is not\npresent after\
    \ the job finished. This helps if your\nfilesystem suffers from latency (default\
    \ 5). (default:\n5)"
  args: !SimpleFlagArg
    name: SECONDS
- !Flag
  optional: true
  synonyms:
  - --wait-for-files
  description: "[FILE ...]\nWait --latency-wait seconds for these files to be\npresent\
    \ before executing the workflow. This option is\nused internally to handle filesystem\
    \ latency in\ncluster environments. (default: None)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --notemp
  - --nt
  description: "Ignore temp() declarations. This is useful when\nrunning only a part\
    \ of the workflow, since temp()\nwould lead to deletion of probably needed files\
    \ by\nother parts of the workflow. (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --keep-remote
  description: "Keep local copies of remote input files. (default:\nFalse)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --keep-target-files
  description: "Do not adjust the paths of given target files relative\nto the working\
    \ directory. (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --allowed-rules
  description: "Only consider given rules. If omitted, all rules in\nSnakefile are\
    \ used. Note that this is intended\nprimarily for internal use and may lead to\
    \ unexpected\nresults otherwise. (default: None)"
  args: !RepeatFlagArg
    name: ALLOWED_RULES
- !Flag
  optional: true
  synonyms:
  - --max-jobs-per-second
  description: "Maximal number of cluster/drmaa jobs per second,\ndefault is 10, fractions\
    \ allowed. (default: 10)"
  args: !SimpleFlagArg
    name: MAX_JOBS_PER_SECOND
- !Flag
  optional: true
  synonyms:
  - --max-status-checks-per-second
  description: "Maximal number of job status checks per second,\ndefault is 10, fractions\
    \ allowed. (default: 10)"
  args: !SimpleFlagArg
    name: MAX_STATUS_CHECKS_PER_SECOND
- !Flag
  optional: true
  synonyms:
  - -T
  - --restart-times
  description: "Number of times to restart failing jobs (defaults to\n0). (default:\
    \ 0)"
  args: !SimpleFlagArg
    name: RESTART_TIMES
- !Flag
  optional: true
  synonyms:
  - --attempt
  description: "Internal use only: define the initial value of the\nattempt parameter\
    \ (default: 1). (default: 1)"
  args: !SimpleFlagArg
    name: ATTEMPT
- !Flag
  optional: true
  synonyms:
  - --wrapper-prefix
  description: "Prefix for URL created from wrapper directive\n(default: https://github.com/snakemake/snakemake-\n\
    wrappers/raw/). Set this to a different URL to use\nyour fork or a local clone\
    \ of the repository, e.g.,\nuse a git URL like\n'git+file://path/to/your/local/clone@'.\
    \ (default:\nhttps://github.com/snakemake/snakemake-wrappers/raw/)"
  args: !SimpleFlagArg
    name: WRAPPER_PREFIX
- !Flag
  optional: true
  synonyms:
  - --default-remote-provider
  description: "Specify default remote provider to be used for all\ninput and output\
    \ files that don't yet specify one.\n(default: None)"
  args: !ChoiceFlagArg
    choices: !!set
      AzBlob:
      S3:
      GS:
      iRODS:
      gridftp:
      S3Mocked:
      FTP:
      SFTP:
      gfal:
- !Flag
  optional: true
  synonyms:
  - --default-remote-prefix
  description: "Specify prefix for default remote provider. E.g. a\nbucket name. (default:\
    \ )"
  args: !SimpleFlagArg
    name: DEFAULT_REMOTE_PREFIX
- !Flag
  optional: true
  synonyms:
  - --no-shared-fs
  description: "Do not assume that jobs share a common file system.\nWhen this flag\
    \ is activated, Snakemake will assume\nthat the filesystem on a cluster node is\
    \ not shared\nwith other nodes. For example, this will lead to\ndownloading remote\
    \ files on each cluster node\nseparately. Further, it won't take special measures\
    \ to\ndeal with filesystem latency issues. This option will\nin most cases only\
    \ make sense in combination with\n--default-remote-provider. Further, when using\n\
    --cluster you will have to also provide --cluster-\nstatus. Only activate this\
    \ if you know what you are\ndoing. (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --greediness
  description: "Set the greediness of scheduling. This value between 0\nand 1 determines\
    \ how careful jobs are selected for\nexecution. The default value (1.0) provides\
    \ the best\nspeed and still acceptable scheduling quality.\n(default: None)"
  args: !SimpleFlagArg
    name: GREEDINESS
- !Flag
  optional: true
  synonyms:
  - --no-hooks
  description: "Do not invoke onstart, onsuccess or onerror hooks\nafter execution.\
    \ (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --overwrite-shellcmd
  description: "Provide a shell command that shall be executed instead\nof those given\
    \ in the workflow. This is for debugging\npurposes only. (default: None)"
  args: !SimpleFlagArg
    name: OVERWRITE_SHELLCMD
- !Flag
  optional: true
  synonyms:
  - --debug
  description: "Allow to debug rules with e.g. PDB. This flag allows\nto set breakpoints\
    \ in run blocks. (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --runtime-profile
  description: "Profile Snakemake and write the output to FILE. This\nrequires yappi\
    \ to be installed. (default: None)"
  args: !SimpleFlagArg
    name: FILE
- !Flag
  optional: true
  synonyms:
  - --mode
  description: "Set execution mode of Snakemake (internal use only).\n(default: 0)"
  args: !ChoiceFlagArg
    choices: !!set
      '2':
      '1':
      '0':
- !Flag
  optional: true
  synonyms:
  - --show-failed-logs
  description: "Automatically display logs of failed jobs. (default:\nFalse)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --log-handler-script
  description: "Provide a custom script containing a function 'def\nlog_handler(msg):'.\
    \ Snakemake will call this function\nfor every logging output (given as a dictionary\n\
    msg)allowing to e.g. send notifications in the form of\ne.g. slack messages or\
    \ emails. (default: None)"
  args: !SimpleFlagArg
    name: FILE
- !Flag
  optional: true
  synonyms:
  - --log-service
  description: "Set a specific messaging service for logging\noutput.Snakemake will\
    \ notify the service on errors and\ncompleted execution.Currently slack and workflow\n\
    management system (wms) are supported. (default: None)"
  args: !ChoiceFlagArg
    choices: !!set
      slack:
      wms:
      none:
- !Flag
  optional: true
  synonyms:
  - --cluster
  - -c
  description: "Execute snakemake rules with the given submit command,\ne.g. qsub.\
    \ Snakemake compiles jobs into scripts that\nare submitted to the cluster with\
    \ the given command,\nonce all input files for a particular job are present.\n\
    The submit command can be decorated to make it aware\nof certain job properties\
    \ (name, rulename, input,\noutput, params, wildcards, log, threads and\ndependencies\
    \ (see the argument below)), e.g.: $\nsnakemake --cluster 'qsub -pe threaded {threads}'.\n\
    (default: None)"
  args: !SimpleFlagArg
    name: CMD
- !Flag
  optional: true
  synonyms:
  - --cluster-sync
  description: "cluster submission command will block, returning the\nremote exitstatus\
    \ upon remote termination (for\nexample, this should be usedif the cluster command\
    \ is\n'qsub -sync y' (SGE) (default: None)"
  args: !SimpleFlagArg
    name: CMD
- !Flag
  optional: true
  synonyms:
  - --drmaa
  description: "[ARGS]        Execute snakemake on a cluster accessed via DRMAA,\n\
    Snakemake compiles jobs into scripts that are\nsubmitted to the cluster with the\
    \ given command, once\nall input files for a particular job are present. ARGS\n\
    can be used to specify options of the underlying\ncluster system, thereby using\
    \ the job properties name,\nrulename, input, output, params, wildcards, log,\n\
    threads and dependencies, e.g.: --drmaa ' -pe threaded\n{threads}'. Note that\
    \ ARGS must be given in quotes and\nwith a leading whitespace. (default: None)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --cluster-config
  - -u
  description: "A JSON or YAML file that defines the wildcards used in\n'cluster'for\
    \ specific rules, instead of having them\nspecified in the Snakefile. For example,\
    \ for rule\n'job' you may define: { 'job' : { 'time' : '24:00:00'\n} } to specify\
    \ the time for rule 'job'. You can\nspecify more than one file. The configuration\
    \ files\nare merged with later values overriding earlier ones.\nThis option is\
    \ deprecated in favor of using --profile,\nsee docs. (default: [])"
  args: !SimpleFlagArg
    name: FILE
- !Flag
  optional: true
  synonyms:
  - --immediate-submit
  - --is
  description: "Immediately submit all jobs to the cluster instead of\nwaiting for\
    \ present input files. This will fail,\nunless you make the cluster aware of job\
    \ dependencies,\ne.g. via: $ snakemake --cluster 'sbatch --dependency\n{dependencies}.\
    \ Assuming that your submit script (here\nsbatch) outputs the generated job id\
    \ to the first\nstdout line, {dependencies} will be filled with space\nseparated\
    \ job ids this job depends on. (default:\nFalse)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --jobscript
  - --js
  description: "Provide a custom job script for submission to the\ncluster. The default\
    \ script resides as 'jobscript.sh'\nin the installation directory. (default: None)"
  args: !SimpleFlagArg
    name: SCRIPT
- !Flag
  optional: true
  synonyms:
  - --jobname
  - --jn
  description: "Provide a custom name for the jobscript that is\nsubmitted to the\
    \ cluster (see --cluster). NAME is\n\"snakejob.{name}.{jobid}.sh\" per default.\
    \ The wildcard\n{jobid} has to be present in the name. (default:\nsnakejob.{name}.{jobid}.sh)"
  args: !SimpleFlagArg
    name: NAME
- !Flag
  optional: true
  synonyms:
  - --cluster-status
  description: "Status command for cluster execution. This is only\nconsidered in\
    \ combination with the --cluster flag. If\nprovided, Snakemake will use the status\
    \ command to\ndetermine if a job has finished successfully or\nfailed. For this\
    \ it is necessary that the submit\ncommand provided to --cluster returns the cluster\
    \ job\nid. Then, the status command will be invoked with the\njob id. Snakemake\
    \ expects it to return 'success' if\nthe job was successfull, 'failed' if the\
    \ job failed\nand 'running' if the job still runs. (default: None)"
  args: !SimpleFlagArg
    name: CLUSTER_STATUS
- !Flag
  optional: true
  synonyms:
  - --drmaa-log-dir
  description: "Specify a directory in which stdout and stderr files\nof DRMAA jobs\
    \ will be written. The value may be given\nas a relative path, in which case Snakemake\
    \ will use\nthe current invocation directory as the origin. If\ngiven, this will\
    \ override any given '-o' and/or '-e'\nnative specification. If not given, all\
    \ DRMAA stdout\nand stderr files are written to the current working\ndirectory.\
    \ (default: None)"
  args: !SimpleFlagArg
    name: DIR
- !Flag
  optional: true
  synonyms:
  - --kubernetes
  description: "[NAMESPACE]\nExecute workflow in a kubernetes cluster (in the\ncloud).\
    \ NAMESPACE is the namespace you want to use for\nyour job (if nothing specified:\
    \ 'default'). Usually,\nthis requires --default-remote-provider and --default-\n\
    remote-prefix to be set to a S3 or GS bucket where\nyour . data shall be stored.\
    \ It is further advisable\nto activate conda integration via --use-conda.\n(default:\
    \ None)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --container-image
  description: "Docker image to use, e.g., when submitting jobs to\nkubernetes Defaults\
    \ to\n'https://hub.docker.com/r/snakemake/snakemake', tagged\nwith the same version\
    \ as the currently running\nSnakemake instance. Note that overwriting this value\n\
    is up to your responsibility. Any used image has to\ncontain a working snakemake\
    \ installation that is\ncompatible with (or ideally the same as) the currently\n\
    running version. (default: None)"
  args: !SimpleFlagArg
    name: IMAGE
- !Flag
  optional: true
  synonyms:
  - --tibanna
  description: "Execute workflow on AWS cloud using Tibanna. This\nrequires --default-remote-prefix\
    \ to be set to S3\nbucket name and prefix (e.g.\n'bucketname/subdirectory') where\
    \ input is already\nstored and output will be sent to. Using --tibanna\nimplies\
    \ --default-resources is set as default.\nOptionally, use --precommand to specify\
    \ any\npreparation command to run before snakemake command on\nthe cloud (inside\
    \ snakemake container on Tibanna VM).\nAlso, --use-conda, --use-singularity, --config,\n\
    --configfile are supported and will be carried over.\n(default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --tibanna-sfn
  description: "Name of Tibanna Unicorn step function (e.g.\ntibanna_unicorn_monty).This\
    \ works as serverless\nscheduler/resource allocator and must be deployed\nfirst\
    \ using tibanna cli. (e.g. tibanna deploy_unicorn\n--usergroup=monty --buckets=bucketname)\
    \ (default:\nNone)"
  args: !SimpleFlagArg
    name: TIBANNA_SFN
- !Flag
  optional: true
  synonyms:
  - --precommand
  description: "Any command to execute before snakemake command on AWS\ncloud such\
    \ as wget, git clone, unzip, etc. This is\nused with --tibanna.Do not include\
    \ input/output\ndownload/upload commands - file transfer between S3\nbucket and\
    \ the run environment (container) is\nautomatically handled by Tibanna. (default:\
    \ None)"
  args: !SimpleFlagArg
    name: PRECOMMAND
- !Flag
  optional: true
  synonyms:
  - --tibanna-config
  description: "Additional tibanna config e.g. --tibanna-config\nspot_instance=true\
    \ subnet=<subnet_id> security\ngroup=<security_group_id> (default: None)"
  args: !RepeatFlagArg
    name: TIBANNA_CONFIG
- !Flag
  optional: true
  synonyms:
  - --google-lifesciences
  description: "Execute workflow on Google Cloud cloud using the\nGoogle Life. Science\
    \ API. This requires default\napplication credentials (json) to be created and\n\
    export to the environment to use Google Cloud Storage,\nCompute Engine, and Life\
    \ Sciences. The credential file\nshould be exported as GOOGLE_APPLICATION_CREDENTIALS\n\
    for snakemake to discover. Also, --use-conda, --use-\nsingularity, --config, --configfile\
    \ are supported and\nwill be carried over. (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --google-lifesciences-regions
  description: "Specify one or more valid instance regions (defaults\nto US) (default:\
    \ ['us-east1', 'us-west1', 'us-\ncentral1'])"
  args: !RepeatFlagArg
    name: GOOGLE_LIFESCIENCES_REGIONS
- !Flag
  optional: true
  synonyms:
  - --google-lifesciences-location
  description: "The Life Sciences API service used to schedule the\njobs. E.g., us-centra1\
    \ (Iowa) and europe-west2\n(London) Watch the terminal output to see all options\n\
    found to be available. If not specified, defaults to\nthe first found with a matching\
    \ prefix from regions\nspecified with --google-lifesciences-regions.\n(default:\
    \ None)"
  args: !SimpleFlagArg
    name: GOOGLE_LIFESCIENCES_LOCATION
- !Flag
  optional: true
  synonyms:
  - --google-lifesciences-keep-cache
  description: "Cache workflows in your Google Cloud Storage Bucket\nspecified by\
    \ --default-remote-prefix/{source}/{cache}.\nEach workflow working directory is\
    \ compressed to a\n.tar.gz, named by the hash of the contents, and kept\nin Google\
    \ Cloud Storage. By default, the caches are\ndeleted at the shutdown step of the\
    \ workflow.\n(default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --tes
  description: "Send workflow tasks to GA4GH TES server specified by\nurl. (default:\
    \ None)"
  args: !SimpleFlagArg
    name: URL
- !Flag
  optional: true
  synonyms:
  - --use-conda
  description: "If defined in the rule, run job in a conda\nenvironment. If this flag\
    \ is not set, the conda\ndirective is ignored. (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --conda-not-block-search-path-envvars
  description: "Do not block environment variables that modify the\nsearch path (R_LIBS,\
    \ PYTHONPATH, PERL5LIB, PERLLIB)\nwhen using conda environments. (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --list-conda-envs
  description: "List all conda environments and their location on\ndisk. (default:\
    \ False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --conda-prefix
  description: "Specify a directory in which the 'conda' and 'conda-\narchive' directories\
    \ are created. These are used to\nstore conda environments and their archives,\n\
    respectively. If not supplied, the value is set to the\n'.snakemake' directory\
    \ relative to the invocation\ndirectory. If supplied, the `--use-conda` flag must\n\
    also be set. The value may be given as a relative\npath, which will be extrapolated\
    \ to the invocation\ndirectory, or as an absolute path. The value can also\nbe\
    \ provided via the environment variable\n$SNAKEMAKE_CONDA_PREFIX. (default: None)"
  args: !SimpleFlagArg
    name: DIR
- !Flag
  optional: true
  synonyms:
  - --conda-cleanup-envs
  description: 'Cleanup unused conda environments. (default: False)'
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --conda-cleanup-pkgs
  description: "[{tarballs,cache}]\nCleanup conda packages after creating environments.\
    \ In\ncase of 'tarballs' mode, will clean up all downloaded\npackage tarballs.\
    \ In case of 'cache' mode, will\nadditionally clean up unused package caches.\
    \ If mode\nis omitted, will default to only cleaning up the\ntarballs. (default:\
    \ None)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --conda-create-envs-only
  description: "If specified, only creates the job-specific conda\nenvironments then\
    \ exits. The `--use-conda` flag must\nalso be set. (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --conda-frontend
  description: "Choose the conda frontend for installing environments.\nMamba is much\
    \ faster and highly recommended. (default:\nmamba)"
  args: !ChoiceFlagArg
    choices: !!set
      mamba:
      conda:
- !Flag
  optional: true
  synonyms:
  - --use-singularity
  description: "If defined in the rule, run job within a singularity\ncontainer. If\
    \ this flag is not set, the singularity\ndirective is ignored. (default: False)"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --singularity-prefix
  description: "Specify a directory in which singularity images will\nbe stored.If\
    \ not supplied, the value is set to the\n'.snakemake' directory relative to the\
    \ invocation\ndirectory. If supplied, the `--use-singularity` flag\nmust also\
    \ be set. The value may be given as a relative\npath, which will be extrapolated\
    \ to the invocation\ndirectory, or as an absolute path. (default: None)"
  args: !SimpleFlagArg
    name: DIR
- !Flag
  optional: true
  synonyms:
  - --singularity-args
  description: 'Pass additional args to singularity. (default: )'
  args: !SimpleFlagArg
    name: ARGS
- !Flag
  optional: true
  synonyms:
  - --use-envmodules
  description: "If defined in the rule, run job within the given\nenvironment modules,\
    \ loaded in the given order. This\ncan be combined with --use-conda and --use-\n\
    singularity, which will then be only used as a\nfallback for rules which don't\
    \ define environment\nmodules. (default: False)\n"
  args: !EmptyFlagArg {}
parent:
subcommands: []
usage: []
help_flag: !Flag
  optional: true
  synonyms:
  - -h
  - --help
  description: show this help message and exit
  args: !EmptyFlagArg {}
usage_flag:
version_flag: !Flag
  optional: true
  synonyms:
  - --version
  - -v
  description: show program's version number and exit
  args: !EmptyFlagArg {}
help_text: "usage: snakemake [-h] [--dry-run] [--profile PROFILE] [--cache [RULE ...]]\n\
  \                 [--snakefile FILE] [--cores [N]] [--local-cores N]\n         \
  \        [--resources [NAME=INT ...]]\n                 [--set-threads RULE=THREADS\
  \ [RULE=THREADS ...]]\n                 [--set-scatter NAME=SCATTERITEMS [NAME=SCATTERITEMS\
  \ ...]]\n                 [--default-resources [NAME=INT ...]]\n               \
  \  [--preemption-default PREEMPTION_DEFAULT]\n                 [--preemptible-rules\
  \ PREEMPTIBLE_RULES [PREEMPTIBLE_RULES ...]]\n                 [--config [KEY=VALUE\
  \ ...]] [--configfile FILE [FILE ...]]\n                 [--envvars VARNAME [VARNAME\
  \ ...]] [--directory DIR] [--touch]\n                 [--keep-going] [--force] [--forceall]\n\
  \                 [--forcerun [TARGET ...]] [--prioritize TARGET [TARGET ...]]\n\
  \                 [--batch RULE=BATCH/BATCHES] [--until TARGET [TARGET ...]]\n \
  \                [--omit-from TARGET [TARGET ...]] [--rerun-incomplete]\n      \
  \           [--shadow-prefix DIR] [--scheduler [{ilp,greedy}]]\n               \
  \  [--wms-monitor [WMS_MONITOR]]\n                 [--wms-monitor-arg [NAME=VALUE\
  \ ...]]\n                 [--scheduler-ilp-solver {COIN_CMD}] [--no-subworkflows]\n\
  \                 [--groups GROUPS [GROUPS ...]]\n                 [--group-components\
  \ GROUP_COMPONENTS [GROUP_COMPONENTS ...]]\n                 [--report [FILE]] [--report-stylesheet\
  \ CSSFILE]\n                 [--edit-notebook TARGET] [--notebook-listen IP:PORT]\n\
  \                 [--lint [{text,json}]] [--generate-unit-tests [TESTPATH]]\n  \
  \               [--containerize] [--export-cwl FILE] [--list]\n                \
  \ [--list-target-rules] [--dag] [--rulegraph] [--filegraph]\n                 [--d3dag]\
  \ [--summary] [--detailed-summary] [--archive FILE]\n                 [--cleanup-metadata\
  \ FILE [FILE ...]] [--cleanup-shadow]\n                 [--skip-script-cleanup]\
  \ [--unlock] [--list-version-changes]\n                 [--list-code-changes] [--list-input-changes]\n\
  \                 [--list-params-changes] [--list-untracked]\n                 [--delete-all-output]\
  \ [--delete-temp-output]\n                 [--bash-completion] [--keep-incomplete]\
  \ [--drop-metadata]\n                 [--version] [--reason] [--gui [PORT]] [--printshellcmds]\n\
  \                 [--debug-dag] [--stats FILE] [--nocolor] [--quiet]\n         \
  \        [--print-compilation] [--verbose] [--force-use-threads]\n             \
  \    [--allow-ambiguity] [--nolock] [--ignore-incomplete]\n                 [--max-inventory-time\
  \ SECONDS] [--latency-wait SECONDS]\n                 [--wait-for-files [FILE ...]]\
  \ [--notemp] [--keep-remote]\n                 [--keep-target-files]\n         \
  \        [--allowed-rules ALLOWED_RULES [ALLOWED_RULES ...]]\n                 [--max-jobs-per-second\
  \ MAX_JOBS_PER_SECOND]\n                 [--max-status-checks-per-second MAX_STATUS_CHECKS_PER_SECOND]\n\
  \                 [-T RESTART_TIMES] [--attempt ATTEMPT]\n                 [--wrapper-prefix\
  \ WRAPPER_PREFIX]\n                 [--default-remote-provider {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp,iRODS,AzBlob}]\n\
  \                 [--default-remote-prefix DEFAULT_REMOTE_PREFIX]\n            \
  \     [--no-shared-fs] [--greediness GREEDINESS] [--no-hooks]\n                \
  \ [--overwrite-shellcmd OVERWRITE_SHELLCMD] [--debug]\n                 [--runtime-profile\
  \ FILE] [--mode {0,1,2}]\n                 [--show-failed-logs] [--log-handler-script\
  \ FILE]\n                 [--log-service {none,slack,wms}]\n                 [--cluster\
  \ CMD | --cluster-sync CMD | --drmaa [ARGS]]\n                 [--cluster-config\
  \ FILE] [--immediate-submit]\n                 [--jobscript SCRIPT] [--jobname NAME]\n\
  \                 [--cluster-status CLUSTER_STATUS] [--drmaa-log-dir DIR]\n    \
  \             [--kubernetes [NAMESPACE]] [--container-image IMAGE]\n           \
  \      [--tibanna] [--tibanna-sfn TIBANNA_SFN]\n                 [--precommand PRECOMMAND]\n\
  \                 [--tibanna-config TIBANNA_CONFIG [TIBANNA_CONFIG ...]]\n     \
  \            [--google-lifesciences]\n                 [--google-lifesciences-regions\
  \ GOOGLE_LIFESCIENCES_REGIONS [GOOGLE_LIFESCIENCES_REGIONS ...]]\n             \
  \    [--google-lifesciences-location GOOGLE_LIFESCIENCES_LOCATION]\n           \
  \      [--google-lifesciences-keep-cache] [--tes URL] [--use-conda]\n          \
  \       [--conda-not-block-search-path-envvars] [--list-conda-envs]\n          \
  \       [--conda-prefix DIR] [--conda-cleanup-envs]\n                 [--conda-cleanup-pkgs\
  \ [{tarballs,cache}]]\n                 [--conda-create-envs-only] [--conda-frontend\
  \ {conda,mamba}]\n                 [--use-singularity] [--singularity-prefix DIR]\n\
  \                 [--singularity-args ARGS] [--use-envmodules]\n               \
  \  [target ...]\n\nSnakemake is a Python based language and execution environment\
  \ for GNU Make-\nlike workflows.\n\noptional arguments:\n  -h, --help          \
  \  show this help message and exit\n\nEXECUTION:\n  target                Targets\
  \ to build. May be rules or files. (default:\n                        None)\n  --dry-run,\
  \ --dryrun, -n\n                        Do not execute anything, and display what\
  \ would be\n                        done. If you have a very large workflow, use\
  \ --dry-run\n                        --quiet to just print a summary of the DAG\
  \ of jobs.\n                        (default: False)\n  --profile PROFILE     Name\
  \ of profile to use for configuring Snakemake.\n                        Snakemake\
  \ will search for a corresponding folder in\n                        /etc/xdg/snakemake\
  \ and /root/.config/snakemake.\n                        Alternatively, this can\
  \ be an absolute or relative\n                        path. The profile folder has\
  \ to contain a file\n                        'config.yaml'. This file can be used\
  \ to set default\n                        values for command line options in YAML\
  \ format. For\n                        example, '--cluster qsub' becomes 'cluster:\
  \ qsub' in\n                        the YAML file. Profiles can be obtained from\n\
  \                        https://github.com/snakemake-profiles. (default: None)\n\
  \  --cache [RULE ...]    Store output files of given rules in a central cache\n\
  \                        given by the environment variable\n                   \
  \     $SNAKEMAKE_OUTPUT_CACHE. Likewise, retrieve output\n                     \
  \   files of the given rules from this cache if they have\n                    \
  \    been created before (by anybody writing to the same\n                     \
  \   cache), instead of actually executing the rules.\n                        Output\
  \ files are identified by hashing all steps,\n                        parameters\
  \ and software stack (conda envs or\n                        containers) needed\
  \ to create them. (default: None)\n  --snakefile FILE, -s FILE\n               \
  \         The workflow definition in form of a\n                        snakefile.Usually,\
  \ you should not need to specify\n                        this. By default, Snakemake\
  \ will search for\n                        'Snakefile', 'snakefile', 'workflow/Snakefile',\n\
  \                        'workflow/snakefile' beneath the current working\n    \
  \                    directory, in this order. Only if you definitely want\n   \
  \                     a different layout, you need to use this parameter.\n    \
  \                    (default: None)\n  --cores [N], --jobs [N], -j [N]\n      \
  \                  Use at most N CPU cores/jobs in parallel. If N is\n         \
  \               omitted or 'all', the limit is set to the number of\n          \
  \              available CPU cores. (default: None)\n  --local-cores N       In\
  \ cluster mode, use at most N cores of the host\n                        machine\
  \ in parallel (default: number of CPU cores of\n                        the host).\
  \ The cores are used to execute local rules.\n                        This option\
  \ is ignored when not in cluster mode.\n                        (default: 2)\n \
  \ --resources [NAME=INT ...], --res [NAME=INT ...]\n                        Define\
  \ additional resources that shall constrain the\n                        scheduling\
  \ analogously to threads (see above). A\n                        resource is defined\
  \ as a name and an integer value.\n                        E.g. --resources mem_mb=1000.\
  \ Rules can use resources\n                        by defining the resource keyword,\
  \ e.g. resources:\n                        mem_mb=600. If now two rules require\
  \ 600 of the\n                        resource 'mem_mb' they won't be run in parallel\
  \ by the\n                        scheduler. (default: None)\n  --set-threads RULE=THREADS\
  \ [RULE=THREADS ...]\n                        Overwrite thread usage of rules. This\
  \ allows to fine-\n                        tune workflow parallelization. In particular,\
  \ this is\n                        helpful to target certain cluster nodes by e.g.\n\
  \                        shifting a rule to use more, or less threads than\n   \
  \                     defined in the workflow. Thereby, THREADS has to be a\n  \
  \                      positive integer, and RULE has to be the name of the\n  \
  \                      rule. (default: None)\n  --set-scatter NAME=SCATTERITEMS\
  \ [NAME=SCATTERITEMS ...]\n                        Overwrite number of scatter items\
  \ of scattergather\n                        processes. This allows to fine-tune\
  \ workflow\n                        parallelization. Thereby, SCATTERITEMS has to\
  \ be a\n                        positive integer, and NAME has to be the name of\
  \ the\n                        scattergather process defined via a scattergather\n\
  \                        directive in the workflow. (default: None)\n  --default-resources\
  \ [NAME=INT ...], --default-res [NAME=INT ...]\n                        Define default\
  \ values of resources for rules that do\n                        not define their\
  \ own values. In addition to plain\n                        integers, python expressions\
  \ over inputsize are\n                        allowed (e.g. '2*input.size_mb').When\
  \ specifying this\n                        without any arguments (--default-resources),\
  \ it\n                        defines 'mem_mb=max(2*input.size_mb, 1000)'\n    \
  \                    'disk_mb=max(2*input.size_mb, 1000)', i.e., default\n     \
  \                   disk and mem usage is twice the input file size but at\n   \
  \                     least 1GB. (default: None)\n  --preemption-default PREEMPTION_DEFAULT\n\
  \                        A preemptible instance can be requested when using the\n\
  \                        Google Life Sciences API. If you set a --preemption-\n\
  \                        default,all rules will be subject to the default.\n   \
  \                     Specifically, this integer is the number of restart\n    \
  \                    attempts that will be made given that the instance is\n   \
  \                     killed unexpectedly. Note that preemptible instances\n   \
  \                     have a maximum running time of 24 hours. If you want\n   \
  \                     to set preemptible instances for only a subset of\n      \
  \                  rules, use --preemptible-rules instead. (default:\n         \
  \               None)\n  --preemptible-rules PREEMPTIBLE_RULES [PREEMPTIBLE_RULES\
  \ ...]\n                        A preemptible instance can be requested when using\
  \ the\n                        Google Life Sciences API. If you want to use these\n\
  \                        instances for a subset of your rules, you can use\n   \
  \                     --preemptible-rules and then specify a list of rule\n    \
  \                    and integer pairs, where each integer indicates the\n     \
  \                   number of restarts to use for the rule's instance in\n     \
  \                   the case that the instance is terminated unexpectedly.\n   \
  \                     --preemptible-rules can be used in combination with\n    \
  \                    --preemption-default, and will take priority. Note\n      \
  \                  that preemptible instances have a maximum running time\n    \
  \                    of 24. If you want to apply a consistent number of\n      \
  \                  retries across all your rules, use --premption-default\n    \
  \                    instead. Example: snakemake --preemption-default 10\n     \
  \                   --preemptible-rules map_reads=3 call_variants=0\n          \
  \              (default: None)\n  --config [KEY=VALUE ...], -C [KEY=VALUE ...]\n\
  \                        Set or overwrite values in the workflow config object.\n\
  \                        The workflow config object is accessible as variable\n\
  \                        config inside the workflow. Default values can be set\n\
  \                        by providing a JSON file (see Documentation).\n       \
  \                 (default: None)\n  --configfile FILE [FILE ...], --configfiles\
  \ FILE [FILE ...]\n                        Specify or overwrite the config file\
  \ of the workflow\n                        (see the docs). Values specified in JSON\
  \ or YAML\n                        format are available in the global config dictionary\n\
  \                        inside the workflow. Multiple files overwrite each\n  \
  \                      other in the given order. (default: None)\n  --envvars VARNAME\
  \ [VARNAME ...]\n                        Environment variables to pass to cloud\
  \ jobs. (default:\n                        None)\n  --directory DIR, -d DIR\n  \
  \                      Specify working directory (relative paths in the\n      \
  \                  snakefile will use this as their origin). (default:\n       \
  \                 None)\n  --touch, -t           Touch output files (mark them up\
  \ to date without\n                        really changing them) instead of running\
  \ their\n                        commands. This is used to pretend that the rules\
  \ were\n                        executed, in order to fool future invocations of\n\
  \                        snakemake. Fails if a file does not yet exist. Note\n \
  \                       that this will only touch files that would otherwise\n \
  \                       be recreated by Snakemake (e.g. because their input\n  \
  \                      files are newer). For enforcing a touch, combine this\n \
  \                       with --force, --forceall, or --forcerun. Note however\n\
  \                        that you loose the provenance information when the\n  \
  \                      files have been created in realitiy. Hence, this\n      \
  \                  should be used only as a last resort. (default: False)\n  --keep-going,\
  \ -k      Go on with independent jobs if a job fails. (default:\n              \
  \          False)\n  --force, -f           Force the execution of the selected target\
  \ or the\n                        first rule regardless of already created output.\n\
  \                        (default: False)\n  --forceall, -F        Force the execution\
  \ of the selected (or the first)\n                        rule and all rules it\
  \ is dependent on regardless of\n                        already created output.\
  \ (default: False)\n  --forcerun [TARGET ...], -R [TARGET ...]\n               \
  \         Force the re-execution or creation of the given rules\n              \
  \          or files. Use this option if you changed a rule and\n               \
  \         want to have all its output in your workflow updated.\n              \
  \          (default: None)\n  --prioritize TARGET [TARGET ...], -P TARGET [TARGET\
  \ ...]\n                        Tell the scheduler to assign creation of given targets\n\
  \                        (and all their dependencies) highest priority.\n      \
  \                  (EXPERIMENTAL) (default: None)\n  --batch RULE=BATCH/BATCHES\n\
  \                        Only create the given BATCH of the input files of the\n\
  \                        given RULE. This can be used to iteratively run parts\n\
  \                        of very large workflows. Only the execution plan of\n \
  \                       the relevant part of the workflow has to be\n          \
  \              calculated, thereby speeding up DAG computation. It is\n        \
  \                recommended to provide the most suitable rule for\n           \
  \             batching when documenting a workflow. It should be\n             \
  \           some aggregating rule that would be executed only\n                \
  \        once, and has a large number of input files. For\n                    \
  \    example, it can be a rule that aggregates over\n                        samples.\
  \ (default: None)\n  --until TARGET [TARGET ...], -U TARGET [TARGET ...]\n     \
  \                   Runs the pipeline until it reaches the specified rules\n   \
  \                     or files. Only runs jobs that are dependencies of the\n  \
  \                      specified rule or files, does not run sibling DAGs.\n   \
  \                     (default: None)\n  --omit-from TARGET [TARGET ...], -O TARGET\
  \ [TARGET ...]\n                        Prevent the execution or creation of the\
  \ given rules\n                        or files as well as any rules or files that\
  \ are\n                        downstream of these targets in the DAG. Also runs\
  \ jobs\n                        in sibling DAGs that are independent of the rules\
  \ or\n                        files specified here. (default: None)\n  --rerun-incomplete,\
  \ --ri\n                        Re-run all jobs the output of which is recognized\
  \ as\n                        incomplete. (default: False)\n  --shadow-prefix DIR\
  \   Specify a directory in which the 'shadow' directory is\n                   \
  \     created. If not supplied, the value is set to the\n                      \
  \  '.snakemake' directory relative to the working\n                        directory.\
  \ (default: None)\n  --scheduler [{ilp,greedy}]\n                        Specifies\
  \ if jobs are selected by a greedy algorithm\n                        or by solving\
  \ an ilp. The ilp scheduler aims to reduce\n                        runtime and\
  \ hdd usage by best possible use of\n                        resources. (default:\
  \ ilp)\n  --wms-monitor [WMS_MONITOR]\n                        IP and port of workflow\
  \ management system to monitor\n                        the execution of snakemake\
  \ (e.g.\n                        http://127.0.0.1:5000) Note that if your service\n\
  \                        requires an authorization token, you must export\n    \
  \                    WMS_MONITOR_TOKEN in the environment. (default: None)\n  --wms-monitor-arg\
  \ [NAME=VALUE ...]\n                        If the workflow management service accepts\
  \ extra\n                        arguments, provide. them in key value pairs with\n\
  \                        --wms-monitor-arg. For example, to run an existing\n  \
  \                      workflow using a wms monitor, you can provide the pair\n\
  \                        id=12345 and the arguments will be provided to the\n  \
  \                      endpoint to first interact with the workflow (default:\n\
  \                        None)\n  --scheduler-ilp-solver {COIN_CMD}\n          \
  \              Specifies solver to be utilized when selecting ilp-\n           \
  \             scheduler. (default: COIN_CMD)\n  --no-subworkflows, --nosw\n    \
  \                    Do not evaluate or execute subworkflows. (default:\n      \
  \                  False)\n\nGROUPING:\n  --groups GROUPS [GROUPS ...]\n       \
  \                 Assign rules to groups (this overwrites any group\n          \
  \              definitions from the workflow). (default: None)\n  --group-components\
  \ GROUP_COMPONENTS [GROUP_COMPONENTS ...]\n                        Set the number\
  \ of connected components a group is\n                        allowed to span. By\
  \ default, this is 1, but this flag\n                        allows to extend this.\
  \ This can be used to run e.g. 3\n                        jobs of the same rule\
  \ in the same group, although they\n                        are not connected. It\
  \ can be helpful for putting\n                        together many small jobs or\
  \ benefitting of shared\n                        memory setups. (default: None)\n\
  \nREPORTS:\n  --report [FILE]       Create an HTML report with results and statistics.\n\
  \                        This can be either a .html file or a .zip file. In the\n\
  \                        former case, all results are embedded into the .html\n\
  \                        (this only works for small data). In the latter case,\n\
  \                        results are stored along with a file report.html in\n \
  \                       the zip archive. If no filename is given, an embedded\n\
  \                        report.html is the default. (default: None)\n  --report-stylesheet\
  \ CSSFILE\n                        Custom stylesheet to use for report. In particular,\n\
  \                        this can be used for branding the report with e.g. a\n\
  \                        custom logo, see docs. (default: None)\n\nNOTEBOOKS:\n\
  \  --edit-notebook TARGET\n                        Interactively edit the notebook\
  \ associated with the\n                        rule used to generate the given target\
  \ file. This will\n                        start a local jupyter notebook server.\
  \ Any changes to\n                        the notebook should be saved, and the\
  \ server has to be\n                        stopped by closing the notebook and\
  \ hitting the 'Quit'\n                        button on the jupyter dashboard. Afterwards,\
  \ the\n                        updated notebook will be automatically stored in\
  \ the\n                        path defined in the rule. If the notebook is not\
  \ yet\n                        present, this will create an empty draft. (default:\n\
  \                        None)\n  --notebook-listen IP:PORT\n                  \
  \      The IP address and PORT the notebook server used for\n                  \
  \      editing the notebook (--edit-notebook) will listen on.\n                \
  \        (default: localhost:8888)\n\nUTILITIES:\n  --lint [{text,json}]  Perform\
  \ linting on the given workflow. This will print\n                        snakemake\
  \ specific suggestions to improve code quality\n                        (work in\
  \ progress, more lints to be added in the\n                        future). If no\
  \ argument is provided, plain text output\n                        is used. (default:\
  \ None)\n  --generate-unit-tests [TESTPATH]\n                        Automatically\
  \ generate unit tests for each workflow\n                        rule. This assumes\
  \ that all input files of each job\n                        are already present.\
  \ Rules without a job with present\n                        input files will be\
  \ skipped (a warning will be\n                        issued). For each rule, one\
  \ test case will be created\n                        in the specified test folder\
  \ (.tests/unit by default).\n                        After successfull execution,\
  \ tests can be run with\n                        'pytest TESTPATH'. (default: None)\n\
  \  --containerize        Print a Dockerfile that provides an execution\n       \
  \                 environment for the workflow, including all conda\n          \
  \              environments. (default: False)\n  --export-cwl FILE     Compile workflow\
  \ to CWL and store it in given FILE.\n                        (default: None)\n\
  \  --list, -l            Show available rules in given Snakefile. (default:\n  \
  \                      False)\n  --list-target-rules, --lt\n                   \
  \     Show available target rules in given Snakefile.\n                        (default:\
  \ False)\n  --dag                 Do not execute anything and print the directed\
  \ acyclic\n                        graph of jobs in the dot language. Recommended\
  \ use on\n                        Unix systems: snakemake --dag | dot | displayNote\n\
  \                        print statements in your Snakefile may interfere with\n\
  \                        visualization. (default: False)\n  --rulegraph        \
  \   Do not execute anything and print the dependency graph\n                   \
  \     of rules in the dot language. This will be less\n                        crowded\
  \ than above DAG of jobs, but also show less\n                        information.\
  \ Note that each rule is displayed once,\n                        hence the displayed\
  \ graph will be cyclic if a rule\n                        appears in several steps\
  \ of the workflow. Use this if\n                        above option leads to a\
  \ DAG that is too large.\n                        Recommended use on Unix systems:\
  \ snakemake --rulegraph\n                        | dot | displayNote print statements\
  \ in your Snakefile\n                        may interfere with visualization. (default:\
  \ False)\n  --filegraph           Do not execute anything and print the dependency\
  \ graph\n                        of rules with their input and output files in the\
  \ dot\n                        language. This is an intermediate solution between\n\
  \                        above DAG of jobs and the rule graph. Note that each\n\
  \                        rule is displayed once, hence the displayed graph will\n\
  \                        be cyclic if a rule appears in several steps of the\n \
  \                       workflow. Use this if above option leads to a DAG that\n\
  \                        is too large. Recommended use on Unix systems:\n      \
  \                  snakemake --filegraph | dot | displayNote print\n           \
  \             statements in your Snakefile may interfere with\n                \
  \        visualization. (default: False)\n  --d3dag               Print the DAG\
  \ in D3.js compatible JSON format.\n                        (default: False)\n \
  \ --summary, -S         Print a summary of all files created by the workflow.\n\
  \                        The has the following columns: filename, modification\n\
  \                        time, rule version, status, plan. Thereby rule version\n\
  \                        contains the versionthe file was created with (see the\n\
  \                        version keyword of rules), and status denotes whether\n\
  \                        the file is missing, its input files are newer or if\n\
  \                        version or implementation of the rule changed since\n \
  \                       file creation. Finally the last column denotes whether\n\
  \                        the file will be updated or created during the next\n \
  \                       workflow execution. (default: False)\n  --detailed-summary,\
  \ -D\n                        Print a summary of all files created by the workflow.\n\
  \                        The has the following columns: filename, modification\n\
  \                        time, rule version, input file(s), shell command,\n   \
  \                     status, plan. Thereby rule version contains the\n        \
  \                version the file was created with (see the version\n          \
  \              keyword of rules), and status denotes whether the file\n        \
  \                is missing, its input files are newer or if version or\n      \
  \                  implementation of the rule changed since file\n             \
  \           creation. The input file and shell command columns are\n           \
  \             self explanatory. Finally the last column denotes\n              \
  \          whether the file will be updated or created during the\n            \
  \            next workflow execution. (default: False)\n  --archive FILE       \
  \ Archive the workflow into the given tar archive FILE.\n                      \
  \  The archive will be created such that the workflow can\n                    \
  \    be re-executed on a vanilla system. The function needs\n                  \
  \      conda and git to be installed. It will archive every\n                  \
  \      file that is under git version control. Note that it\n                  \
  \      is best practice to have the Snakefile, config files,\n                 \
  \       and scripts under version control. Hence, they will be\n               \
  \         included in the archive. Further, it will add input\n                \
  \        files that are not generated by by the workflow itself\n              \
  \          and conda environments. Note that symlinks are\n                    \
  \    dereferenced. Supported formats are .tar, .tar.gz,\n                      \
  \  .tar.bz2 and .tar.xz. (default: None)\n  --cleanup-metadata FILE [FILE ...],\
  \ --cm FILE [FILE ...]\n                        Cleanup the metadata of given files.\
  \ That means that\n                        snakemake removes any tracked version\
  \ info, and any\n                        marks that files are incomplete. (default:\
  \ None)\n  --cleanup-shadow      Cleanup old shadow directories which have not been\n\
  \                        deleted due to failures or power loss. (default:\n    \
  \                    False)\n  --skip-script-cleanup\n                        Don't\
  \ delete wrapper scripts used for execution\n                        (default: False)\n\
  \  --unlock              Remove a lock on the working directory. (default:\n   \
  \                     False)\n  --list-version-changes, --lv\n                 \
  \       List all output files that have been created with a\n                  \
  \      different version (as determined by the version\n                       \
  \ keyword). (default: False)\n  --list-code-changes, --lc\n                    \
  \    List all output files for which the rule body (run or\n                   \
  \     shell) have changed in the Snakefile. (default: False)\n  --list-input-changes,\
  \ --li\n                        List all output files for which the defined input\n\
  \                        files have changed in the Snakefile (e.g. new input\n \
  \                       files were added in the rule definition or files were\n\
  \                        renamed). For listing input file modification in the\n\
  \                        filesystem, use --summary. (default: False)\n  --list-params-changes,\
  \ --lp\n                        List all output files for which the defined params\n\
  \                        have changed in the Snakefile. (default: False)\n  --list-untracked,\
  \ --lu\n                        List all files in the working directory that are\
  \ not\n                        used in the workflow. This can be used e.g. for\n\
  \                        identifying leftover files. Hidden files and\n        \
  \                directories are ignored. (default: False)\n  --delete-all-output\
  \   Remove all files generated by the workflow. Use\n                        together\
  \ with --dry-run to list files without actually\n                        deleting\
  \ anything. Note that this will not recurse\n                        into subworkflows.\
  \ Write-protected files are not\n                        removed. Nevertheless,\
  \ use with care! (default: False)\n  --delete-temp-output  Remove all temporary\
  \ files generated by the workflow.\n                        Use together with --dry-run\
  \ to list files without\n                        actually deleting anything. Note\
  \ that this will not\n                        recurse into subworkflows. (default:\
  \ False)\n  --bash-completion     Output code to register bash completion for snakemake.\n\
  \                        Put the following in your .bashrc (including the\n    \
  \                    accents): `snakemake --bash-completion` or issue it in\n  \
  \                      an open terminal session. (default: False)\n  --keep-incomplete\
  \     Do not remove incomplete output files by failed jobs.\n                  \
  \      (default: False)\n  --drop-metadata       Drop metadata file tracking information\
  \ after job\n                        finishes. Provenance-information based reports\
  \ (e.g.\n                        --report and the --list_x_changes functions) will\
  \ be\n                        empty or incomplete. (default: False)\n  --version,\
  \ -v         show program's version number and exit\n\nOUTPUT:\n  --reason, -r \
  \         Print the reason for each executed rule. (default:\n                 \
  \       False)\n  --gui [PORT]          Serve an HTML based user interface to the\
  \ given\n                        network and port e.g. 168.129.10.15:8000. By default\n\
  \                        Snakemake is only available in the local network\n    \
  \                    (default port: 8000). To make Snakemake listen to all\n   \
  \                     ip addresses add the special host address 0.0.0.0 to\n   \
  \                     the url (0.0.0.0:8000). This is important if Snakemake\n \
  \                       is used in a virtualised environment like Docker. If\n \
  \                       possible, a browser window is opened. (default: None)\n\
  \  --printshellcmds, -p  Print out the shell commands that will be executed.\n \
  \                       (default: False)\n  --debug-dag           Print candidate\
  \ and selected jobs (including their\n                        wildcards) while inferring\
  \ DAG. This can help to debug\n                        unexpected DAG topology or\
  \ errors. (default: False)\n  --stats FILE          Write stats about Snakefile\
  \ execution in JSON format\n                        to the given file. (default:\
  \ None)\n  --nocolor             Do not use a colored output. (default: False)\n\
  \  --quiet, -q           Do not output any progress or rule information.\n     \
  \                   (default: False)\n  --print-compilation   Print the python representation\
  \ of the workflow.\n                        (default: False)\n  --verbose      \
  \       Print debugging output. (default: False)\n\nBEHAVIOR:\n  --force-use-threads\
  \   Force threads rather than processes. Helpful if shared\n                   \
  \     memory (/dev/shm) is full or unavailable. (default:\n                    \
  \    False)\n  --allow-ambiguity, -a\n                        Don't check for ambiguous\
  \ rules and simply use the\n                        first if several can produce\
  \ the same file. This\n                        allows the user to prioritize rules\
  \ by their order in\n                        the snakefile. (default: False)\n \
  \ --nolock              Do not lock the working directory (default: False)\n  --ignore-incomplete,\
  \ --ii\n                        Do not check for incomplete output files. (default:\n\
  \                        False)\n  --max-inventory-time SECONDS\n              \
  \          Spend at most SECONDS seconds to create a file\n                    \
  \    inventory for the working directory. The inventory\n                      \
  \  vastly speeds up file modification and existence\n                        checks\
  \ when computing which jobs need to be executed.\n                        However,\
  \ creating the inventory itself can be slow,\n                        e.g. on network\
  \ file systems. Hence, we do not spend\n                        more than a given\
  \ amount of time and fall back to\n                        individual checks for\
  \ the rest. (default: 20)\n  --latency-wait SECONDS, --output-wait SECONDS, -w SECONDS\n\
  \                        Wait given seconds if an output file of a job is not\n\
  \                        present after the job finished. This helps if your\n  \
  \                      filesystem suffers from latency (default 5). (default:\n\
  \                        5)\n  --wait-for-files [FILE ...]\n                   \
  \     Wait --latency-wait seconds for these files to be\n                      \
  \  present before executing the workflow. This option is\n                     \
  \   used internally to handle filesystem latency in\n                        cluster\
  \ environments. (default: None)\n  --notemp, --nt        Ignore temp() declarations.\
  \ This is useful when\n                        running only a part of the workflow,\
  \ since temp()\n                        would lead to deletion of probably needed\
  \ files by\n                        other parts of the workflow. (default: False)\n\
  \  --keep-remote         Keep local copies of remote input files. (default:\n  \
  \                      False)\n  --keep-target-files   Do not adjust the paths of\
  \ given target files relative\n                        to the working directory.\
  \ (default: False)\n  --allowed-rules ALLOWED_RULES [ALLOWED_RULES ...]\n      \
  \                  Only consider given rules. If omitted, all rules in\n       \
  \                 Snakefile are used. Note that this is intended\n             \
  \           primarily for internal use and may lead to unexpected\n            \
  \            results otherwise. (default: None)\n  --max-jobs-per-second MAX_JOBS_PER_SECOND\n\
  \                        Maximal number of cluster/drmaa jobs per second,\n    \
  \                    default is 10, fractions allowed. (default: 10)\n  --max-status-checks-per-second\
  \ MAX_STATUS_CHECKS_PER_SECOND\n                        Maximal number of job status\
  \ checks per second,\n                        default is 10, fractions allowed.\
  \ (default: 10)\n  -T RESTART_TIMES, --restart-times RESTART_TIMES\n           \
  \             Number of times to restart failing jobs (defaults to\n           \
  \             0). (default: 0)\n  --attempt ATTEMPT     Internal use only: define\
  \ the initial value of the\n                        attempt parameter (default:\
  \ 1). (default: 1)\n  --wrapper-prefix WRAPPER_PREFIX\n                        Prefix\
  \ for URL created from wrapper directive\n                        (default: https://github.com/snakemake/snakemake-\n\
  \                        wrappers/raw/). Set this to a different URL to use\n  \
  \                      your fork or a local clone of the repository, e.g.,\n   \
  \                     use a git URL like\n                        'git+file://path/to/your/local/clone@'.\
  \ (default:\n                        https://github.com/snakemake/snakemake-wrappers/raw/)\n\
  \  --default-remote-provider {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp,iRODS,AzBlob}\n\
  \                        Specify default remote provider to be used for all\n  \
  \                      input and output files that don't yet specify one.\n    \
  \                    (default: None)\n  --default-remote-prefix DEFAULT_REMOTE_PREFIX\n\
  \                        Specify prefix for default remote provider. E.g. a\n  \
  \                      bucket name. (default: )\n  --no-shared-fs        Do not\
  \ assume that jobs share a common file system.\n                        When this\
  \ flag is activated, Snakemake will assume\n                        that the filesystem\
  \ on a cluster node is not shared\n                        with other nodes. For\
  \ example, this will lead to\n                        downloading remote files on\
  \ each cluster node\n                        separately. Further, it won't take\
  \ special measures to\n                        deal with filesystem latency issues.\
  \ This option will\n                        in most cases only make sense in combination\
  \ with\n                        --default-remote-provider. Further, when using\n\
  \                        --cluster you will have to also provide --cluster-\n  \
  \                      status. Only activate this if you know what you are\n   \
  \                     doing. (default: False)\n  --greediness GREEDINESS\n     \
  \                   Set the greediness of scheduling. This value between 0\n   \
  \                     and 1 determines how careful jobs are selected for\n     \
  \                   execution. The default value (1.0) provides the best\n     \
  \                   speed and still acceptable scheduling quality.\n           \
  \             (default: None)\n  --no-hooks            Do not invoke onstart, onsuccess\
  \ or onerror hooks\n                        after execution. (default: False)\n\
  \  --overwrite-shellcmd OVERWRITE_SHELLCMD\n                        Provide a shell\
  \ command that shall be executed instead\n                        of those given\
  \ in the workflow. This is for debugging\n                        purposes only.\
  \ (default: None)\n  --debug               Allow to debug rules with e.g. PDB. This\
  \ flag allows\n                        to set breakpoints in run blocks. (default:\
  \ False)\n  --runtime-profile FILE\n                        Profile Snakemake and\
  \ write the output to FILE. This\n                        requires yappi to be installed.\
  \ (default: None)\n  --mode {0,1,2}        Set execution mode of Snakemake (internal\
  \ use only).\n                        (default: 0)\n  --show-failed-logs    Automatically\
  \ display logs of failed jobs. (default:\n                        False)\n  --log-handler-script\
  \ FILE\n                        Provide a custom script containing a function 'def\n\
  \                        log_handler(msg):'. Snakemake will call this function\n\
  \                        for every logging output (given as a dictionary\n     \
  \                   msg)allowing to e.g. send notifications in the form of\n   \
  \                     e.g. slack messages or emails. (default: None)\n  --log-service\
  \ {none,slack,wms}\n                        Set a specific messaging service for\
  \ logging\n                        output.Snakemake will notify the service on errors\
  \ and\n                        completed execution.Currently slack and workflow\n\
  \                        management system (wms) are supported. (default: None)\n\
  \nCLUSTER:\n  --cluster CMD, -c CMD\n                        Execute snakemake rules\
  \ with the given submit command,\n                        e.g. qsub. Snakemake compiles\
  \ jobs into scripts that\n                        are submitted to the cluster with\
  \ the given command,\n                        once all input files for a particular\
  \ job are present.\n                        The submit command can be decorated\
  \ to make it aware\n                        of certain job properties (name, rulename,\
  \ input,\n                        output, params, wildcards, log, threads and\n\
  \                        dependencies (see the argument below)), e.g.: $\n     \
  \                   snakemake --cluster 'qsub -pe threaded {threads}'.\n       \
  \                 (default: None)\n  --cluster-sync CMD    cluster submission command\
  \ will block, returning the\n                        remote exitstatus upon remote\
  \ termination (for\n                        example, this should be usedif the cluster\
  \ command is\n                        'qsub -sync y' (SGE) (default: None)\n  --drmaa\
  \ [ARGS]        Execute snakemake on a cluster accessed via DRMAA,\n           \
  \             Snakemake compiles jobs into scripts that are\n                  \
  \      submitted to the cluster with the given command, once\n                 \
  \       all input files for a particular job are present. ARGS\n               \
  \         can be used to specify options of the underlying\n                   \
  \     cluster system, thereby using the job properties name,\n                 \
  \       rulename, input, output, params, wildcards, log,\n                     \
  \   threads and dependencies, e.g.: --drmaa ' -pe threaded\n                   \
  \     {threads}'. Note that ARGS must be given in quotes and\n                 \
  \       with a leading whitespace. (default: None)\n  --cluster-config FILE, -u\
  \ FILE\n                        A JSON or YAML file that defines the wildcards used\
  \ in\n                        'cluster'for specific rules, instead of having them\n\
  \                        specified in the Snakefile. For example, for rule\n   \
  \                     'job' you may define: { 'job' : { 'time' : '24:00:00'\n  \
  \                      } } to specify the time for rule 'job'. You can\n       \
  \                 specify more than one file. The configuration files\n        \
  \                are merged with later values overriding earlier ones.\n       \
  \                 This option is deprecated in favor of using --profile,\n     \
  \                   see docs. (default: [])\n  --immediate-submit, --is\n      \
  \                  Immediately submit all jobs to the cluster instead of\n     \
  \                   waiting for present input files. This will fail,\n         \
  \               unless you make the cluster aware of job dependencies,\n       \
  \                 e.g. via: $ snakemake --cluster 'sbatch --dependency\n       \
  \                 {dependencies}. Assuming that your submit script (here\n     \
  \                   sbatch) outputs the generated job id to the first\n        \
  \                stdout line, {dependencies} will be filled with space\n       \
  \                 separated job ids this job depends on. (default:\n           \
  \             False)\n  --jobscript SCRIPT, --js SCRIPT\n                      \
  \  Provide a custom job script for submission to the\n                        cluster.\
  \ The default script resides as 'jobscript.sh'\n                        in the installation\
  \ directory. (default: None)\n  --jobname NAME, --jn NAME\n                    \
  \    Provide a custom name for the jobscript that is\n                        submitted\
  \ to the cluster (see --cluster). NAME is\n                        \"snakejob.{name}.{jobid}.sh\"\
  \ per default. The wildcard\n                        {jobid} has to be present in\
  \ the name. (default:\n                        snakejob.{name}.{jobid}.sh)\n  --cluster-status\
  \ CLUSTER_STATUS\n                        Status command for cluster execution.\
  \ This is only\n                        considered in combination with the --cluster\
  \ flag. If\n                        provided, Snakemake will use the status command\
  \ to\n                        determine if a job has finished successfully or\n\
  \                        failed. For this it is necessary that the submit\n    \
  \                    command provided to --cluster returns the cluster job\n   \
  \                     id. Then, the status command will be invoked with the\n  \
  \                      job id. Snakemake expects it to return 'success' if\n   \
  \                     the job was successfull, 'failed' if the job failed\n    \
  \                    and 'running' if the job still runs. (default: None)\n  --drmaa-log-dir\
  \ DIR   Specify a directory in which stdout and stderr files\n                 \
  \       of DRMAA jobs will be written. The value may be given\n                \
  \        as a relative path, in which case Snakemake will use\n                \
  \        the current invocation directory as the origin. If\n                  \
  \      given, this will override any given '-o' and/or '-e'\n                  \
  \      native specification. If not given, all DRMAA stdout\n                  \
  \      and stderr files are written to the current working\n                   \
  \     directory. (default: None)\n\nKUBERNETES:\n  --kubernetes [NAMESPACE]\n  \
  \                      Execute workflow in a kubernetes cluster (in the\n      \
  \                  cloud). NAMESPACE is the namespace you want to use for\n    \
  \                    your job (if nothing specified: 'default'). Usually,\n    \
  \                    this requires --default-remote-provider and --default-\n  \
  \                      remote-prefix to be set to a S3 or GS bucket where\n    \
  \                    your . data shall be stored. It is further advisable\n    \
  \                    to activate conda integration via --use-conda.\n          \
  \              (default: None)\n  --container-image IMAGE\n                    \
  \    Docker image to use, e.g., when submitting jobs to\n                      \
  \  kubernetes Defaults to\n                        'https://hub.docker.com/r/snakemake/snakemake',\
  \ tagged\n                        with the same version as the currently running\n\
  \                        Snakemake instance. Note that overwriting this value\n\
  \                        is up to your responsibility. Any used image has to\n \
  \                       contain a working snakemake installation that is\n     \
  \                   compatible with (or ideally the same as) the currently\n   \
  \                     running version. (default: None)\n\nTIBANNA:\n  --tibanna\
  \             Execute workflow on AWS cloud using Tibanna. This\n              \
  \          requires --default-remote-prefix to be set to S3\n                  \
  \      bucket name and prefix (e.g.\n                        'bucketname/subdirectory')\
  \ where input is already\n                        stored and output will be sent\
  \ to. Using --tibanna\n                        implies --default-resources is set\
  \ as default.\n                        Optionally, use --precommand to specify any\n\
  \                        preparation command to run before snakemake command on\n\
  \                        the cloud (inside snakemake container on Tibanna VM).\n\
  \                        Also, --use-conda, --use-singularity, --config,\n     \
  \                   --configfile are supported and will be carried over.\n     \
  \                   (default: False)\n  --tibanna-sfn TIBANNA_SFN\n            \
  \            Name of Tibanna Unicorn step function (e.g.\n                     \
  \   tibanna_unicorn_monty).This works as serverless\n                        scheduler/resource\
  \ allocator and must be deployed\n                        first using tibanna cli.\
  \ (e.g. tibanna deploy_unicorn\n                        --usergroup=monty --buckets=bucketname)\
  \ (default:\n                        None)\n  --precommand PRECOMMAND\n        \
  \                Any command to execute before snakemake command on AWS\n      \
  \                  cloud such as wget, git clone, unzip, etc. This is\n        \
  \                used with --tibanna.Do not include input/output\n             \
  \           download/upload commands - file transfer between S3\n              \
  \          bucket and the run environment (container) is\n                     \
  \   automatically handled by Tibanna. (default: None)\n  --tibanna-config TIBANNA_CONFIG\
  \ [TIBANNA_CONFIG ...]\n                        Additional tibanna config e.g. --tibanna-config\n\
  \                        spot_instance=true subnet=<subnet_id> security\n      \
  \                  group=<security_group_id> (default: None)\n\nGOOGLE_LIFE_SCIENCE:\n\
  \  --google-lifesciences\n                        Execute workflow on Google Cloud\
  \ cloud using the\n                        Google Life. Science API. This requires\
  \ default\n                        application credentials (json) to be created\
  \ and\n                        export to the environment to use Google Cloud Storage,\n\
  \                        Compute Engine, and Life Sciences. The credential file\n\
  \                        should be exported as GOOGLE_APPLICATION_CREDENTIALS\n\
  \                        for snakemake to discover. Also, --use-conda, --use-\n\
  \                        singularity, --config, --configfile are supported and\n\
  \                        will be carried over. (default: False)\n  --google-lifesciences-regions\
  \ GOOGLE_LIFESCIENCES_REGIONS [GOOGLE_LIFESCIENCES_REGIONS ...]\n              \
  \          Specify one or more valid instance regions (defaults\n              \
  \          to US) (default: ['us-east1', 'us-west1', 'us-\n                    \
  \    central1'])\n  --google-lifesciences-location GOOGLE_LIFESCIENCES_LOCATION\n\
  \                        The Life Sciences API service used to schedule the\n  \
  \                      jobs. E.g., us-centra1 (Iowa) and europe-west2\n        \
  \                (London) Watch the terminal output to see all options\n       \
  \                 found to be available. If not specified, defaults to\n       \
  \                 the first found with a matching prefix from regions\n        \
  \                specified with --google-lifesciences-regions.\n               \
  \         (default: None)\n  --google-lifesciences-keep-cache\n                \
  \        Cache workflows in your Google Cloud Storage Bucket\n                 \
  \       specified by --default-remote-prefix/{source}/{cache}.\n               \
  \         Each workflow working directory is compressed to a\n                 \
  \       .tar.gz, named by the hash of the contents, and kept\n                 \
  \       in Google Cloud Storage. By default, the caches are\n                  \
  \      deleted at the shutdown step of the workflow.\n                        (default:\
  \ False)\n\nTES:\n  --tes URL             Send workflow tasks to GA4GH TES server\
  \ specified by\n                        url. (default: None)\n\nCONDA:\n  --use-conda\
  \           If defined in the rule, run job in a conda\n                       \
  \ environment. If this flag is not set, the conda\n                        directive\
  \ is ignored. (default: False)\n  --conda-not-block-search-path-envvars\n      \
  \                  Do not block environment variables that modify the\n        \
  \                search path (R_LIBS, PYTHONPATH, PERL5LIB, PERLLIB)\n         \
  \               when using conda environments. (default: False)\n  --list-conda-envs\
  \     List all conda environments and their location on\n                      \
  \  disk. (default: False)\n  --conda-prefix DIR    Specify a directory in which\
  \ the 'conda' and 'conda-\n                        archive' directories are created.\
  \ These are used to\n                        store conda environments and their\
  \ archives,\n                        respectively. If not supplied, the value is\
  \ set to the\n                        '.snakemake' directory relative to the invocation\n\
  \                        directory. If supplied, the `--use-conda` flag must\n \
  \                       also be set. The value may be given as a relative\n    \
  \                    path, which will be extrapolated to the invocation\n      \
  \                  directory, or as an absolute path. The value can also\n     \
  \                   be provided via the environment variable\n                 \
  \       $SNAKEMAKE_CONDA_PREFIX. (default: None)\n  --conda-cleanup-envs  Cleanup\
  \ unused conda environments. (default: False)\n  --conda-cleanup-pkgs [{tarballs,cache}]\n\
  \                        Cleanup conda packages after creating environments. In\n\
  \                        case of 'tarballs' mode, will clean up all downloaded\n\
  \                        package tarballs. In case of 'cache' mode, will\n     \
  \                   additionally clean up unused package caches. If mode\n     \
  \                   is omitted, will default to only cleaning up the\n         \
  \               tarballs. (default: None)\n  --conda-create-envs-only\n        \
  \                If specified, only creates the job-specific conda\n           \
  \             environments then exits. The `--use-conda` flag must\n           \
  \             also be set. (default: False)\n  --conda-frontend {conda,mamba}\n\
  \                        Choose the conda frontend for installing environments.\n\
  \                        Mamba is much faster and highly recommended. (default:\n\
  \                        mamba)\n\nSINGULARITY:\n  --use-singularity     If defined\
  \ in the rule, run job within a singularity\n                        container.\
  \ If this flag is not set, the singularity\n                        directive is\
  \ ignored. (default: False)\n  --singularity-prefix DIR\n                      \
  \  Specify a directory in which singularity images will\n                      \
  \  be stored.If not supplied, the value is set to the\n                        '.snakemake'\
  \ directory relative to the invocation\n                        directory. If supplied,\
  \ the `--use-singularity` flag\n                        must also be set. The value\
  \ may be given as a relative\n                        path, which will be extrapolated\
  \ to the invocation\n                        directory, or as an absolute path.\
  \ (default: None)\n  --singularity-args ARGS\n                        Pass additional\
  \ args to singularity. (default: )\n\nENVIRONMENT MODULES:\n  --use-envmodules \
  \     If defined in the rule, run job within the given\n                       \
  \ environment modules, loaded in the given order. This\n                       \
  \ can be combined with --use-conda and --use-\n                        singularity,\
  \ which will then be only used as a\n                        fallback for rules\
  \ which don't define environment\n                        modules. (default: False)\n"
generated_using:
- --help
docker_image: quay.io/biocontainers/snakemake-minimal:6.1.0--pyhdfd78af_1
