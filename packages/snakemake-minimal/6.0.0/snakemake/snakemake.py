from datetime import datetime
from typing import List, Optional, Dict, Any

from janis_core import *
from janis_core.types.common_data_types import Boolean, File, Int, Array, String, Directory, Float

Snakemake_V0_1_0 = CommandToolBuilder(tool="snakemake", base_command=["snakemake"], inputs=[ToolInput(tag="in_dry_run", input_type=Boolean(optional=True), prefix="--dry-run", doc=InputDocumentation(doc="Do not execute anything, and display what would be\ndone. If you have a very large workflow, use --dry-run\n--quiet to just print a summary of the DAG of jobs.\n(default: False)")), ToolInput(tag="in_profile", input_type=File(optional=True), prefix="--profile", doc=InputDocumentation(doc="Name of profile to use for configuring Snakemake.\nSnakemake will search for a corresponding folder in\n/etc/xdg/snakemake and /root/.config/snakemake.\nAlternatively, this can be an absolute or relative\npath. The profile folder has to contain a file\n'config.yaml'. This file can be used to set default\nvalues for command line options in YAML format. For\nexample, '--cluster qsub' becomes 'cluster: qsub' in\nthe YAML file. Profiles can be obtained from\nhttps://github.com/snakemake-profiles. (default: None)")), ToolInput(tag="in_cache", input_type=Boolean(optional=True), prefix="--cache", doc=InputDocumentation(doc="[RULE ...]    Store output files of given rules in a central cache\ngiven by the environment variable\n$SNAKEMAKE_OUTPUT_CACHE. Likewise, retrieve output\nfiles of the given rules from this cache if they have\nbeen created before (by anybody writing to the same\ncache), instead of actually executing the rules.\nOutput files are identified by hashing all steps,\nparameters and software stack (conda envs or\ncontainers) needed to create them. (default: None)")), ToolInput(tag="in_snake_file", input_type=File(optional=True), prefix="--snakefile", doc=InputDocumentation(doc="The workflow definition in form of a\nsnakefile.Usually, you should not need to specify\nthis. By default, Snakemake will search for\n'Snakefile', 'snakefile', 'workflow/Snakefile',\n'workflow/snakefile' beneath the current working\ndirectory, in this order. Only if you definitely want\na different layout, you need to use this parameter.\n(default: None)")), ToolInput(tag="in_cores", input_type=Boolean(optional=True), prefix="--cores", doc=InputDocumentation(doc="[N], --jobs [N], -j [N]\nUse at most N CPU cores/jobs in parallel. If N is\nomitted or 'all', the limit is set to the number of\navailable CPU cores. (default: None)")), ToolInput(tag="in_local_cores", input_type=Int(optional=True), prefix="--local-cores", doc=InputDocumentation(doc="In cluster mode, use at most N cores of the host\nmachine in parallel (default: number of CPU cores of\nthe host). The cores are used to execute local rules.\nThis option is ignored when not in cluster mode.\n(default: 2)")), ToolInput(tag="in_resources", input_type=Boolean(optional=True), prefix="--resources", doc=InputDocumentation(doc="[NAME=INT ...], --res [NAME=INT ...]\nDefine additional resources that shall constrain the\nscheduling analogously to threads (see above). A\nresource is defined as a name and an integer value.\nE.g. --resources mem_mb=1000. Rules can use resources\nby defining the resource keyword, e.g. resources:\nmem_mb=600. If now two rules require 600 of the\nresource 'mem_mb' they won't be run in parallel by the\nscheduler. (default: None)")), ToolInput(tag="in_set_threads", input_type=Int(optional=True), prefix="--set-threads", doc=InputDocumentation(doc="=THREADS [RULE=THREADS ...]\nOverwrite thread usage of rules. This allows to fine-\ntune workflow parallelization. In particular, this is\nhelpful to target certain cluster nodes by e.g.\nshifting a rule to use more, or less threads than\ndefined in the workflow. Thereby, THREADS has to be a\npositive integer, and RULE has to be the name of the\nrule. (default: None)")), ToolInput(tag="in_set_scatter", input_type=Int(optional=True), prefix="--set-scatter", doc=InputDocumentation(doc="=SCATTERITEMS [NAME=SCATTERITEMS ...]\nOverwrite number of scatter items of scattergather\nprocesses. This allows to fine-tune workflow\nparallelization. Thereby, SCATTERITEMS has to be a\npositive integer, and NAME has to be the name of the\nscattergather process defined via a scattergather\ndirective in the workflow. (default: None)")), ToolInput(tag="in_default_resources", input_type=Boolean(optional=True), prefix="--default-resources", doc=InputDocumentation(doc="[NAME=INT ...], --default-res [NAME=INT ...]\nDefine default values of resources for rules that do\nnot define their own values. In addition to plain\nintegers, python expressions over inputsize are\nallowed (e.g. '2*input.size_mb').When specifying this\nwithout any arguments (--default-resources), it\ndefines 'mem_mb=max(2*input.size_mb, 1000)'\n'disk_mb=max(2*input.size_mb, 1000)', i.e., default\ndisk and mem usage is twice the input file size but at\nleast 1GB. (default: None)")), ToolInput(tag="in_preemption_default", input_type=Int(optional=True), prefix="--preemption-default", doc=InputDocumentation(doc="A preemptible instance can be requested when using the\nGoogle Life Sciences API. If you set a --preemption-\ndefault,all rules will be subject to the default.\nSpecifically, this integer is the number of restart\nattempts that will be made given that the instance is\nkilled unexpectedly. Note that preemptible instances\nhave a maximum running time of 24 hours. If you want\nto set preemptible instances for only a subset of\nrules, use --preemptible-rules instead. (default:\nNone)")), ToolInput(tag="in_preemptible_rules", input_type=Array(t=String(), optional=True), prefix="--preemptible-rules", doc=InputDocumentation(doc="A preemptible instance can be requested when using the\nGoogle Life Sciences API. If you want to use these\ninstances for a subset of your rules, you can use\n--preemptible-rules and then specify a list of rule\nand integer pairs, where each integer indicates the\nnumber of restarts to use for the rule's instance in\nthe case that the instance is terminated unexpectedly.\n--preemptible-rules can be used in combination with\n--preemption-default, and will take priority. Note\nthat preemptible instances have a maximum running time\nof 24. If you want to apply a consistent number of\nretries across all your rules, use --premption-default\ninstead. Example: snakemake --preemption-default 10\n--preemptible-rules map_reads=3 call_variants=0\n(default: None)")), ToolInput(tag="in_config", input_type=Boolean(optional=True), prefix="--config", doc=InputDocumentation(doc="[KEY=VALUE ...], -C [KEY=VALUE ...]\nSet or overwrite values in the workflow config object.\nThe workflow config object is accessible as variable\nconfig inside the workflow. Default values can be set\nby providing a JSON file (see Documentation).\n(default: None)")), ToolInput(tag="in_config_files", input_type=Array(t=File(), optional=True), prefix="--configfiles", doc=InputDocumentation(doc="Specify or overwrite the config file of the workflow\n(see the docs). Values specified in JSON or YAML\nformat are available in the global config dictionary\ninside the workflow. Multiple files overwrite each\nother in the given order. (default: None)")), ToolInput(tag="in_env_vars", input_type=Array(t=String(), optional=True), prefix="--envvars", doc=InputDocumentation(doc="Environment variables to pass to cloud jobs. (default:\nNone)")), ToolInput(tag="in_directory", input_type=Directory(optional=True), prefix="--directory", doc=InputDocumentation(doc="Specify working directory (relative paths in the\nsnakefile will use this as their origin). (default:\nNone)")), ToolInput(tag="in_touch", input_type=Boolean(optional=True), prefix="--touch", doc=InputDocumentation(doc="Touch output files (mark them up to date without\nreally changing them) instead of running their\ncommands. This is used to pretend that the rules were\nexecuted, in order to fool future invocations of\nsnakemake. Fails if a file does not yet exist. Note\nthat this will only touch files that would otherwise\nbe recreated by Snakemake (e.g. because their input\nfiles are newer). For enforcing a touch, combine this\nwith --force, --forceall, or --forcerun. Note however\nthat you loose the provenance information when the\nfiles have been created in realitiy. Hence, this\nshould be used only as a last resort. (default: False)")), ToolInput(tag="in_keep_going", input_type=Boolean(optional=True), prefix="--keep-going", doc=InputDocumentation(doc="Go on with independent jobs if a job fails. (default:\nFalse)")), ToolInput(tag="in_force", input_type=Boolean(optional=True), prefix="--force", doc=InputDocumentation(doc="Force the execution of the selected target or the\nfirst rule regardless of already created output.\n(default: False)")), ToolInput(tag="in_force_all", input_type=Boolean(optional=True), prefix="--forceall", doc=InputDocumentation(doc="Force the execution of the selected (or the first)\nrule and all rules it is dependent on regardless of\nalready created output. (default: False)")), ToolInput(tag="in_force_run", input_type=Boolean(optional=True), prefix="--forcerun", doc=InputDocumentation(doc="[TARGET ...], -R [TARGET ...]\nForce the re-execution or creation of the given rules\nor files. Use this option if you changed a rule and\nwant to have all its output in your workflow updated.\n(default: None)")), ToolInput(tag="in_prioritize", input_type=Array(t=String(), optional=True), prefix="--prioritize", doc=InputDocumentation(doc="Tell the scheduler to assign creation of given targets\n(and all their dependencies) highest priority.\n(EXPERIMENTAL) (default: None)")), ToolInput(tag="in_batch", input_type=Int(optional=True), prefix="--batch", doc=InputDocumentation(doc="=BATCH/BATCHES\nOnly create the given BATCH of the input files of the\ngiven RULE. This can be used to iteratively run parts\nof very large workflows. Only the execution plan of\nthe relevant part of the workflow has to be\ncalculated, thereby speeding up DAG computation. It is\nrecommended to provide the most suitable rule for\nbatching when documenting a workflow. It should be\nsome aggregating rule that would be executed only\nonce, and has a large number of input files. For\nexample, it can be a rule that aggregates over\nsamples. (default: None)")), ToolInput(tag="in_until", input_type=Array(t=String(), optional=True), prefix="--until", doc=InputDocumentation(doc="Runs the pipeline until it reaches the specified rules\nor files. Only runs jobs that are dependencies of the\nspecified rule or files, does not run sibling DAGs.\n(default: None)")), ToolInput(tag="in_omit_from", input_type=Array(t=String(), optional=True), prefix="--omit-from", doc=InputDocumentation(doc="Prevent the execution or creation of the given rules\nor files as well as any rules or files that are\ndownstream of these targets in the DAG. Also runs jobs\nin sibling DAGs that are independent of the rules or\nfiles specified here. (default: None)")), ToolInput(tag="in_rerun_incomplete", input_type=Boolean(optional=True), prefix="--rerun-incomplete", doc=InputDocumentation(doc="Re-run all jobs the output of which is recognized as\nincomplete. (default: False)")), ToolInput(tag="in_shadow_prefix", input_type=Directory(optional=True), prefix="--shadow-prefix", doc=InputDocumentation(doc="Specify a directory in which the 'shadow' directory is\ncreated. If not supplied, the value is set to the\n'.snakemake' directory relative to the working\ndirectory. (default: None)")), ToolInput(tag="in_scheduler", input_type=Boolean(optional=True), prefix="--scheduler", doc=InputDocumentation(doc="[{ilp,greedy}]\nSpecifies if jobs are selected by a greedy algorithm\nor by solving an ilp. The ilp scheduler aims to reduce\nruntime and hdd usage by best possible use of\nresources. (default: ilp)")), ToolInput(tag="in_wms_monitor", input_type=Boolean(optional=True), prefix="--wms-monitor", doc=InputDocumentation(doc="[WMS_MONITOR]\nIP and port of workflow management system to monitor\nthe execution of snakemake (e.g.\nhttp://127.0.0.1:5000) Note that if your service\nrequires an authorization token, you must export\nWMS_MONITOR_TOKEN in the environment. (default: None)")), ToolInput(tag="in_wms_monitor_arg", input_type=Boolean(optional=True), prefix="--wms-monitor-arg", doc=InputDocumentation(doc="[NAME=VALUE ...]\nIf the workflow management service accepts extra\narguments, provide. them in key value pairs with\n--wms-monitor-arg. For example, to run an existing\nworkflow using a wms monitor, you can provide the pair\nid=12345 and the arguments will be provided to the\nendpoint to first interact with the workflow (default:\nNone)")), ToolInput(tag="in_scheduler_ilp_solver", input_type=String(optional=True), prefix="--scheduler-ilp-solver", doc=InputDocumentation(doc="Specifies solver to be utilized when selecting ilp-\nscheduler. (default: COIN_CMD)")), ToolInput(tag="in_no_sub_workflows", input_type=Boolean(optional=True), prefix="--no-subworkflows", doc=InputDocumentation(doc="Do not evaluate or execute subworkflows. (default:\nFalse)")), ToolInput(tag="in_groups", input_type=Array(t=String(), optional=True), prefix="--groups", doc=InputDocumentation(doc="Assign rules to groups (this overwrites any group\ndefinitions from the workflow). (default: None)")), ToolInput(tag="in_group_components", input_type=Array(t=String(), optional=True), prefix="--group-components", doc=InputDocumentation(doc="Set the number of connected components a group is\nallowed to span. By default, this is 1, but this flag\nallows to extend this. This can be used to run e.g. 3\njobs of the same rule in the same group, although they\nare not connected. It can be helpful for putting\ntogether many small jobs or benefitting of shared\nmemory setups. (default: None)")), ToolInput(tag="in_report", input_type=Boolean(optional=True), prefix="--report", doc=InputDocumentation(doc="[FILE]       Create an HTML report with results and statistics.\nThis can be either a .html file or a .zip file. In the\nformer case, all results are embedded into the .html\n(this only works for small data). In the latter case,\nresults are stored along with a file report.html in\nthe zip archive. If no filename is given, an embedded\nreport.html is the default. (default: None)")), ToolInput(tag="in_report_stylesheet", input_type=File(optional=True), prefix="--report-stylesheet", doc=InputDocumentation(doc="Custom stylesheet to use for report. In particular,\nthis can be used for branding the report with e.g. a\ncustom logo, see docs. (default: None)")), ToolInput(tag="in_edit_notebook", input_type=File(optional=True), prefix="--edit-notebook", doc=InputDocumentation(doc="Interactively edit the notebook associated with the\nrule used to generate the given target file. This will\nstart a local jupyter notebook server. Any changes to\nthe notebook should be saved, and the server has to be\nstopped by closing the notebook and hitting the 'Quit'\nbutton on the jupyter dashboard. Afterwards, the\nupdated notebook will be automatically stored in the\npath defined in the rule. If the notebook is not yet\npresent, this will create an empty draft. (default:\nNone)")), ToolInput(tag="in_notebook_listen", input_type=Int(optional=True), prefix="--notebook-listen", doc=InputDocumentation(doc=":PORT\nThe IP address and PORT the notebook server used for\nediting the notebook (--edit-notebook) will listen on.\n(default: localhost:8888)")), ToolInput(tag="in_lint", input_type=Boolean(optional=True), prefix="--lint", doc=InputDocumentation(doc="[{text,json}]  Perform linting on the given workflow. This will print\nsnakemake specific suggestions to improve code quality\n(work in progress, more lints to be added in the\nfuture). If no argument is provided, plain text output\nis used. (default: None)")), ToolInput(tag="in_generate_unit_tests", input_type=Boolean(optional=True), prefix="--generate-unit-tests", doc=InputDocumentation(doc="[TESTPATH]\nAutomatically generate unit tests for each workflow\nrule. This assumes that all input files of each job\nare already present. Rules without a job with present\ninput files will be skipped (a warning will be\nissued). For each rule, one test case will be created\nin the specified test folder (.tests/unit by default).\nAfter successfull execution, tests can be run with\n'pytest TESTPATH'. (default: None)")), ToolInput(tag="in_container_ize", input_type=Boolean(optional=True), prefix="--containerize", doc=InputDocumentation(doc="Print a Dockerfile that provides an execution\nenvironment for the workflow, including all conda\nenvironments. (default: False)")), ToolInput(tag="in_export_cwl", input_type=File(optional=True), prefix="--export-cwl", doc=InputDocumentation(doc="Compile workflow to CWL and store it in given FILE.\n(default: None)")), ToolInput(tag="in_list", input_type=Boolean(optional=True), prefix="--list", doc=InputDocumentation(doc="Show available rules in given Snakefile. (default:\nFalse)")), ToolInput(tag="in_list_target_rules", input_type=Boolean(optional=True), prefix="--list-target-rules", doc=InputDocumentation(doc="Show available target rules in given Snakefile.\n(default: False)")), ToolInput(tag="in_dag", input_type=Boolean(optional=True), prefix="--dag", doc=InputDocumentation(doc="Do not execute anything and print the directed acyclic\ngraph of jobs in the dot language. Recommended use on\nUnix systems: snakemake --dag | dot | displayNote\nprint statements in your Snakefile may interfere with\nvisualization. (default: False)")), ToolInput(tag="in_rule_graph", input_type=Boolean(optional=True), prefix="--rulegraph", doc=InputDocumentation(doc="Do not execute anything and print the dependency graph\nof rules in the dot language. This will be less\ncrowded than above DAG of jobs, but also show less\ninformation. Note that each rule is displayed once,\nhence the displayed graph will be cyclic if a rule\nappears in several steps of the workflow. Use this if\nabove option leads to a DAG that is too large.\nRecommended use on Unix systems: snakemake --rulegraph\n| dot | displayNote print statements in your Snakefile\nmay interfere with visualization. (default: False)")), ToolInput(tag="in_file_graph", input_type=Boolean(optional=True), prefix="--filegraph", doc=InputDocumentation(doc="Do not execute anything and print the dependency graph\nof rules with their input and output files in the dot\nlanguage. This is an intermediate solution between\nabove DAG of jobs and the rule graph. Note that each\nrule is displayed once, hence the displayed graph will\nbe cyclic if a rule appears in several steps of the\nworkflow. Use this if above option leads to a DAG that\nis too large. Recommended use on Unix systems:\nsnakemake --filegraph | dot | displayNote print\nstatements in your Snakefile may interfere with\nvisualization. (default: False)")), ToolInput(tag="in_d_three_dag", input_type=Boolean(optional=True), prefix="--d3dag", doc=InputDocumentation(doc="Print the DAG in D3.js compatible JSON format.\n(default: False)")), ToolInput(tag="in_summary", input_type=Boolean(optional=True), prefix="--summary", doc=InputDocumentation(doc="Print a summary of all files created by the workflow.\nThe has the following columns: filename, modification\ntime, rule version, status, plan. Thereby rule version\ncontains the versionthe file was created with (see the\nversion keyword of rules), and status denotes whether\nthe file is missing, its input files are newer or if\nversion or implementation of the rule changed since\nfile creation. Finally the last column denotes whether\nthe file will be updated or created during the next\nworkflow execution. (default: False)")), ToolInput(tag="in_detailed_summary", input_type=Boolean(optional=True), prefix="--detailed-summary", doc=InputDocumentation(doc="Print a summary of all files created by the workflow.\nThe has the following columns: filename, modification\ntime, rule version, input file(s), shell command,\nstatus, plan. Thereby rule version contains the\nversion the file was created with (see the version\nkeyword of rules), and status denotes whether the file\nis missing, its input files are newer or if version or\nimplementation of the rule changed since file\ncreation. The input file and shell command columns are\nself explanatory. Finally the last column denotes\nwhether the file will be updated or created during the\nnext workflow execution. (default: False)")), ToolInput(tag="in_archive", input_type=File(optional=True), prefix="--archive", doc=InputDocumentation(doc="Archive the workflow into the given tar archive FILE.\nThe archive will be created such that the workflow can\nbe re-executed on a vanilla system. The function needs\nconda and git to be installed. It will archive every\nfile that is under git version control. Note that it\nis best practice to have the Snakefile, config files,\nand scripts under version control. Hence, they will be\nincluded in the archive. Further, it will add input\nfiles that are not generated by by the workflow itself\nand conda environments. Note that symlinks are\ndereferenced. Supported formats are .tar, .tar.gz,\n.tar.bz2 and .tar.xz. (default: None)")), ToolInput(tag="in_cleanup_metadata", input_type=Array(t=File(), optional=True), prefix="--cleanup-metadata", doc=InputDocumentation(doc="Cleanup the metadata of given files. That means that\nsnakemake removes any tracked version info, and any\nmarks that files are incomplete. (default: None)")), ToolInput(tag="in_cleanup_shadow", input_type=Boolean(optional=True), prefix="--cleanup-shadow", doc=InputDocumentation(doc="Cleanup old shadow directories which have not been\ndeleted due to failures or power loss. (default:\nFalse)")), ToolInput(tag="in_skip_script_cleanup", input_type=Boolean(optional=True), prefix="--skip-script-cleanup", doc=InputDocumentation(doc="Don't delete wrapper scripts used for execution\n(default: False)")), ToolInput(tag="in_unlock", input_type=Boolean(optional=True), prefix="--unlock", doc=InputDocumentation(doc="Remove a lock on the working directory. (default:\nFalse)")), ToolInput(tag="in_list_version_changes", input_type=Boolean(optional=True), prefix="--list-version-changes", doc=InputDocumentation(doc="List all output files that have been created with a\ndifferent version (as determined by the version\nkeyword). (default: False)")), ToolInput(tag="in_list_code_changes", input_type=Boolean(optional=True), prefix="--list-code-changes", doc=InputDocumentation(doc="List all output files for which the rule body (run or\nshell) have changed in the Snakefile. (default: False)")), ToolInput(tag="in_list_input_changes", input_type=Boolean(optional=True), prefix="--list-input-changes", doc=InputDocumentation(doc="List all output files for which the defined input\nfiles have changed in the Snakefile (e.g. new input\nfiles were added in the rule definition or files were\nrenamed). For listing input file modification in the\nfilesystem, use --summary. (default: False)")), ToolInput(tag="in_list_params_changes", input_type=Boolean(optional=True), prefix="--list-params-changes", doc=InputDocumentation(doc="List all output files for which the defined params\nhave changed in the Snakefile. (default: False)")), ToolInput(tag="in_list_untracked", input_type=Boolean(optional=True), prefix="--list-untracked", doc=InputDocumentation(doc="List all files in the working directory that are not\nused in the workflow. This can be used e.g. for\nidentifying leftover files. Hidden files and\ndirectories are ignored. (default: False)")), ToolInput(tag="in_delete_all_output", input_type=Boolean(optional=True), prefix="--delete-all-output", doc=InputDocumentation(doc="Remove all files generated by the workflow. Use\ntogether with --dry-run to list files without actually\ndeleting anything. Note that this will not recurse\ninto subworkflows. Write-protected files are not\nremoved. Nevertheless, use with care! (default: False)")), ToolInput(tag="in_delete_temp_output", input_type=Boolean(optional=True), prefix="--delete-temp-output", doc=InputDocumentation(doc="Remove all temporary files generated by the workflow.\nUse together with --dry-run to list files without\nactually deleting anything. Note that this will not\nrecurse into subworkflows. (default: False)")), ToolInput(tag="in_bash_completion", input_type=Boolean(optional=True), prefix="--bash-completion", doc=InputDocumentation(doc="Output code to register bash completion for snakemake.\nPut the following in your .bashrc (including the\naccents): `snakemake --bash-completion` or issue it in\nan open terminal session. (default: False)")), ToolInput(tag="in_keep_incomplete", input_type=Boolean(optional=True), prefix="--keep-incomplete", doc=InputDocumentation(doc="Do not remove incomplete output files by failed jobs.\n(default: False)")), ToolInput(tag="in_drop_metadata", input_type=Boolean(optional=True), prefix="--drop-metadata", doc=InputDocumentation(doc="Drop metadata file tracking information after job\nfinishes. Provenance-information based reports (e.g.\n--report and the --list_x_changes functions) will be\nempty or incomplete. (default: False)")), ToolInput(tag="in_reason", input_type=Boolean(optional=True), prefix="--reason", doc=InputDocumentation(doc="Print the reason for each executed rule. (default:\nFalse)")), ToolInput(tag="in_gui", input_type=Boolean(optional=True), prefix="--gui", doc=InputDocumentation(doc="[PORT]          Serve an HTML based user interface to the given\nnetwork and port e.g. 168.129.10.15:8000. By default\nSnakemake is only available in the local network\n(default port: 8000). To make Snakemake listen to all\nip addresses add the special host address 0.0.0.0 to\nthe url (0.0.0.0:8000). This is important if Snakemake\nis used in a virtualised environment like Docker. If\npossible, a browser window is opened. (default: None)")), ToolInput(tag="in_print_shell_cmds", input_type=Boolean(optional=True), prefix="--printshellcmds", doc=InputDocumentation(doc="Print out the shell commands that will be executed.\n(default: False)")), ToolInput(tag="in_debug_dag", input_type=Boolean(optional=True), prefix="--debug-dag", doc=InputDocumentation(doc="Print candidate and selected jobs (including their\nwildcards) while inferring DAG. This can help to debug\nunexpected DAG topology or errors. (default: False)")), ToolInput(tag="in_stats", input_type=File(optional=True), prefix="--stats", doc=InputDocumentation(doc="Write stats about Snakefile execution in JSON format\nto the given file. (default: None)")), ToolInput(tag="in_no_color", input_type=Boolean(optional=True), prefix="--nocolor", doc=InputDocumentation(doc="Do not use a colored output. (default: False)")), ToolInput(tag="in_quiet", input_type=Boolean(optional=True), prefix="--quiet", doc=InputDocumentation(doc="Do not output any progress or rule information.\n(default: False)")), ToolInput(tag="in_print_compilation", input_type=Boolean(optional=True), prefix="--print-compilation", doc=InputDocumentation(doc="Print the python representation of the workflow.\n(default: False)")), ToolInput(tag="in_verbose", input_type=Boolean(optional=True), prefix="--verbose", doc=InputDocumentation(doc="Print debugging output. (default: False)")), ToolInput(tag="in_force_use_threads", input_type=Boolean(optional=True), prefix="--force-use-threads", doc=InputDocumentation(doc="Force threads rather than processes. Helpful if shared\nmemory (/dev/shm) is full or unavailable. (default:\nFalse)")), ToolInput(tag="in_allow_ambiguity", input_type=Boolean(optional=True), prefix="--allow-ambiguity", doc=InputDocumentation(doc="Don't check for ambiguous rules and simply use the\nfirst if several can produce the same file. This\nallows the user to prioritize rules by their order in\nthe snakefile. (default: False)")), ToolInput(tag="in_no_lock", input_type=Boolean(optional=True), prefix="--nolock", doc=InputDocumentation(doc="Do not lock the working directory (default: False)")), ToolInput(tag="in_ignore_incomplete", input_type=Boolean(optional=True), prefix="--ignore-incomplete", doc=InputDocumentation(doc="Do not check for incomplete output files. (default:\nFalse)")), ToolInput(tag="in_max_inventory_time", input_type=Int(optional=True), prefix="--max-inventory-time", doc=InputDocumentation(doc="Spend at most SECONDS seconds to create a file\ninventory for the working directory. The inventory\nvastly speeds up file modification and existence\nchecks when computing which jobs need to be executed.\nHowever, creating the inventory itself can be slow,\ne.g. on network file systems. Hence, we do not spend\nmore than a given amount of time and fall back to\nindividual checks for the rest. (default: 20)")), ToolInput(tag="in_latency_wait", input_type=File(optional=True), prefix="--latency-wait", doc=InputDocumentation(doc="Wait given seconds if an output file of a job is not\npresent after the job finished. This helps if your\nfilesystem suffers from latency (default 5). (default:\n5)")), ToolInput(tag="in_wait_for_files", input_type=Boolean(optional=True), prefix="--wait-for-files", doc=InputDocumentation(doc="[FILE ...]\nWait --latency-wait seconds for these files to be\npresent before executing the workflow. This option is\nused internally to handle filesystem latency in\ncluster environments. (default: None)")), ToolInput(tag="in_no_temp", input_type=Boolean(optional=True), prefix="--notemp", doc=InputDocumentation(doc="Ignore temp() declarations. This is useful when\nrunning only a part of the workflow, since temp()\nwould lead to deletion of probably needed files by\nother parts of the workflow. (default: False)")), ToolInput(tag="in_keep_remote", input_type=Boolean(optional=True), prefix="--keep-remote", doc=InputDocumentation(doc="Keep local copies of remote input files. (default:\nFalse)")), ToolInput(tag="in_keep_target_files", input_type=Boolean(optional=True), prefix="--keep-target-files", doc=InputDocumentation(doc="Do not adjust the paths of given target files relative\nto the working directory. (default: False)")), ToolInput(tag="in_allowed_rules", input_type=Array(t=String(), optional=True), prefix="--allowed-rules", doc=InputDocumentation(doc="Only consider given rules. If omitted, all rules in\nSnakefile are used. Note that this is intended\nprimarily for internal use and may lead to unexpected\nresults otherwise. (default: None)")), ToolInput(tag="in_max_jobs_per_second", input_type=Int(optional=True), prefix="--max-jobs-per-second", doc=InputDocumentation(doc="Maximal number of cluster/drmaa jobs per second,\ndefault is 10, fractions allowed. (default: 10)")), ToolInput(tag="in_max_status_checks_per_second", input_type=Int(optional=True), prefix="--max-status-checks-per-second", doc=InputDocumentation(doc="Maximal number of job status checks per second,\ndefault is 10, fractions allowed. (default: 10)")), ToolInput(tag="in_restart_times", input_type=Int(optional=True), prefix="--restart-times", doc=InputDocumentation(doc="Number of times to restart failing jobs (defaults to\n0). (default: 0)")), ToolInput(tag="in_attempt", input_type=Int(optional=True), prefix="--attempt", doc=InputDocumentation(doc="Internal use only: define the initial value of the\nattempt parameter (default: 1). (default: 1)")), ToolInput(tag="in_wrapper_prefix", input_type=File(optional=True), prefix="--wrapper-prefix", doc=InputDocumentation(doc="Prefix for URL created from wrapper directive\n(default: https://github.com/snakemake/snakemake-\nwrappers/raw/). Set this to a different URL to use\nyour fork or a local clone of the repository, e.g.,\nuse a git URL like\n'git+file://path/to/your/local/clone@'. (default:\nhttps://github.com/snakemake/snakemake-wrappers/raw/)")), ToolInput(tag="in_default_remote_provider", input_type=String(optional=True), prefix="--default-remote-provider", doc=InputDocumentation(doc="Specify default remote provider to be used for all\ninput and output files that don't yet specify one.\n(default: None)")), ToolInput(tag="in_default_remote_prefix", input_type=String(optional=True), prefix="--default-remote-prefix", doc=InputDocumentation(doc="Specify prefix for default remote provider. E.g. a\nbucket name. (default: )")), ToolInput(tag="in_no_shared_fs", input_type=Boolean(optional=True), prefix="--no-shared-fs", doc=InputDocumentation(doc="Do not assume that jobs share a common file system.\nWhen this flag is activated, Snakemake will assume\nthat the filesystem on a cluster node is not shared\nwith other nodes. For example, this will lead to\ndownloading remote files on each cluster node\nseparately. Further, it won't take special measures to\ndeal with filesystem latency issues. This option will\nin most cases only make sense in combination with\n--default-remote-provider. Further, when using\n--cluster you will have to also provide --cluster-\nstatus. Only activate this if you know what you are\ndoing. (default: False)")), ToolInput(tag="in_greediness", input_type=Float(optional=True), prefix="--greediness", doc=InputDocumentation(doc="Set the greediness of scheduling. This value between 0\nand 1 determines how careful jobs are selected for\nexecution. The default value (1.0) provides the best\nspeed and still acceptable scheduling quality.\n(default: None)")), ToolInput(tag="in_no_hooks", input_type=Boolean(optional=True), prefix="--no-hooks", doc=InputDocumentation(doc="Do not invoke onstart, onsuccess or onerror hooks\nafter execution. (default: False)")), ToolInput(tag="in_overwrite_shell_cmd", input_type=String(optional=True), prefix="--overwrite-shellcmd", doc=InputDocumentation(doc="Provide a shell command that shall be executed instead\nof those given in the workflow. This is for debugging\npurposes only. (default: None)")), ToolInput(tag="in_debug", input_type=Boolean(optional=True), prefix="--debug", doc=InputDocumentation(doc="Allow to debug rules with e.g. PDB. This flag allows\nto set breakpoints in run blocks. (default: False)")), ToolInput(tag="in_runtime_profile", input_type=File(optional=True), prefix="--runtime-profile", doc=InputDocumentation(doc="Profile Snakemake and write the output to FILE. This\nrequires yappi to be installed. (default: None)")), ToolInput(tag="in_mode", input_type=String(optional=True), prefix="--mode", doc=InputDocumentation(doc="Set execution mode of Snakemake (internal use only).\n(default: 0)")), ToolInput(tag="in_show_failed_logs", input_type=Boolean(optional=True), prefix="--show-failed-logs", doc=InputDocumentation(doc="Automatically display logs of failed jobs. (default:\nFalse)")), ToolInput(tag="in_log_handler_script", input_type=File(optional=True), prefix="--log-handler-script", doc=InputDocumentation(doc="Provide a custom script containing a function 'def\nlog_handler(msg):'. Snakemake will call this function\nfor every logging output (given as a dictionary\nmsg)allowing to e.g. send notifications in the form of\ne.g. slack messages or emails. (default: None)")), ToolInput(tag="in_log_service", input_type=String(optional=True), prefix="--log-service", doc=InputDocumentation(doc="Set a specific messaging service for logging\noutput.Snakemake will notify the service on errors and\ncompleted execution.Currently slack and workflow\nmanagement system (wms) are supported. (default: None)")), ToolInput(tag="in_cluster", input_type=String(optional=True), prefix="--cluster", doc=InputDocumentation(doc="Execute snakemake rules with the given submit command,\ne.g. qsub. Snakemake compiles jobs into scripts that\nare submitted to the cluster with the given command,\nonce all input files for a particular job are present.\nThe submit command can be decorated to make it aware\nof certain job properties (name, rulename, input,\noutput, params, wildcards, log, threads and\ndependencies (see the argument below)), e.g.: $\nsnakemake --cluster 'qsub -pe threaded {threads}'.\n(default: None)")), ToolInput(tag="in_cluster_sync", input_type=String(optional=True), prefix="--cluster-sync", doc=InputDocumentation(doc="cluster submission command will block, returning the\nremote exitstatus upon remote termination (for\nexample, this should be usedif the cluster command is\n'qsub -sync y' (SGE) (default: None)")), ToolInput(tag="in_drmaa", input_type=Boolean(optional=True), prefix="--drmaa", doc=InputDocumentation(doc="[ARGS]        Execute snakemake on a cluster accessed via DRMAA,\nSnakemake compiles jobs into scripts that are\nsubmitted to the cluster with the given command, once\nall input files for a particular job are present. ARGS\ncan be used to specify options of the underlying\ncluster system, thereby using the job properties name,\nrulename, input, output, params, wildcards, log,\nthreads and dependencies, e.g.: --drmaa ' -pe threaded\n{threads}'. Note that ARGS must be given in quotes and\nwith a leading whitespace. (default: None)")), ToolInput(tag="in_cluster_config", input_type=File(optional=True), prefix="--cluster-config", doc=InputDocumentation(doc="A JSON or YAML file that defines the wildcards used in\n'cluster'for specific rules, instead of having them\nspecified in the Snakefile. For example, for rule\n'job' you may define: { 'job' : { 'time' : '24:00:00'\n} } to specify the time for rule 'job'. You can\nspecify more than one file. The configuration files\nare merged with later values overriding earlier ones.\nThis option is deprecated in favor of using --profile,\nsee docs. (default: [])")), ToolInput(tag="in_immediate_submit", input_type=Boolean(optional=True), prefix="--immediate-submit", doc=InputDocumentation(doc="Immediately submit all jobs to the cluster instead of\nwaiting for present input files. This will fail,\nunless you make the cluster aware of job dependencies,\ne.g. via: $ snakemake --cluster 'sbatch --dependency\n{dependencies}. Assuming that your submit script (here\nsbatch) outputs the generated job id to the first\nstdout line, {dependencies} will be filled with space\nseparated job ids this job depends on. (default:\nFalse)")), ToolInput(tag="in_job_script", input_type=Directory(optional=True), prefix="--jobscript", doc=InputDocumentation(doc="Provide a custom job script for submission to the\ncluster. The default script resides as 'jobscript.sh'\nin the installation directory. (default: None)")), ToolInput(tag="in_job_name", input_type=String(optional=True), prefix="--jobname", doc=InputDocumentation(doc="Provide a custom name for the jobscript that is\nsubmitted to the cluster (see --cluster). NAME is\n'snakejob.{name}.{jobid}.sh' per default. The wildcard\n{jobid} has to be present in the name. (default:\nsnakejob.{name}.{jobid}.sh)")), ToolInput(tag="in_cluster_status", input_type=String(optional=True), prefix="--cluster-status", doc=InputDocumentation(doc="Status command for cluster execution. This is only\nconsidered in combination with the --cluster flag. If\nprovided, Snakemake will use the status command to\ndetermine if a job has finished successfully or\nfailed. For this it is necessary that the submit\ncommand provided to --cluster returns the cluster job\nid. Then, the status command will be invoked with the\njob id. Snakemake expects it to return 'success' if\nthe job was successfull, 'failed' if the job failed\nand 'running' if the job still runs. (default: None)")), ToolInput(tag="in_drmaa_log_dir", input_type=File(optional=True), prefix="--drmaa-log-dir", doc=InputDocumentation(doc="Specify a directory in which stdout and stderr files\nof DRMAA jobs will be written. The value may be given\nas a relative path, in which case Snakemake will use\nthe current invocation directory as the origin. If\ngiven, this will override any given '-o' and/or '-e'\nnative specification. If not given, all DRMAA stdout\nand stderr files are written to the current working\ndirectory. (default: None)")), ToolInput(tag="in_ku_bernet_es", input_type=Boolean(optional=True), prefix="--kubernetes", doc=InputDocumentation(doc="[NAMESPACE]\nExecute workflow in a kubernetes cluster (in the\ncloud). NAMESPACE is the namespace you want to use for\nyour job (if nothing specified: 'default'). Usually,\nthis requires --default-remote-provider and --default-\nremote-prefix to be set to a S3 or GS bucket where\nyour . data shall be stored. It is further advisable\nto activate conda integration via --use-conda.\n(default: None)")), ToolInput(tag="in_container_image", input_type=String(optional=True), prefix="--container-image", doc=InputDocumentation(doc="Docker image to use, e.g., when submitting jobs to\nkubernetes Defaults to\n'https://hub.docker.com/r/snakemake/snakemake', tagged\nwith the same version as the currently running\nSnakemake instance. Note that overwriting this value\nis up to your responsibility. Any used image has to\ncontain a working snakemake installation that is\ncompatible with (or ideally the same as) the currently\nrunning version. (default: None)")), ToolInput(tag="in_tib_anna", input_type=Boolean(optional=True), prefix="--tibanna", doc=InputDocumentation(doc="Execute workflow on AWS cloud using Tibanna. This\nrequires --default-remote-prefix to be set to S3\nbucket name and prefix (e.g.\n'bucketname/subdirectory') where input is already\nstored and output will be sent to. Using --tibanna\nimplies --default-resources is set as default.\nOptionally, use --precommand to specify any\npreparation command to run before snakemake command on\nthe cloud (inside snakemake container on Tibanna VM).\nAlso, --use-conda, --use-singularity, --config,\n--configfile are supported and will be carried over.\n(default: False)")), ToolInput(tag="in_tib_anna_sfn", input_type=String(optional=True), prefix="--tibanna-sfn", doc=InputDocumentation(doc="Name of Tibanna Unicorn step function (e.g.\ntibanna_unicorn_monty).This works as serverless\nscheduler/resource allocator and must be deployed\nfirst using tibanna cli. (e.g. tibanna deploy_unicorn\n--usergroup=monty --buckets=bucketname) (default:\nNone)")), ToolInput(tag="in_pre_command", input_type=File(optional=True), prefix="--precommand", doc=InputDocumentation(doc="Any command to execute before snakemake command on AWS\ncloud such as wget, git clone, unzip, etc. This is\nused with --tibanna.Do not include input/output\ndownload/upload commands - file transfer between S3\nbucket and the run environment (container) is\nautomatically handled by Tibanna. (default: None)")), ToolInput(tag="in_tib_anna_config", input_type=Array(t=String(), optional=True), prefix="--tibanna-config", doc=InputDocumentation(doc="Additional tibanna config e.g. --tibanna-config\nspot_instance=true subnet=<subnet_id> security\ngroup=<security_group_id> (default: None)")), ToolInput(tag="in_google_life_sciences", input_type=Boolean(optional=True), prefix="--google-lifesciences", doc=InputDocumentation(doc="Execute workflow on Google Cloud cloud using the\nGoogle Life. Science API. This requires default\napplication credentials (json) to be created and\nexport to the environment to use Google Cloud Storage,\nCompute Engine, and Life Sciences. The credential file\nshould be exported as GOOGLE_APPLICATION_CREDENTIALS\nfor snakemake to discover. Also, --use-conda, --use-\nsingularity, --config, --configfile are supported and\nwill be carried over. (default: False)")), ToolInput(tag="in_google_life_sciences_regions", input_type=Array(t=String(), optional=True), prefix="--google-lifesciences-regions", doc=InputDocumentation(doc="Specify one or more valid instance regions (defaults\nto US) (default: ['us-east1', 'us-west1', 'us-\ncentral1'])")), ToolInput(tag="in_google_life_sciences_location", input_type=Int(optional=True), prefix="--google-lifesciences-location", doc=InputDocumentation(doc="The Life Sciences API service used to schedule the\njobs. E.g., us-centra1 (Iowa) and europe-west2\n(London) Watch the terminal output to see all options\nfound to be available. If not specified, defaults to\nthe first found with a matching prefix from regions\nspecified with --google-lifesciences-regions.\n(default: None)")), ToolInput(tag="in_google_life_sciences_keep_cache", input_type=Boolean(optional=True), prefix="--google-lifesciences-keep-cache", doc=InputDocumentation(doc="Cache workflows in your Google Cloud Storage Bucket\nspecified by --default-remote-prefix/{source}/{cache}.\nEach workflow working directory is compressed to a\n.tar.gz, named by the hash of the contents, and kept\nin Google Cloud Storage. By default, the caches are\ndeleted at the shutdown step of the workflow.\n(default: False)")), ToolInput(tag="in_tes", input_type=Int(optional=True), prefix="--tes", doc=InputDocumentation(doc="Send workflow tasks to GA4GH TES server specified by\nurl. (default: None)")), ToolInput(tag="in_use_cond_a", input_type=Boolean(optional=True), prefix="--use-conda", doc=InputDocumentation(doc="If defined in the rule, run job in a conda\nenvironment. If this flag is not set, the conda\ndirective is ignored. (default: False)")), ToolInput(tag="in_cond_a_not_block_search_path_env_vars", input_type=Boolean(optional=True), prefix="--conda-not-block-search-path-envvars", doc=InputDocumentation(doc="Do not block environment variables that modify the\nsearch path (R_LIBS, PYTHONPATH, PERL5LIB, PERLLIB)\nwhen using conda environments. (default: False)")), ToolInput(tag="in_list_cond_a_envs", input_type=Boolean(optional=True), prefix="--list-conda-envs", doc=InputDocumentation(doc="List all conda environments and their location on\ndisk. (default: False)")), ToolInput(tag="in_cond_a_prefix", input_type=File(optional=True), prefix="--conda-prefix", doc=InputDocumentation(doc="Specify a directory in which the 'conda' and 'conda-\narchive' directories are created. These are used to\nstore conda environments and their archives,\nrespectively. If not supplied, the value is set to the\n'.snakemake' directory relative to the invocation\ndirectory. If supplied, the `--use-conda` flag must\nalso be set. The value may be given as a relative\npath, which will be extrapolated to the invocation\ndirectory, or as an absolute path. The value can also\nbe provided via the environment variable\n$SNAKEMAKE_CONDA_PREFIX. (default: None)")), ToolInput(tag="in_cond_a_cleanup_envs", input_type=Boolean(optional=True), prefix="--conda-cleanup-envs", doc=InputDocumentation(doc="Cleanup unused conda environments. (default: False)")), ToolInput(tag="in_cond_a_cleanup_pkgs", input_type=Boolean(optional=True), prefix="--conda-cleanup-pkgs", doc=InputDocumentation(doc="[{tarballs,cache}]\nCleanup conda packages after creating environments. In\ncase of 'tarballs' mode, will clean up all downloaded\npackage tarballs. In case of 'cache' mode, will\nadditionally clean up unused package caches. If mode\nis omitted, will default to only cleaning up the\ntarballs. (default: None)")), ToolInput(tag="in_cond_a_create_envs_only", input_type=Boolean(optional=True), prefix="--conda-create-envs-only", doc=InputDocumentation(doc="If specified, only creates the job-specific conda\nenvironments then exits. The `--use-conda` flag must\nalso be set. (default: False)")), ToolInput(tag="in_cond_a_front_end", input_type=String(optional=True), prefix="--conda-frontend", doc=InputDocumentation(doc="Choose the conda frontend for installing environments.\nMamba is much faster and highly recommended. (default:\nconda)")), ToolInput(tag="in_use_singularity", input_type=Boolean(optional=True), prefix="--use-singularity", doc=InputDocumentation(doc="If defined in the rule, run job within a singularity\ncontainer. If this flag is not set, the singularity\ndirective is ignored. (default: False)")), ToolInput(tag="in_singularity_prefix", input_type=File(optional=True), prefix="--singularity-prefix", doc=InputDocumentation(doc="Specify a directory in which singularity images will\nbe stored.If not supplied, the value is set to the\n'.snakemake' directory relative to the invocation\ndirectory. If supplied, the `--use-singularity` flag\nmust also be set. The value may be given as a relative\npath, which will be extrapolated to the invocation\ndirectory, or as an absolute path. (default: None)")), ToolInput(tag="in_singularity_args", input_type=String(optional=True), prefix="--singularity-args", doc=InputDocumentation(doc="Pass additional args to singularity. (default: )")), ToolInput(tag="in_use_env_modules", input_type=Boolean(optional=True), prefix="--use-envmodules", doc=InputDocumentation(doc="If defined in the rule, run job within the given\nenvironment modules, loaded in the given order. This\ncan be combined with --use-conda and --use-\nsingularity, which will then be only used as a\nfallback for rules which don't define environment\nmodules. (default: False)\n")), ToolInput(tag="in_target", input_type=String(), position=0, doc=InputDocumentation(doc="Targets to build. May be rules or files. (default:\nNone)"))], outputs=[ToolOutput(tag="out_latency_wait", output_type=File(optional=True), selector=InputSelector(input_to_select="in_latency_wait", type_hint=File()), doc=OutputDocumentation(doc="Wait given seconds if an output file of a job is not\npresent after the job finished. This helps if your\nfilesystem suffers from latency (default 5). (default:\n5)")), ToolOutput(tag="out_runtime_profile", output_type=File(optional=True), selector=InputSelector(input_to_select="in_runtime_profile", type_hint=File()), doc=OutputDocumentation(doc="Profile Snakemake and write the output to FILE. This\nrequires yappi to be installed. (default: None)"))], container="quay.io/biocontainers/snakemake-minimal:6.0.0--py_0", version="v0.1.0")


if __name__ == "__main__":
    # or "cwl"
    Snakemake_V0_1_0().translate("wdl")

