from datetime import datetime
from typing import List, Optional, Dict, Any

from janis_core import *
from janis_core.types.common_data_types import Boolean, String, File, Int, Float

Cactus_Preprocess_V0_1_0 = CommandToolBuilder(tool="cactus_preprocess", base_command=["cactus_preprocess"], inputs=[ToolInput(tag="in_logoff", input_type=Boolean(optional=True), prefix="--logOff", doc=InputDocumentation(doc="Same as --logCritical")), ToolInput(tag="in_log_critical", input_type=Boolean(optional=True), prefix="--logCritical", doc=InputDocumentation(doc="Turn on logging at level CRITICAL and above. (default\nis INFO)")), ToolInput(tag="in_log_error", input_type=Boolean(optional=True), prefix="--logError", doc=InputDocumentation(doc="Turn on logging at level ERROR and above. (default is\nINFO)")), ToolInput(tag="in_log_warning", input_type=Boolean(optional=True), prefix="--logWarning", doc=InputDocumentation(doc="Turn on logging at level WARNING and above. (default\nis INFO)")), ToolInput(tag="in_loginfo", input_type=Boolean(optional=True), prefix="--logInfo", doc=InputDocumentation(doc="Turn on logging at level INFO and above. (default is\nINFO)")), ToolInput(tag="in_log_debug", input_type=Boolean(optional=True), prefix="--logDebug", doc=InputDocumentation(doc="Turn on logging at level DEBUG and above. (default is\nINFO)")), ToolInput(tag="in_loglevel", input_type=String(optional=True), prefix="--logLevel", doc=InputDocumentation(doc="Log at given level (may be either OFF (or CRITICAL),\nERROR, WARN (or WARNING), INFO or DEBUG). (default is\nINFO)")), ToolInput(tag="in_log_file", input_type=File(optional=True), prefix="--logFile", doc=InputDocumentation(doc="File to log in")), ToolInput(tag="in_rotating_logging", input_type=Boolean(optional=True), prefix="--rotatingLogging", doc=InputDocumentation(doc="Turn on rotating logging, which prevents log files\ngetting too big.")), ToolInput(tag="in_workdir", input_type=File(optional=True), prefix="--workDir", doc=InputDocumentation(doc="Absolute path to directory where temporary files\ngenerated during the Toil run should be placed. Temp\nfiles and folders will be placed in a directory\ntoil-<workflowID> within workDir (The workflowID is\ngenerated by Toil and will be reported in the workflow\nlogs. Default is determined by the variables (TMPDIR,\nTEMP, TMP) via mkdtemp. This directory needs to exist\non all machines running jobs.")), ToolInput(tag="in_stats", input_type=Boolean(optional=True), prefix="--stats", doc=InputDocumentation(doc="Records statistics about the toil workflow to be used\nby 'toil stats'.")), ToolInput(tag="in_clean", input_type=String(optional=True), prefix="--clean", doc=InputDocumentation(doc="Determines the deletion of the jobStore upon\ncompletion of the program. Choices: 'always',\n'onError','never', 'onSuccess'. The --stats option\nrequires information from the jobStore upon completion\nso the jobStore will never be deleted withthat flag.\nIf you wish to be able to restart the run, choose\n'never' or 'onSuccess'. Default is 'never' if stats is\nenabled, and 'onSuccess' otherwise")), ToolInput(tag="in_clean_workdir", input_type=String(optional=True), prefix="--cleanWorkDir", doc=InputDocumentation(doc="Determines deletion of temporary worker directory upon\ncompletion of a job. Choices: 'always', 'never',\n'onSuccess'. Default = always. WARNING: This option\nshould be changed for debugging only. Running a full\npipeline with this option could fill your disk with\nintermediate data.")), ToolInput(tag="in_cluster_stats", input_type=File(optional=True), prefix="--clusterStats", doc=InputDocumentation(doc="[CLUSTERSTATS]\nIf enabled, writes out JSON resource usage statistics\nto a file. The default location for this file is the\ncurrent working directory, but an absolute path can\nalso be passed to specify where this file should be\nwritten. This options only applies when using scalable\nbatch systems.")), ToolInput(tag="in_restart", input_type=Boolean(optional=True), prefix="--restart", doc=InputDocumentation(doc="If --restart is specified then will attempt to restart\nexisting workflow at the location pointed to by the\n--jobStore option. Will raise an exception if the\nworkflow does not exist")), ToolInput(tag="in_batch_system", input_type=String(optional=True), prefix="--batchSystem", doc=InputDocumentation(doc="The type of batch system to run the job(s) with,\ncurrently can be one of LSF, Mesos, Slurm, Torque,\nHTCondor, singleMachine, parasol, gridEngine'.\ndefault=singleMachine")), ToolInput(tag="in_disable_hot_deployment", input_type=Boolean(optional=True), prefix="--disableHotDeployment", doc=InputDocumentation(doc="Should hot-deployment of the user script be\ndeactivated? If True, the user script/package should\nbe present at the same location on all workers.\ndefault=false")), ToolInput(tag="in_parasol_command", input_type=File(optional=True), prefix="--parasolCommand", doc=InputDocumentation(doc="The name or path of the parasol program. Will be\nlooked up on PATH unless it starts with a\nslashdefault=parasol")), ToolInput(tag="in_parasol_max_batches", input_type=Int(optional=True), prefix="--parasolMaxBatches", doc=InputDocumentation(doc="Maximum number of job batches the Parasol batch is\nallowed to create. One batch is created for jobs with\na a unique set of resource requirements. default=1000")), ToolInput(tag="in_scale", input_type=Int(optional=True), prefix="--scale", doc=InputDocumentation(doc="A scaling factor to change the value of all submitted\ntasks's submitted cores. Used in singleMachine batch\nsystem. default=1")), ToolInput(tag="in_link_imports", input_type=Boolean(optional=True), prefix="--linkImports", doc=InputDocumentation(doc="When using Toil's importFile function for staging,\ninput files are copied to the job store. Specifying\nthis option saves space by sym-linking imported files.\nAs long as caching is enabled Toil will protect the\nfile automatically by changing the permissions to\nread-only.")), ToolInput(tag="in_mesos_master", input_type=Float(optional=True), prefix="--mesosMaster", doc=InputDocumentation(doc="The host and port of the Mesos master separated by\ncolon. (default: 172.17.0.9:5050)")), ToolInput(tag="in_node_types", input_type=Int(optional=True), prefix="--nodeTypes", doc=InputDocumentation(doc="List of node types separated by commas. The syntax for\neach node type depends on the provisioner used. For\nthe cgcloud and AWS provisioners this is the name of\nan EC2 instance type, optionally followed by a colon\nand the price in dollars to bid for a spot instance of\nthat type, for example 'c3.8xlarge:0.42'.If no spot\nbid is specified, nodes of this type will be non-\npreemptable.It is acceptable to specify an instance as\nboth preemptable and non-preemptable, including it\ntwice in the list. In that case,preemptable nodes of\nthat type will be preferred when creating new nodes\nonce the maximum number of preemptable-nodes has\nbeenreached.")), ToolInput(tag="in_node_options", input_type=String(optional=True), prefix="--nodeOptions", doc=InputDocumentation(doc="Options for provisioning the nodes. The syntax depends\non the provisioner used. Neither the CGCloud nor the\nAWS provisioner support any node options.")), ToolInput(tag="in_min_nodes", input_type=Int(optional=True), prefix="--minNodes", doc=InputDocumentation(doc="Mininum number of nodes of each type in the cluster,\nif using auto-scaling. This should be provided as a\ncomma-separated list of the same length as the list of\nnode types. default=0")), ToolInput(tag="in_max_nodes", input_type=Int(optional=True), prefix="--maxNodes", doc=InputDocumentation(doc="Maximum number of nodes of each type in the cluster,\nif using autoscaling, provided as a comma-separated\nlist. The first value is used as a default if the list\nlength is less than the number of nodeTypes.\ndefault=10")), ToolInput(tag="in_alpha_packing", input_type=Int(optional=True), prefix="--alphaPacking", doc=InputDocumentation(doc="The total number of nodes estimated to be required to\ncompute the issued jobs is multiplied by the alpha\npacking parameter to produce the actual number of\nnodes requested. Values of this coefficient greater\nthan one will tend to over provision and values less\nthan one will under provision. default=0.8")), ToolInput(tag="in_beta_inertia", input_type=Int(optional=True), prefix="--betaInertia", doc=InputDocumentation(doc="A smoothing parameter to prevent unnecessary\noscillations in the number of provisioned nodes. If\nthe number of nodes is within the beta inertia of the\ncurrently provisioned number of nodes then no change\nis made to the number of requested nodes. default=1.2")), ToolInput(tag="in_scale_interval", input_type=Int(optional=True), prefix="--scaleInterval", doc=InputDocumentation(doc="The interval (seconds) between assessing if the scale\nof the cluster needs to change. default=30")), ToolInput(tag="in_preempt_able_compensation", input_type=Float(optional=True), prefix="--preemptableCompensation", doc=InputDocumentation(doc="The preference of the autoscaler to replace\npreemptable nodes with non-preemptable nodes, when\npreemptable nodes cannot be started for some reason.\nDefaults to 0.0. This value must be between 0.0 and\n1.0, inclusive. A value of 0.0 disables such\ncompensation, a value of 0.5 compensates two missing\npreemptable nodes with a non-preemptable one. A value\nof 1.0 replaces every missing pre-emptable node with a\nnon-preemptable one.")), ToolInput(tag="in_node_storage", input_type=Int(optional=True), prefix="--nodeStorage", doc=InputDocumentation(doc="Specify the size of the root volume of worker nodes\nwhen they are launched in gigabytes. You may want to\nset this if your jobs require a lot of disk space. The\ndefault value is 50.")), ToolInput(tag="in_metrics", input_type=Boolean(optional=True), prefix="--metrics", doc=InputDocumentation(doc="Enable the prometheus/grafana dashboard for monitoring\nCPU/RAM usage, queue size, and issued jobs.")), ToolInput(tag="in_max_service_jobs", input_type=Int(optional=True), prefix="--maxServiceJobs", doc=InputDocumentation(doc="The maximum number of service jobs that can be run\nconcurrently, excluding service jobs running on\npreemptable nodes. default=9223372036854775807")), ToolInput(tag="in_max_preempt_able_service_jobs", input_type=Int(optional=True), prefix="--maxPreemptableServiceJobs", doc=InputDocumentation(doc="The maximum number of service jobs that can run\nconcurrently on preemptable nodes.\ndefault=9223372036854775807")), ToolInput(tag="in_deadlock_wait", input_type=Int(optional=True), prefix="--deadlockWait", doc=InputDocumentation(doc="The minimum number of seconds to observe the cluster\nstuck running only the same service jobs before\nthrowing a deadlock exception. default=60")), ToolInput(tag="in_state_polling_wait", input_type=String(optional=True), prefix="--statePollingWait", doc=InputDocumentation(doc="Time, in seconds, to wait before doing a scheduler\nquery for job state. Return cached results if within\nthe waiting period.")), ToolInput(tag="in_default_memory", input_type=Int(optional=True), prefix="--defaultMemory", doc=InputDocumentation(doc="The default amount of memory to request for a job.\nOnly applicable to jobs that do not specify an\nexplicit value for this requirement. Standard suffixes\nlike K, Ki, M, Mi, G or Gi are supported. Default is\n2.0 Gi")), ToolInput(tag="in_default_cores", input_type=Float(optional=True), prefix="--defaultCores", doc=InputDocumentation(doc="The default number of CPU cores to dedicate a job.\nOnly applicable to jobs that do not specify an\nexplicit value for this requirement. Fractions of a\ncore (for example 0.1) are supported on some batch\nsystems, namely Mesos and singleMachine. Default is\n1.0")), ToolInput(tag="in_default_disk", input_type=Int(optional=True), prefix="--defaultDisk", doc=InputDocumentation(doc="The default amount of disk space to dedicate a job.\nOnly applicable to jobs that do not specify an\nexplicit value for this requirement. Standard suffixes\nlike K, Ki, M, Mi, G or Gi are supported. Default is\n2.0 Gi")), ToolInput(tag="in_default_preempt_able", input_type=Int(optional=True), prefix="--defaultPreemptable", doc=InputDocumentation(doc="The maximum number of CPU cores to request from the\nbatch system at any one time. Standard suffixes like\nK, Ki, M, Mi, G or Gi are supported. Default is 8.0 Ei")), ToolInput(tag="in_max_memory", input_type=Int(optional=True), prefix="--maxMemory", doc=InputDocumentation(doc="The maximum amount of memory to request from the batch\nsystem at any one time. Standard suffixes like K, Ki,\nM, Mi, G or Gi are supported. Default is 8.0 Ei")), ToolInput(tag="in_max_disk", input_type=Int(optional=True), prefix="--maxDisk", doc=InputDocumentation(doc="The maximum amount of disk space to request from the\nbatch system at any one time. Standard suffixes like\nK, Ki, M, Mi, G or Gi are supported. Default is 8.0 Ei")), ToolInput(tag="in_retry_count", input_type=Int(optional=True), prefix="--retryCount", doc=InputDocumentation(doc="Number of times to retry a failing job before giving\nup and labeling job failed. default=1")), ToolInput(tag="in_max_job_duration", input_type=Int(optional=True), prefix="--maxJobDuration", doc=InputDocumentation(doc="Maximum runtime of a job (in seconds) before we kill\nit (this is a lower bound, and the actual time before\nkilling the job may be longer).\ndefault=9223372036854775807")), ToolInput(tag="in_rescue_jobs_frequency", input_type=Int(optional=True), prefix="--rescueJobsFrequency", doc=InputDocumentation(doc="Period of time to wait (in seconds) between checking\nfor missing/overlong jobs, that is jobs which get lost\nby the batch system. Expert parameter. default=3600")), ToolInput(tag="in_disable_caching", input_type=Boolean(optional=True), prefix="--disableCaching", doc=InputDocumentation(doc="Disables caching in the file store. This flag must be\nset to use a batch system that does not support\ncaching such as Grid Engine, Parasol, LSF, or Slurm")), ToolInput(tag="in_max_log_file_size", input_type=Int(optional=True), prefix="--maxLogFileSize", doc=InputDocumentation(doc="The maximum size of a job log file to keep (in bytes),\nlog files larger than this will be truncated to the\nlast X bytes. Setting this option to zero will prevent\nany truncation. Setting this option to a negative\nvalue will truncate from the beginning.Default=62.5 K")), ToolInput(tag="in_write_logs", input_type=Boolean(optional=True), prefix="--writeLogs", doc=InputDocumentation(doc="[WRITELOGS]\nWrite worker logs received by the leader into their\nown files at the specified path. The current working\ndirectory will be used if a path is not specified\nexplicitly. Note: By default only the logs of failed\njobs are returned to leader. Set log level to 'debug'\nto get logs back from successful jobs, and adjust\n'maxLogFileSize' to control the truncation limit for\nworker logs.")), ToolInput(tag="in_write_logs_gzip", input_type=Boolean(optional=True), prefix="--writeLogsGzip", doc=InputDocumentation(doc="[WRITELOGSGZIP]\nIdentical to --writeLogs except the logs files are\ngzipped on the leader.")), ToolInput(tag="in_real_time_logging", input_type=Boolean(optional=True), prefix="--realTimeLogging", doc=InputDocumentation(doc="Enable real-time logging from workers to masters")), ToolInput(tag="in_sse_key", input_type=File(optional=True), prefix="--sseKey", doc=InputDocumentation(doc="Path to file containing 32 character key to be used\nfor server-side encryption on awsJobStore or\ngoogleJobStore. SSE will not be used if this flag is\nnot passed.")), ToolInput(tag="in_cse_key", input_type=File(optional=True), prefix="--cseKey", doc=InputDocumentation(doc="Path to file containing 256-bit key to be used for\nclient-side encryption on azureJobStore. By default,\nno encryption is used.")), ToolInput(tag="in_setenv", input_type=String(optional=True), prefix="--setEnv", doc=InputDocumentation(doc="=VALUE or NAME, -e NAME=VALUE or NAME\nSet an environment variable early on in the worker. If\nVALUE is omitted, it will be looked up in the current\nenvironment. Independently of this option, the worker\nwill try to emulate the leader's environment before\nrunning a job. Using this option, a variable can be\ninjected into the worker process itself before it is\nstarted.")), ToolInput(tag="in_service_polling_interval", input_type=Int(optional=True), prefix="--servicePollingInterval", doc=InputDocumentation(doc="Interval of time service jobs wait between polling for\nthe existence of the keep-alive flag (defailt=60)")), ToolInput(tag="in_debug_worker", input_type=Boolean(optional=True), prefix="--debugWorker", doc=InputDocumentation(doc="Experimental no forking mode for local debugging.\nSpecifically, workers are not forked and stderr/stdout\nare not redirected to the log.")), ToolInput(tag="in_bad_worker", input_type=Float(optional=True), prefix="--badWorker", doc=InputDocumentation(doc="For testing purposes randomly kill 'badWorker'\nproportion of jobs using SIGKILL, default=0.0")), ToolInput(tag="in_bad_worker_fail_interval", input_type=Float(optional=True), prefix="--badWorkerFailInterval", doc=InputDocumentation(doc="When killing the job pick uniformly within the\ninterval from 0.0 to 'badWorkerFailInterval' seconds\nafter the worker starts, default=0.01\n")), ToolInput(tag="in_output_sequence_dir", input_type=String(), position=0, doc=InputDocumentation(doc="Directory where the processed sequences will be placed")), ToolInput(tag="in_input_sequences", input_type=String(), position=1, doc=InputDocumentation(doc="input FASTA file(s)")), ToolInput(tag="in_job_store", input_type=String(), position=0, doc=InputDocumentation(doc="The location of the job store for the workflow. A job\nstore holds persistent information about the jobs and\nfiles in a workflow. If the workflow is run with a\ndistributed batch system, the job store must be\naccessible by all worker nodes. Depending on the\ndesired job store implementation, the location should\nbe formatted according to one of the following\nschemes: file:<path> where <path> points to a\ndirectory on the file systen aws:<region>:<prefix>\nwhere <region> is the name of an AWS region like us-\nwest-2 and <prefix> will be prepended to the names of\nany top-level AWS resources in use by job store, e.g.\nS3 buckets. azure:<account>:<prefix>\ngoogle:<project_id>:<prefix> TODO: explain For\nbackwards compatibility, you may also specify ./foo\n(equivalent to file:./foo or just file:foo) or /bar\n(equivalent to file:/bar).")), ToolInput(tag="in_provisioning_dot", input_type=String(), position=0, doc=InputDocumentation(doc="--provisioner {aws}   The provisioner for cluster auto-scaling. The\ncurrently supported choices are'cgcloud' or 'aws'. The\ndefault is None."))], outputs=[ToolOutput(tag="out_cluster_stats", output_type=File(optional=True), selector=InputSelector(input_to_select="in_cluster_stats", type_hint=File()), doc=OutputDocumentation(doc="[CLUSTERSTATS]\nIf enabled, writes out JSON resource usage statistics\nto a file. The default location for this file is the\ncurrent working directory, but an absolute path can\nalso be passed to specify where this file should be\nwritten. This options only applies when using scalable\nbatch systems."))], container=None, version="v0.1.0")


if __name__ == "__main__":
    # or "cwl"
    Cactus_Preprocess_V0_1_0().translate("wdl", allow_empty_container=True)

