&id001 !Command
command:
- gsutil
positional: []
named:
- !Flag
  description: ''
  synonyms:
  - -h
  args: !SimpleFlagArg
    name: header
  optional: true
- !Flag
  description: ''
  synonyms:
  - -DD
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: ''
  synonyms:
  - -D
  args: !EmptyFlagArg {}
  optional: true
parent:
subcommands:
- !Command
  command:
  - gsutil
  - test
  positional: []
  named:
  - !Flag
    description: "Run tests against multi-regional US buckets. By default,\ntests\
      \ run against regional buckets in us-central1."
    synonyms:
    - -b
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: Output coverage information.
    synonyms:
    - -c
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: Exit on first sequential test failure.
    synonyms:
    - -f
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: List available tests.
    synonyms:
    - -l
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: Run at most N tests in parallel. The default value is 5.
    synonyms:
    - -p
    args: !SimpleFlagArg
      name: N
    optional: true
  - !Flag
    description: Run tests against S3 instead of GS.
    synonyms:
    - -s
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: Only run unit tests.
    synonyms:
    - -u
    args: !EmptyFlagArg {}
    optional: true
  parent: *id001
  subcommands: []
  usage: []
  help_flag:
  usage_flag:
  version_flag:
  help_text: "NAME\n  test - Run gsutil unit/integration tests (for developers)\n\n\
    \nSYNOPSIS\n\n  gsutil test [-l] [-u] [-f] [command command...]\n\n\n\nDESCRIPTION\n\
    \  The gsutil test command runs the gsutil unit tests and integration tests.\n\
    \  The unit tests use an in-memory mock storage service implementation, while\n\
    \  the integration tests send requests to the production service using the\n \
    \ preferred API set in the boto configuration file (see \"gsutil help apis\" for\n\
    \  details).\n\n  To run both the unit tests and integration tests, run the command\
    \ with no\n  arguments:\n\n    gsutil test\n\n  To run the unit tests only (which\
    \ run quickly):\n\n    gsutil test -u\n\n  Tests run in parallel regardless of\
    \ whether the top-level -m flag is\n  present. To limit the number of tests run\
    \ in parallel to 10 at a time:\n\n    gsutil test -p 10\n\n  To force tests to\
    \ run sequentially:\n\n    gsutil test -p 1    \n\n  To have sequentially-run\
    \ tests stop running immediately when an error occurs:\n\n    gsutil test -f\n\
    \n  To run tests for one or more individual commands add those commands as\n \
    \ arguments. For example, the following command will run the cp and mv command\n\
    \  tests:\n\n    gsutil test cp mv\n\n  To list available tests, run the test\
    \ command with the -l argument:\n\n    gsutil test -l\n\n  The tests are defined\
    \ in the code under the gslib/tests module. Each test\n  file is of the format\
    \ test_[name].py where [name] is the test name you can\n  pass to this command.\
    \ For example, running \"gsutil test ls\" would run the\n  tests in \"gslib/tests/test_ls.py\"\
    .\n\n  You can also run an individual test class or function name by passing the\n\
    \  test module followed by the class name and optionally a test name. For\n  example,\
    \ to run the an entire test class by name:\n\n    gsutil test naming.GsutilNamingTests\n\
    \n  or an individual test function:\n\n    gsutil test cp.TestCp.test_streaming\n\
    \n  You can list the available tests under a module or class by passing arguments\n\
    \  with the -l option. For example, to list all available test functions in the\n\
    \  cp module:\n\n    gsutil test -l cp\n\n  To output test coverage:\n\n    gsutil\
    \ test -c -p 500\n    coverage html\n\n  This will output an HTML report to a\
    \ directory named 'htmlcov'.\n\n  Test coverage is compatible with v4.1 of the\
    \ coverage module\n  (https://pypi.python.org/pypi/coverage).\n\n\nOPTIONS\n \
    \ -b          Run tests against multi-regional US buckets. By default,\n     \
    \         tests run against regional buckets in us-central1.\n\n  -c         \
    \ Output coverage information.\n\n  -f          Exit on first sequential test\
    \ failure.\n\n  -l          List available tests.\n\n  -p N        Run at most\
    \ N tests in parallel. The default value is 5.\n\n  -s          Run tests against\
    \ S3 instead of GS.\n\n  -u          Only run unit tests.\n"
  generated_using: &id002
  - --help
- !Command
  command:
  - gsutil
  - defacl
  positional:
  - !Positional
    description: CH EXAMPLES
    position: 0
    name: description.
    optional: false
  named:
  - !Flag
    description: Remove all roles associated with the matching entity.
    synonyms:
    - -d
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Normally gsutil stops at the first error. The -f option causes\n\
      it to continue when it encounters errors. With this option the\ngsutil exit\
      \ status will be 0 even if some ACLs couldn't be\nchanged."
    synonyms:
    - -f
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: Add or modify a group entity's role.
    synonyms:
    - -g
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: Add or modify a project viewers/editors/owners role.
    synonyms:
    - -p
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: Add or modify a user entity's role.
    synonyms:
    - -u
    args: !EmptyFlagArg {}
    optional: true
  parent: *id001
  subcommands: []
  usage: []
  help_flag:
  usage_flag:
  version_flag:
  help_text: "NAME\n  defacl - Get, set, or change default ACL on buckets\n\n\nSYNOPSIS\n\
    \  gsutil defacl set file-or-canned_acl_name url...\n  gsutil defacl get url\n\
    \  gsutil defacl ch [-f] -u|-g|-d|-p <grant>... url...\n\n\n\nDESCRIPTION\n  The\
    \ defacl command has three sub-commands:\n\nSET\n  The \"defacl set\" command\
    \ sets default object ACLs for the specified buckets.\n  If you specify a default\
    \ object ACL for a certain bucket, Google Cloud\n  Storage applies the default\
    \ object ACL to all new objects uploaded to that\n  bucket, unless an ACL for\
    \ that object is separately specified during upload.\n\n  Similar to the \"acl\
    \ set\" command, the file-or-canned_acl_name names either a\n  canned ACL or the\
    \ path to a file that contains ACL text. (See \"gsutil\n  help acl\" for examples\
    \ of editing and setting ACLs via the\n  acl command.)\n\n  Setting a default\
    \ object ACL on a bucket provides a convenient way to ensure\n  newly uploaded\
    \ objects have a specific ACL. If you don't set the bucket's\n  default object\
    \ ACL, it will default to project-private. If you then upload\n  objects that\
    \ need a different ACL, you will need to perform a separate ACL\n  update operation\
    \ for each object. Depending on how many objects require\n  updates, this could\
    \ be very time-consuming.\n\nGET\n  Gets the default ACL text for a bucket, which\
    \ you can save and edit\n  for use with the \"defacl set\" command.\n\nCH\n  The\
    \ \"defacl ch\" (or \"defacl change\") command updates the default object\n  access\
    \ control list for a bucket. The syntax is shared with the \"acl ch\"\n  command,\
    \ so see the \"CH\" section of \"gsutil help acl\" for the full help\n  description.\n\
    \nCH EXAMPLES\n  Grant anyone on the internet READ access by default to any object\
    \ created\n  in the bucket example-bucket:\n\n    gsutil defacl ch -u AllUsers:R\
    \ gs://example-bucket\n\n  NOTE: By default, publicly readable objects are served\
    \ with a Cache-Control\n  header allowing such objects to be cached for 3600 seconds.\
    \ If you need to\n  ensure that updates become visible immediately, you should\
    \ set a\n  Cache-Control header of \"Cache-Control:private, max-age=0, no-transform\"\
    \ on\n  such objects. For help doing this, see \"gsutil help setmeta\".\n\n  Add\
    \ the user john.doe@example.com to the default object ACL on bucket\n  example-bucket\
    \ with READ access:\n\n    gsutil defacl ch -u john.doe@example.com:READ gs://example-bucket\n\
    \n  Add the group admins@example.com to the default object ACL on bucket\n  example-bucket\
    \ with OWNER access:\n\n    gsutil defacl ch -g admins@example.com:O gs://example-bucket\n\
    \n  Remove the group admins@example.com from the default object ACL on bucket\n\
    \  example-bucket:\n\n    gsutil defacl ch -d admins@example.com gs://example-bucket\n\
    \n  Add the owners of project example-project-123 to the default object ACL on\n\
    \  bucket example-bucket with READ access:\n\n    gsutil defacl ch -p owners-example-project-123:R\
    \ gs://example-bucket\n\n  NOTE: You can replace 'owners' with 'viewers' or 'editors'\
    \ to grant access\n  to a project's viewers/editors respectively.\n\nCH OPTIONS\n\
    \  The \"ch\" sub-command has the following options\n\n    -d          Remove\
    \ all roles associated with the matching entity.\n\n    -f          Normally gsutil\
    \ stops at the first error. The -f option causes\n                it to continue\
    \ when it encounters errors. With this option the\n                gsutil exit\
    \ status will be 0 even if some ACLs couldn't be\n                changed.\n\n\
    \    -g          Add or modify a group entity's role.\n\n    -p          Add or\
    \ modify a project viewers/editors/owners role.\n\n    -u          Add or modify\
    \ a user entity's role.\n"
  generated_using: *id002
- !Command
  command:
  - gsutil
  - rsync
  positional:
  - !Positional
    description: BE CAREFUL WHEN USING -d OPTION!
    position: 0
    name: workstation.
    optional: false
  - !Positional
    description: Note that there are cases where retrying will never succeed, such
      as if you
    position: 0
    name: operations.
    optional: false
  - !Positional
    description: 4. The gsutil rsync command copies changed files in their entirety
      and does
    position: 0
    name: hang.
    optional: false
  named:
  - !Flag
    description: "Sets named canned_acl when uploaded objects created. See\n\"gsutil\
      \ help acls\" for further details. Note that rsync will\ndecide whether or not\
      \ to perform a copy based only on object size\nand modification time, not current\
      \ ACL state. Also see the -p\noption below."
    synonyms:
    - -a
    args: !SimpleFlagArg
      name: canned_acl
    optional: true
  - !Flag
    description: "Causes the rsync command to compute and compare checksums\n(instead\
      \ of comparing mtime) for files if the size of source and\ndestination as well\
      \ as mtime (if available) match. This option\nincreases local disk I/O and run\
      \ time if either src_url or\ndst_url are on the local file system."
    synonyms:
    - -c
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "If an error occurs, continue to attempt to copy the remaining\n\
      files. If errors occurred, gsutil's exit status will be non-zero\neven if this\
      \ flag is set. This option is implicitly set when\nrunning \"gsutil -m rsync...\"\
      .  Note: -C only applies to the\nactual copying operation. If an error occurs\
      \ while iterating\nover the files in the local directory (e.g., invalid Unicode\n\
      file name) gsutil will print an error message and abort."
    synonyms:
    - -C
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Delete extra files under dst_url not found under src_url. By\ndefault\
      \ extra files are not deleted. Note: this option can\ndelete data quickly if\
      \ you specify the wrong source/destination\ncombination. See the help section\
      \ above,\n\"BE CAREFUL WHEN USING -d OPTION!\"."
    synonyms:
    - -d
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Exclude symlinks. When specified, symbolic links will be\nignored.\
      \ Note that gsutil does not follow directory symlinks,\nregardless of whether\
      \ -e is specified."
    synonyms:
    - -e
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Causes rsync to run in \"dry run\" mode, i.e., just outputting\n\
      what would be copied or deleted without actually doing any\ncopying/deleting."
    synonyms:
    - -n
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Causes ACLs to be preserved when objects are copied. Note that\n\
      rsync will decide whether or not to perform a copy based only\non object size\
      \ and modification time, not current ACL state.\nThus, if the source and destination\
      \ differ in size or\nmodification time and you run gsutil rsync -p, the file\
      \ will be\ncopied and ACL preserved. However, if the source and destination\n\
      don't differ in size or checksum but have different ACLs,\nrunning gsutil rsync\
      \ -p will have no effect.\nNote that this option has performance and cost implications\
      \ when\nusing the XML API, as it requires separate HTTP calls for\ninteracting\
      \ with ACLs. The performance issue can be mitigated to\nsome degree by using\
      \ gsutil -m rsync to cause parallel\nsynchronization. Also, this option only\
      \ works if you have OWNER\naccess to all of the objects that are copied.\nYou\
      \ can avoid the additional performance and cost of using\nrsync -p if you want\
      \ all objects in the destination bucket to\nend up with the same ACL by setting\
      \ a default object ACL on that\nbucket instead of using rsync -p. See 'gsutil\
      \ help defacl'."
    synonyms:
    - -p
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Causes POSIX attributes to be preserved when objects are copied.\n\
      With this feature enabled, gsutil rsync will copy fields\nprovided by stat.\
      \ These are the user ID of the owner, the group\nID of the owning group, the\
      \ mode (permissions) of the file, and\nthe access/modification time of the file.\
      \ For downloads, these\nattributes will only be set if the source objects were\
      \ uploaded\nwith this flag enabled.\nOn Windows, this flag will only set and\
      \ restore access time and\nmodification time. This is because Windows doesn't\
      \ have a notion\nof POSIX uid/gid/mode."
    synonyms:
    - -P
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "The -R and -r options are synonymous. Causes directories,\nbuckets,\
      \ and bucket subdirectories to be synchronized\nrecursively. If you neglect\
      \ to use this option gsutil will make\nonly the top-level directory in the source\
      \ and destination URLs\nmatch, skipping any sub-directories."
    synonyms:
    - -R
    - -r
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Skip objects with unsupported object types instead of failing.\n\
      Unsupported object types are Amazon S3 Objects in the GLACIER\nstorage class."
    synonyms:
    - -U
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Causes files/objects matching pattern to be excluded, i.e., any\n\
      matching files/objects will not be copied or deleted. Note that\nthe pattern\
      \ is a Python regular expression, not a wildcard (so,\nmatching any string ending\
      \ in \"abc\" would be specified using\n\".*abc$\" rather than \"*abc\"). Note\
      \ also that the exclude path is\nalways relative (similar to Unix rsync or tar\
      \ exclude options).\nFor example, if you run the command:\ngsutil rsync -x \"\
      data./.*\\.txt$\" dir gs://my-bucket\nit will skip the file dir/data1/a.txt.\n\
      You can use regex alternation to specify multiple exclusions,\nfor example:\n\
      gsutil rsync -x \".*\\.txt$|.*\\.jpg$\" dir gs://my-bucket\nNOTE: While it will\
      \ work to surround the regular expression with\neither single or double quotes\
      \ on Linux and MacOS, on Windows\nyou need to use double quotes.\n"
    synonyms:
    - -x
    args: !SimpleFlagArg
      name: pattern
    optional: true
  parent: *id001
  subcommands: []
  usage: []
  help_flag:
  usage_flag:
  version_flag:
  help_text: "NAME\n  rsync - Synchronize content of two buckets/directories\n\n\n\
    SYNOPSIS\n\n  gsutil rsync [-a] [-c] [-C] [-d] [-e] [-n] [-p] [-r] [-U] [-x] src_url\
    \ dst_url\n\n\n\nDESCRIPTION\n  The gsutil rsync command makes the contents under\
    \ dst_url the same as the\n  contents under src_url, by copying any missing files/objects\
    \ (or those whose\n  data has changed), and (if the -d option is specified) deleting\
    \ any extra\n  files/objects. src_url must specify a directory, bucket, or bucket\n\
    \  subdirectory. For example, to make gs://mybucket/data match the contents of\n\
    \  the local directory \"data\" you could do:\n\n    gsutil rsync -d data gs://mybucket/data\n\
    \n  To recurse into directories use the -r option:\n\n    gsutil rsync -d -r data\
    \ gs://mybucket/data\n\n  To copy only new/changed files without deleting extra\
    \ files from\n  gs://mybucket/data leave off the -d option:\n\n    gsutil rsync\
    \ -r data gs://mybucket/data\n\n  If you have a large number of objects to synchronize\
    \ you might want to use the\n  gsutil -m option, to perform parallel (multi-threaded/multi-processing)\n\
    \  synchronization:\n\n    gsutil -m rsync -d -r data gs://mybucket/data\n\n \
    \ The -m option typically will provide a large performance boost if either the\n\
    \  source or destination (or both) is a cloud URL. If both source and\n  destination\
    \ are file URLs the -m option will typically thrash the disk and\n  slow synchronization\
    \ down.\n\n  To make the local directory \"data\" the same as the contents of\n\
    \  gs://mybucket/data:\n\n    gsutil rsync -d -r gs://mybucket/data data\n\n \
    \ To make the contents of gs://mybucket2 the same as gs://mybucket1:\n\n    gsutil\
    \ rsync -d -r gs://mybucket1 gs://mybucket2\n\n  You can also mirror data across\
    \ local directories:\n\n    gsutil rsync -d -r dir1 dir2\n\n  To mirror your content\
    \ across clouds:\n\n    gsutil rsync -d -r gs://my-gs-bucket s3://my-s3-bucket\n\
    \n  Note: If you are synchronizing a large amount of data between clouds you might\n\
    \  consider setting up a\n  `Google Compute Engine <https://cloud.google.com/products/compute-engine>`_\n\
    \  account and running gsutil there. Since cross-provider gsutil data transfers\n\
    \  flow through the machine where gsutil is running, doing this can make your\n\
    \  transfer run significantly faster than running gsutil on your local\n  workstation.\n\
    \n\nBE CAREFUL WHEN USING -d OPTION!\n  The rsync -d option is very useful and\
    \ commonly used, because it provides a\n  means of making the contents of a destination\
    \ bucket or directory match those\n  of a source bucket or directory. However,\
    \ please exercise caution when you\n  use this option: It's possible to delete\
    \ large amounts of data accidentally\n  if, for example, you erroneously reverse\
    \ source and destination. For example,\n  if you meant to synchronize a local\
    \ directory from a bucket in the cloud but\n  instead run the command:\n\n   \
    \ gsutil -m rsync -r -d ./your-dir gs://your-bucket\n\n  and your-dir is currently\
    \ empty, you will quickly delete all of the objects in\n  gs://your-bucket.\n\n\
    \  You can also cause large amounts of data to be lost quickly by specifying a\n\
    \  subdirectory of the destination as the source of an rsync. For example, the\n\
    \  command:\n\n    gsutil -m rsync -r -d gs://your-bucket/data gs://your-bucket\n\
    \n  would cause most or all of the objects in gs://your-bucket to be deleted\n\
    \  (some objects may survive if there are any with names that sort lower than\n\
    \  \"data\" under gs://your-bucket/data).\n\n  In addition to paying careful attention\
    \ to the source and destination you\n  specify with the rsync command, there are\
    \ two more safety measures your can\n  take when using gsutil rsync -d:\n\n  1.\
    \ Try running the command with the rsync -n option first, to see what it\n   \
    \  would do without actually performing the operations. For example, if\n    \
    \ you run the command:\n\n       gsutil -m rsync -r -d -n gs://your-bucket/data\
    \ gs://your-bucket\n\n     it will be immediately evident that running that command\
    \ without the -n\n     option would cause many objects to be deleted.\n\n  2.\
    \ Enable object versioning in your bucket, which will allow you to restore\n \
    \    objects if you accidentally delete them. For more details see\n     \"gsutil\
    \ help versions\".\n\n\nIMPACT OF OBJECT LISTING EVENTUAL CONSISTENCY\n  The rsync\
    \ command operates by listing the source and destination URLs, and\n  then performing\
    \ copy and remove operations according to the differences\n  between these listings.\
    \ Because object listing is eventually (not strongly)\n  consistent within multi-regional\
    \ locations, if you upload new objects or\n  delete objects from a bucket in a\
    \ multi-regional location and then\n  immediately run gsutil rsync with that bucket\
    \ as the source or destination,\n  it's possible the rsync command will not see\
    \ the recent updates and thus\n  synchronize incorrectly. For example, if you\
    \ rsync to a ``US`` bucket\n  immediately after uploading to or deleting objects\
    \ from that bucket, it's\n  possible gsutil will re-upload objects that have already\
    \ been uploaded or\n  attempt to delete objects that were already deleted. A more\
    \ troublesome\n  problem can occur if you run gsutil rsync, specifying a bucket\
    \ as the\n  source immediately after uploading to or deleting objects from that\
    \ bucket.\n  In that case it's possible rsync will miss copying objects to, or\
    \ deleting\n  objects from, the destination. If this happens you can rerun the\
    \ rsync\n  operation again later (after the object listing has \"caught up\"),\
    \ to cause\n  the missing objects to be copied and extra objects to be deleted.\n\
    \n\nCHECKSUM VALIDATION AND FAILURE HANDLING\n  At the end of every upload or\
    \ download, the gsutil rsync command validates\n  that the checksum of the source\
    \ file/object matches the checksum of the\n  destination file/object. If the checksums\
    \ do not match, gsutil will delete\n  the invalid copy and print a warning message.\
    \ This very rarely happens, but\n  if it does, please contact gs-team@google.com.\n\
    \n  The rsync command will retry when failures occur, but if enough failures\n\
    \  happen during a particular copy or delete operation the command will fail.\n\
    \n  If the -C option is provided, the command will instead skip the failing\n\
    \  object and move on. At the end of the synchronization run if any failures\n\
    \  were not successfully retried, the rsync command will report the count of\n\
    \  failures, and exit with non-zero status. At this point you can run the rsync\n\
    \  command again, and it will attempt any remaining needed copy and/or delete\n\
    \  operations.\n\n  Note that there are cases where retrying will never succeed,\
    \ such as if you\n  don't have write permission to the destination bucket or if\
    \ the destination\n  path for some objects is longer than the maximum allowed\
    \ length.\n\n  For more details about gsutil's retry handling, please see\n  \"\
    gsutil help retries\".\n\n\nCHANGE DETECTION ALGORITHM\n  To determine if a file\
    \ or object has changed, gsutil rsync first checks\n  whether the file modification\
    \ time (mtime) of both the source and destination\n  is available. If mtime is\
    \ available at both source and destination, and the\n  destination mtime is different\
    \ than the source, or if the source and\n  destination file size differ, gsutil\
    \ rsync will update the destination. If the\n  source is a cloud bucket and the\
    \ destination is a local file system, and if\n  mtime is not available for the\
    \ source, gsutil rsync will use the time created\n  for the cloud object as a\
    \ substitute for mtime. Otherwise, if mtime is not\n  available for either the\
    \ source or the destination, gsutil rsync will fall\n  back to using checksums.\
    \ If the source and destination are both cloud buckets\n  with checksums available,\
    \ gsutil rsync will use these hashes instead of mtime.\n  However, gsutil rsync\
    \ will still update mtime at the destination if it is not\n  present. If the source\
    \ and destination have matching checksums and only the\n  source has an mtime,\
    \ gsutil rsync will copy the mtime to the destination. If\n  neither mtime nor\
    \ checksums are available, gsutil rsync will resort to\n  comparing file sizes.\n\
    \n  Checksums will not be available when comparing composite Google Cloud Storage\n\
    \  objects with objects at a cloud provider that does not support CRC32C (which\n\
    \  is the only checksum available for composite objects). See 'gsutil help\n \
    \ compose' for details about composite objects.\n\n\nCOPYING IN THE CLOUD AND\
    \ METADATA PRESERVATION\n  If both the source and destination URL are cloud URLs\
    \ from the same provider,\n  gsutil copies data \"in the cloud\" (i.e., without\
    \ downloading to and uploading\n  from the machine where you run gsutil). In addition\
    \ to the performance and\n  cost advantages of doing this, copying in the cloud\
    \ preserves metadata (like\n  Content-Type and Cache-Control). In contrast, when\
    \ you download data from the\n  cloud it ends up in a file, which has no associated\
    \ metadata, other than file\n  modification time (mtime). Thus, unless you have\
    \ some way to hold on to or\n  re-create that metadata, synchronizing a bucket\
    \ to a directory in the local\n  file system will not retain the metadata other\
    \ than mtime.\n\n  Note that by default, the gsutil rsync command does not copy\
    \ the ACLs of\n  objects being synchronized and instead will use the default bucket\
    \ ACL (see\n  \"gsutil help defacl\"). You can override this behavior with the\
    \ -p option (see\n  OPTIONS below).\n\n\nSLOW CHECKSUMS\n  If you find that CRC32C\
    \ checksum computation runs slowly, this is likely\n  because you don't have a\
    \ compiled CRC32c on your system. Try running:\n\n    gsutil ver -l\n\n  If the\
    \ output contains:\n\n    compiled crcmod: False\n\n  you are running a Python\
    \ library for computing CRC32C, which is much slower\n  than using the compiled\
    \ code. For information on getting a compiled CRC32C\n  implementation, see 'gsutil\
    \ help crc32c'.\n\n\nLIMITATIONS\n\n  1. The gsutil rsync command will only allow\
    \ non-negative file modification\n     times to be used in its comparisons. This\
    \ means gsutil rsync will resort to\n     using checksums for any file with a\
    \ timestamp before 1970-01-01 UTC.\n\n  2. The gsutil rsync command considers\
    \ only the current object generations in\n     the source and destination buckets\
    \ when deciding what to copy / delete. If\n     versioning is enabled in the destination\
    \ bucket then gsutil rsync's\n     overwriting or deleting objects will end up\
    \ creating versions, but the\n     command doesn't try to make the archived generations\
    \ match in the source\n     and destination buckets.\n\n  3. The gsutil rsync\
    \ command does not support copying special file types\n     such as sockets, device\
    \ files, named pipes, or any other non-standard\n     files intended to represent\
    \ an operating system resource. If you run\n     gsutil rsync on a source directory\
    \ that includes such files (for example,\n     copying the root directory on Linux\
    \ that includes /dev ), you should use\n     the -x flag to exclude these files.\
    \ Otherwise, gsutil rsync may fail or\n     hang.\n\n  4. The gsutil rsync command\
    \ copies changed files in their entirety and does\n     not employ the\n     `rsync\
    \ delta-transfer algorithm <https://rsync.samba.org/tech_report/>`_\n     to transfer\
    \ portions of a changed file. This is because cloud objects are\n     immutable\
    \ and no facility exists to read partial cloud object checksums or\n     perform\
    \ partial overwrites.\n\nOPTIONS\n  -a canned_acl Sets named canned_acl when uploaded\
    \ objects created. See\n                \"gsutil help acls\" for further details.\
    \ Note that rsync will\n                decide whether or not to perform a copy\
    \ based only on object size\n                and modification time, not current\
    \ ACL state. Also see the -p\n                option below.\n\n  -c          \
    \  Causes the rsync command to compute and compare checksums\n               \
    \ (instead of comparing mtime) for files if the size of source and\n         \
    \       destination as well as mtime (if available) match. This option\n     \
    \           increases local disk I/O and run time if either src_url or\n     \
    \           dst_url are on the local file system.\n\n  -C            If an error\
    \ occurs, continue to attempt to copy the remaining\n                files. If\
    \ errors occurred, gsutil's exit status will be non-zero\n                even\
    \ if this flag is set. This option is implicitly set when\n                running\
    \ \"gsutil -m rsync...\".  Note: -C only applies to the\n                actual\
    \ copying operation. If an error occurs while iterating\n                over\
    \ the files in the local directory (e.g., invalid Unicode\n                file\
    \ name) gsutil will print an error message and abort.\n\n  -d            Delete\
    \ extra files under dst_url not found under src_url. By\n                default\
    \ extra files are not deleted. Note: this option can\n                delete data\
    \ quickly if you specify the wrong source/destination\n                combination.\
    \ See the help section above,\n                \"BE CAREFUL WHEN USING -d OPTION!\"\
    .\n\n  -e            Exclude symlinks. When specified, symbolic links will be\n\
    \                ignored. Note that gsutil does not follow directory symlinks,\n\
    \                regardless of whether -e is specified.\n\n  -n            Causes\
    \ rsync to run in \"dry run\" mode, i.e., just outputting\n                what\
    \ would be copied or deleted without actually doing any\n                copying/deleting.\n\
    \n  -p            Causes ACLs to be preserved when objects are copied. Note that\n\
    \                rsync will decide whether or not to perform a copy based only\n\
    \                on object size and modification time, not current ACL state.\n\
    \                Thus, if the source and destination differ in size or\n     \
    \           modification time and you run gsutil rsync -p, the file will be\n\
    \                copied and ACL preserved. However, if the source and destination\n\
    \                don't differ in size or checksum but have different ACLs,\n \
    \               running gsutil rsync -p will have no effect.\n\n             \
    \   Note that this option has performance and cost implications when\n       \
    \         using the XML API, as it requires separate HTTP calls for\n        \
    \        interacting with ACLs. The performance issue can be mitigated to\n  \
    \              some degree by using gsutil -m rsync to cause parallel\n      \
    \          synchronization. Also, this option only works if you have OWNER\n \
    \               access to all of the objects that are copied.\n\n            \
    \    You can avoid the additional performance and cost of using\n            \
    \    rsync -p if you want all objects in the destination bucket to\n         \
    \       end up with the same ACL by setting a default object ACL on that\n   \
    \             bucket instead of using rsync -p. See 'gsutil help defacl'.\n\n\
    \  -P            Causes POSIX attributes to be preserved when objects are copied.\n\
    \                With this feature enabled, gsutil rsync will copy fields\n  \
    \              provided by stat. These are the user ID of the owner, the group\n\
    \                ID of the owning group, the mode (permissions) of the file, and\n\
    \                the access/modification time of the file. For downloads, these\n\
    \                attributes will only be set if the source objects were uploaded\n\
    \                with this flag enabled.\n\n                On Windows, this flag\
    \ will only set and restore access time and\n                modification time.\
    \ This is because Windows doesn't have a notion\n                of POSIX uid/gid/mode.\n\
    \n  -R, -r        The -R and -r options are synonymous. Causes directories,\n\
    \                buckets, and bucket subdirectories to be synchronized\n     \
    \           recursively. If you neglect to use this option gsutil will make\n\
    \                only the top-level directory in the source and destination URLs\n\
    \                match, skipping any sub-directories.\n\n  -U            Skip\
    \ objects with unsupported object types instead of failing.\n                Unsupported\
    \ object types are Amazon S3 Objects in the GLACIER\n                storage class.\n\
    \n  -x pattern    Causes files/objects matching pattern to be excluded, i.e.,\
    \ any\n                matching files/objects will not be copied or deleted. Note\
    \ that\n                the pattern is a Python regular expression, not a wildcard\
    \ (so,\n                matching any string ending in \"abc\" would be specified\
    \ using\n                \".*abc$\" rather than \"*abc\"). Note also that the\
    \ exclude path is\n                always relative (similar to Unix rsync or tar\
    \ exclude options).\n                For example, if you run the command:\n\n\
    \                  gsutil rsync -x \"data./.*\\.txt$\" dir gs://my-bucket\n\n\
    \                it will skip the file dir/data1/a.txt.\n\n                You\
    \ can use regex alternation to specify multiple exclusions,\n                for\
    \ example:\n\n                  gsutil rsync -x \".*\\.txt$|.*\\.jpg$\" dir gs://my-bucket\n\
    \n                NOTE: While it will work to surround the regular expression\
    \ with\n                either single or double quotes on Linux and MacOS, on\
    \ Windows\n                you need to use double quotes.\n"
  generated_using: *id002
- !Command
  command:
  - gsutil
  - signurl
  positional:
  - !Positional
    description: <https://cloud.google.com/storage/docs/authentication#generating-a-private-key>`_
    position: 0
    name: documentation.
    optional: false
  named:
  - !Flag
    description: "Specifies the HTTP method to be authorized for use\nwith the signed\
      \ url, default is GET. You may also specify\nRESUMABLE to create a signed resumable\
      \ upload start URL. When\nusing a signed URL to start a resumable upload session,\
      \ you will\nneed to specify the 'x-goog-resumable:start' header in the\nrequest\
      \ or else signature validation will fail."
    synonyms:
    - -m
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Specifies the duration that the signed url should be valid\nfor,\
      \ default duration is 1 hour.\nTimes may be specified with no suffix (default\
      \ hours), or\nwith s = seconds, m = minutes, h = hours, d = days.\nThis option\
      \ may be specified multiple times, in which case\nthe duration the link remains\
      \ valid is the sum of all the\nduration options."
    synonyms:
    - -d
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Specifies the content type for which the signed url is\nvalid for."
    synonyms:
    - -c
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: Specify the keystore password instead of prompting.
    synonyms:
    - -p
    args: !EmptyFlagArg {}
    optional: true
  parent: *id001
  subcommands: []
  usage: []
  help_flag:
  usage_flag:
  version_flag:
  help_text: "NAME\n  signurl - Create a signed url\n\n\nSYNOPSIS\n\n  gsutil signurl\
    \ [-c content_type] [-d duration] [-m http_method] \\\n      [-p password] keystore-file\
    \ url...\n\n\n\nDESCRIPTION\n  The signurl command will generate signed urls that\
    \ can be used to access\n  the specified objects without authentication for a\
    \ specific period of time.\n\n  Please see the `Signed URLs documentation\n  <https://cloud.google.com/storage/docs/access-control/signed-urls>`_\
    \ for\n  background about signed URLs.\n\n  Multiple gs:// urls may be provided\
    \ and may contain wildcards.  A signed url\n  will be produced for each provided\
    \ url, authorized\n  for the specified HTTP method and valid for the given duration.\n\
    \n  Note: Unlike the gsutil ls command, the signurl command does not support\n\
    \  operations on sub-directories. For example, if you run the command:\n\n   \
    \ gsutil signurl <private-key-file> gs://some-bucket/some-object/\n\n  The signurl\
    \ command uses the private key for a  service account (the\n  '<private-key-file>'\
    \ argument) to generate the cryptographic\n  signature for the generated URL.\
    \ The private key file must be in PKCS12\n  or JSON format. If the private key\
    \ is encrypted the signed url command will\n  prompt for the passphrase used to\
    \ protect the private key file\n  (default 'notasecret').  For more information\
    \ regarding generating a private\n  key for use with the signurl command please\
    \ see the `Authentication\n  documentation.\n  <https://cloud.google.com/storage/docs/authentication#generating-a-private-key>`_\n\
    \n  gsutil will look up information about the object \"some-object/\" (with a\n\
    \  trailing slash) inside bucket \"some-bucket\", as opposed to operating on\n\
    \  objects nested under gs://some-bucket/some-object. Unless you actually\n  have\
    \ an object with that name, the operation will fail.\n\nOPTIONS\n  -m        \
    \  Specifies the HTTP method to be authorized for use\n              with the\
    \ signed url, default is GET. You may also specify\n              RESUMABLE to\
    \ create a signed resumable upload start URL. When\n              using a signed\
    \ URL to start a resumable upload session, you will\n              need to specify\
    \ the 'x-goog-resumable:start' header in the\n              request or else signature\
    \ validation will fail.\n\n  -d          Specifies the duration that the signed\
    \ url should be valid\n              for, default duration is 1 hour.\n\n    \
    \          Times may be specified with no suffix (default hours), or\n       \
    \       with s = seconds, m = minutes, h = hours, d = days.\n\n              This\
    \ option may be specified multiple times, in which case\n              the duration\
    \ the link remains valid is the sum of all the\n              duration options.\n\
    \n  -c          Specifies the content type for which the signed url is\n     \
    \         valid for.\n\n  -p          Specify the keystore password instead of\
    \ prompting.\n\nUSAGE\n  Create a signed url for downloading an object valid for\
    \ 10 minutes:\n\n    gsutil signurl -d 10m <private-key-file> gs://<bucket>/<object>\n\
    \n  Create a signed url, valid for one hour, for uploading a plain text\n  file\
    \ via HTTP PUT:\n\n    gsutil signurl -m PUT -d 1h -c text/plain <private-key-file>\
    \ \\\n        gs://<bucket>/<obj>\n\n  To construct a signed URL that allows anyone\
    \ in possession of\n  the URL to PUT to the specified bucket for one day, creating\n\
    \  an object of Content-Type image/jpg, run:\n\n    gsutil signurl -m PUT -d 1d\
    \ -c image/jpg <private-key-file> \\\n        gs://<bucket>/<obj>\n\n  To construct\
    \ a signed URL that allows anyone in possession of\n  the URL to POST a resumable\
    \ upload to the specified bucket for one day,\n  creating an object of Content-Type\
    \ image/jpg, run:\n\n    gsutil signurl -m RESUMABLE -d 1d -c image/jpg <private-key-file>\
    \ \\\n        gs://bucket/<obj>\n"
  generated_using: *id002
- !Command
  command:
  - gsutil
  - acl
  positional: []
  named:
  - !Flag
    description: :<perm>
    synonyms:
    - -u
    args: !SimpleFlagArg
      name: id|email
    optional: true
  - !Flag
    description: :<perm>
    synonyms:
    - -g
    args: !SimpleFlagArg
      name: id|email|domain|All|AllAuth
    optional: true
  - !Flag
    description: -<project number>
    synonyms:
    - -p
    args: !SimpleFlagArg
      name: viewers|editors|owners
    optional: true
  - !Flag
    description: <id|email|domain|All|AllAuth|<viewers|editors|owners>-<project number>>
    synonyms:
    - -d
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Performs \"acl set\" request recursively, to all objects under\n\
      the specified URL."
    synonyms:
    - -R
    - -r
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: Performs "acl set" request on all object versions.
    synonyms:
    - -a
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Normally gsutil stops at the first error. The -f option causes\n\
      it to continue when it encounters errors. If some of the ACLs\ncouldn't be set,\
      \ gsutil's exit status will be non-zero even if\nthis flag is set. This option\
      \ is implicitly set when running\n\"gsutil -m acl...\"."
    synonyms:
    - -f
    args: !EmptyFlagArg {}
    optional: true
  parent: *id001
  subcommands: []
  usage: []
  help_flag:
  usage_flag:
  version_flag:
  help_text: "NAME\n  acl - Get, set, or change bucket and/or object ACLs\n\n\nSYNOPSIS\n\
    \  gsutil acl set [-f] [-r] [-a] file-or-canned_acl_name url...\n  gsutil acl\
    \ get url\n  gsutil acl ch [-f] [-r] -u|-g|-d|-p <grant>... url...\n\n  where\
    \ each <grant> is one of the following forms:\n\n    -u <id|email>:<perm>\n  \
    \  -g <id|email|domain|All|AllAuth>:<perm>\n    -p <viewers|editors|owners>-<project\
    \ number>\n    -d <id|email|domain|All|AllAuth|<viewers|editors|owners>-<project\
    \ number>>\n\n\n\nDESCRIPTION\n  The acl command has three sub-commands:\n\nGET\n\
    \  The \"acl get\" command gets the ACL text for a bucket or object, which you\
    \ can\n  save and edit for the acl set command.\n\n\nSET\n  The \"acl set\" command\
    \ allows you to set an Access Control List on one or\n  more buckets and objects.\
    \ The simplest way to use it is to specify one of\n  the canned ACLs, e.g.,:\n\
    \n    gsutil acl set private gs://bucket\n\n  If you want to make an object or\
    \ bucket publicly readable or writable, it is\n  recommended to use \"acl ch\"\
    , to avoid accidentally removing OWNER permissions.\n  See \"gsutil help acl ch\"\
    \ for details.\n\n  See \"gsutil help acls\" for a list of all canned ACLs.\n\n\
    \  If you want to define more fine-grained control over your data, you can\n \
    \ retrieve an ACL using the \"acl get\" command, save the output to a file, edit\n\
    \  the file, and then use the \"acl set\" command to set that ACL on the buckets\n\
    \  and/or objects. For example:\n\n    gsutil acl get gs://bucket/file.txt > acl.txt\n\
    \n  Make changes to acl.txt such as adding an additional grant, then:\n\n    gsutil\
    \ acl set acl.txt gs://cats/file.txt\n\n  Note that you can set an ACL on multiple\
    \ buckets or objects at once,\n  for example:\n\n    gsutil acl set acl.txt gs://bucket/*.jpg\n\
    \n  If you have a large number of ACLs to update you might want to use the\n \
    \ gsutil -m option, to perform a parallel (multi-threaded/multi-processing)\n\
    \  update:\n\n    gsutil -m acl set acl.txt gs://bucket/*.jpg\n\n  Note that multi-threading/multi-processing\
    \ is only done when the named URLs\n  refer to objects, which happens either if\
    \ you name specific objects or \n  if you enumerate objects by using an object\
    \ wildcard or specifying\n  the acl -r flag.\n\n\nSET OPTIONS\n  The \"set\" sub-command\
    \ has the following options\n\n    -R, -r      Performs \"acl set\" request recursively,\
    \ to all objects under\n                the specified URL.\n\n    -a         \
    \ Performs \"acl set\" request on all object versions.\n\n    -f          Normally\
    \ gsutil stops at the first error. The -f option causes\n                it to\
    \ continue when it encounters errors. If some of the ACLs\n                couldn't\
    \ be set, gsutil's exit status will be non-zero even if\n                this\
    \ flag is set. This option is implicitly set when running\n                \"\
    gsutil -m acl...\".\n\n\nCH\n  The \"acl ch\" (or \"acl change\") command updates\
    \ access control lists, similar\n  in spirit to the Linux chmod command. You can\
    \ specify multiple access grant\n  additions and deletions in a single command\
    \ run; all changes will be made\n  atomically to each object in turn. For example,\
    \ if the command requests\n  deleting one grant and adding a different grant,\
    \ the ACLs being updated will\n  never be left in an intermediate state where\
    \ one grant has been deleted but\n  the second grant not yet added. Each change\
    \ specifies a user or group grant\n  to add or delete, and for grant additions,\
    \ one of R, W, O (for the\n  permission to be granted). A more formal description\
    \ is provided in a later\n  section; below we provide examples.\n\nCH EXAMPLES\n\
    \  Examples for \"ch\" sub-command:\n  \n  Grant anyone on the internet READ access\
    \ to the object example-object:\n  \n    gsutil acl ch -u AllUsers:R gs://example-bucket/example-object\n\
    \n  NOTE: By default, publicly readable objects are served with a Cache-Control\n\
    \  header allowing such objects to be cached for 3600 seconds. If you need to\n\
    \  ensure that updates become visible immediately, you should set a\n  Cache-Control\
    \ header of \"Cache-Control:private, max-age=0, no-transform\" on\n  such objects.\
    \ For help doing this, see \"gsutil help setmeta\".\n\n  Grant anyone on the internet\
    \ WRITE access to the bucket example-bucket\n  (WARNING: this is not recommended\
    \ as you will be responsible for the content):\n\n    gsutil acl ch -u AllUsers:W\
    \ gs://example-bucket\n    \n  Grant the user john.doe@example.com WRITE access\
    \ to the bucket\n  example-bucket:\n\n    gsutil acl ch -u john.doe@example.com:WRITE\
    \ gs://example-bucket\n\n  Grant the group admins@example.com OWNER access to\
    \ all jpg files in\n  the top level of example-bucket:\n\n    gsutil acl ch -g\
    \ admins@example.com:O gs://example-bucket/*.jpg\n\n  Grant the owners of project\
    \ example-project WRITE access to the bucket\n  example-bucket:\n\n    gsutil\
    \ acl ch -p owners-example-project:W gs://example-bucket\n\n  NOTE: You can replace\
    \ 'owners' with 'viewers' or 'editors' to grant access\n  to a project's viewers/editors\
    \ respectively.\n\n  Remove access to the bucket example-bucket for the owners\
    \ of project number\n  12345:\n\n    gsutil acl ch -d owners-12345 gs://example-bucket\n\
    \n  Note that removing a project requires you to reference the project by\n  its\
    \ number (which you can see with the acl get command) as opposed to its\n  project\
    \ ID string.\n\n  Grant the user with the specified canonical ID READ access to\
    \ all objects\n  in example-bucket that begin with folder/:\n\n    gsutil acl\
    \ ch -r \\\n      -u 84fac329bceSAMPLE777d5d22b8SAMPLE785ac2SAMPLE2dfcf7c4adf34da46:R\
    \ \\\n      gs://example-bucket/folder/\n\n  Grant the service account foo@developer.gserviceaccount.com\
    \ WRITE access to\n  the bucket example-bucket:\n\n    gsutil acl ch -u foo@developer.gserviceaccount.com:W\
    \ gs://example-bucket\n\n  Grant all users from the `Google Apps\n  <https://www.google.com/work/apps/business/>`_\
    \ domain my-domain.org READ\n  access to the bucket gcs.my-domain.org:\n\n   \
    \ gsutil acl ch -g my-domain.org:R gs://gcs.my-domain.org\n\n  Remove any current\
    \ access by john.doe@example.com from the bucket\n  example-bucket:\n\n    gsutil\
    \ acl ch -d john.doe@example.com gs://example-bucket\n\n  If you have a large\
    \ number of objects to update, enabling multi-threading\n  with the gsutil -m\
    \ flag can significantly improve performance. The\n  following command adds OWNER\
    \ for admin@example.org using\n  multi-threading:\n\n    gsutil -m acl ch -r -u\
    \ admin@example.org:O gs://example-bucket\n\n  Grant READ access to everyone from\
    \ my-domain.org and to all authenticated\n  users, and grant OWNER to admin@mydomain.org,\
    \ for the buckets\n  my-bucket and my-other-bucket, with multi-threading enabled:\n\
    \n    gsutil -m acl ch -r -g my-domain.org:R -g AllAuth:R \\\n      -u admin@mydomain.org:O\
    \ gs://my-bucket/ gs://my-other-bucket\n\nCH ROLES\n  You may specify the following\
    \ roles with either their shorthand or\n  their full name:\n\n    R: READ\n  \
    \  W: WRITE\n    O: OWNER\n\nCH ENTITIES\n  There are four different entity types:\
    \ Users, Groups, All Authenticated Users,\n  and All Users.\n\n  Users are added\
    \ with -u and a plain ID or email address, as in\n  \"-u john-doe@gmail.com:r\"\
    . Note: Service Accounts are considered to be users.\n\n  Groups are like users,\
    \ but specified with the -g flag, as in\n  \"-g power-users@example.com:fc\".\
    \ Groups may also be specified as a full\n  domain, as in \"-g my-company.com:r\"\
    .\n\n  AllAuthenticatedUsers and AllUsers are specified directly, as\n  in \"\
    -g AllUsers:R\" or \"-g AllAuthenticatedUsers:O\". These are case\n  insensitive,\
    \ and may be shortened to \"all\" and \"allauth\", respectively.\n\n  Removing\
    \ roles is specified with the -d flag and an ID, email\n  address, domain, or\
    \ one of AllUsers or AllAuthenticatedUsers.\n\n  Many entities' roles can be specified\
    \ on the same command line, allowing\n  bundled changes to be executed in a single\
    \ run. This will reduce the number of\n  requests made to the server.\n\nCH OPTIONS\n\
    \  The \"ch\" sub-command has the following options\n\n    -d          Remove\
    \ all roles associated with the matching entity.\n\n    -f          Normally gsutil\
    \ stops at the first error. The -f option causes\n                it to continue\
    \ when it encounters errors. With this option the\n                gsutil exit\
    \ status will be 0 even if some ACLs couldn't be\n                changed.\n\n\
    \    -g          Add or modify a group entity's role.\n\n    -p          Add or\
    \ modify a project viewers/editors/owners role.\n\n    -R, -r      Performs acl\
    \ ch request recursively, to all objects under the\n                specified\
    \ URL.\n\n    -u          Add or modify a user entity's role.\n"
  generated_using: *id002
- !Command
  command:
  - gsutil
  - label
  positional:
  - !Positional
    description: SET
    position: 0
    name: command.
    optional: false
  named:
  - !Flag
    description: :<value>
    synonyms:
    - -l
    args: !SimpleFlagArg
      name: key
    optional: true
  - !Flag
    description: ''
    synonyms:
    - -d
    args: !SimpleFlagArg
      name: key
    optional: true
  parent: *id001
  subcommands: []
  usage: []
  help_flag:
  usage_flag:
  version_flag:
  help_text: "NAME\n  label - Get, set, or change the label configuration of a bucket.\n\
    \n\nSYNOPSIS\n  gsutil label set label-json-file url...\n  gsutil label get url\n\
    \  gsutil label ch <label_modifier>... url...\n\n  where each <label_modifier>\
    \ is one of the following forms:\n\n    -l <key>:<value>\n    -d <key>\n\n\n\n\
    DESCRIPTION\n  Gets, sets, or changes the label configuration (also called the\
    \ tagging\n  configuration by other storage providers) of one or more buckets.\
    \ An example\n  label JSON document looks like the following:\n\n    {\n     \
    \ \"your_label_key\": \"your_label_value\",\n      \"your_other_label_key\": \"\
    your_other_label_value\"\n    }\n\n  The label command has three sub-commands:\n\
    \nGET\n  The \"label get\" command gets the\n  `labels <https://cloud.google.com/storage/docs/key-terms#bucket-labels>`_\n\
    \  applied to a bucket, which you can save and edit for use with the \"label set\"\
    \n  command.\n\nSET\n  The \"label set\" command allows you to set the labels\
    \ on one or more\n  buckets. You can retrieve a bucket's labels using the \"label\
    \ get\" command,\n  save the output to a file, edit the file, and then use the\
    \ \"label set\"\n  command to apply those labels to the specified bucket(s). For\n\
    \  example:\n\n    gsutil label get gs://bucket > labels.json\n\n  Make changes\
    \ to labels.json, such as adding an additional label, then:\n\n    gsutil label\
    \ set labels.json gs://example-bucket\n\n  Note that you can set these labels\
    \ on multiple buckets at once:\n\n    gsutil label set labels.json gs://bucket-foo\
    \ gs://bucket-bar\n\nCH\n  The \"label ch\" command updates a bucket's label configuration,\
    \ applying the\n  label changes specified by the -l and -d flags. You can specify\
    \ multiple\n  label changes in a single command run; all changes will be made\
    \ atomically to\n  each bucket.\n\nCH EXAMPLES\n  Examples for \"ch\" sub-command:\n\
    \n  Add the label \"key-foo:value-bar\" to the bucket \"example-bucket\":\n\n\
    \    gsutil label ch -l key-foo:value-bar gs://example-bucket\n\n  Change the\
    \ above label to have a new value:\n\n    gsutil label ch -l key-foo:other-value\
    \ gs://example-bucket\n\n  Add a new label and delete the old one from above:\n\
    \n    gsutil label ch -l new-key:new-value -d key-foo gs://example-bucket\n\n\
    CH OPTIONS\n  The \"ch\" sub-command has the following options\n\n    -l     \
    \     Add or update a label with the specified key and value.\n\n    -d      \
    \    Remove the label with the specified key.\n"
  generated_using: *id002
- !Command
  command:
  - gsutil
  - iam
  positional:
  - !Positional
    description: CH EXAMPLES
    position: 0
    name: efficiently.
    optional: false
  named:
  - !Flag
    description: "Performs \"iam set\" recursively to all objects under the\nspecified\
      \ bucket."
    synonyms:
    - -R
    - -r
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: Performs "iam set" request on all object versions.
    synonyms:
    - -a
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Performs the precondition check on each object with the\nspecified\
      \ etag before setting the policy."
    synonyms:
    - -e
    args: !SimpleFlagArg
      name: etag
    optional: true
  - !Flag
    description: "Default gsutil error handling is fail-fast. This flag\nchanges the\
      \ request to fail-silent mode. This is implicitly\nset when invoking the gsutil\
      \ -m option."
    synonyms:
    - -f
    args: !EmptyFlagArg {}
    optional: true
  parent: *id001
  subcommands: []
  usage: []
  help_flag:
  usage_flag:
  version_flag:
  help_text: "NAME\n  iam - Get, set, or change bucket and/or object IAM permissions.\n\
    \n\nSYNOPSIS\n  gsutil iam set [-afRr] [-e <etag>] file url ...\n  gsutil iam\
    \ get url\n  gsutil iam ch [-fRr] binding ...\n\n  where each binding is of the\
    \ form:\n\n      [-d] (\"user\"|\"serviceAccount\"|\"domain\"|\"group\"):id:role[,...]\n\
    \      [-d] (\"allUsers\"|\"allAuthenticatedUsers\"):role[,...]\n      -d (\"\
    user\"|\"serviceAccount\"|\"domain\"|\"group\"):id\n      -d (\"allUsers\"|\"\
    allAuthenticatedUsers\")\n\n\n\nDESCRIPTION\n  The iam command has three sub-commands:\n\
    \nGET\n  The \"iam get\" command gets the IAM policy for a bucket or object, which\
    \ you\n  can save and edit for use with the \"iam set\" command.\n\n  For example:\n\
    \n    gsutil iam get gs://example > bucket_iam.txt\n    gsutil iam get gs://example/important.txt\
    \ > object_iam.txt\n\n  The IAM policy returned by \"iam get\" includes the etag\
    \ of the IAM policy and\n  will be used in the precondition check for \"iam set\"\
    , unless the etag is\n  overridden by setting the \"iam set\" -e option.\n\n\n\
    SET\n  The \"iam set\" command sets the IAM policy for one or more buckets and\
    \ / or\n  objects. It overwrites the current IAM policy that exists on a bucket\
    \ (or\n  object) with the policy specified in the input file. The \"iam set\"\
    \ command\n  takes as input a file with an IAM policy in the format of the output\n\
    \  generated by \"iam get\".\n\n  The \"iam ch\" command can be used to edit an\
    \ existing policy. It works\n  correctly in the presence of concurrent updates.\
    \ You may also do this\n  manually by using the -e flag and overriding the etag\
    \ returned in \"iam get\".\n  Specifying -e with an empty string (i.e. \"gsutil\
    \ iam set -e '' ...\") will\n  instruct gsutil to skip the precondition check\
    \ when setting the IAM policy.\n\n  If you wish to set an IAM policy on a large\
    \ number of objects, you may want\n  to use the gsutil -m option for concurrent\
    \ processing. The following command\n  will apply iam.txt to all objects in the\
    \ \"cats\" bucket.\n\n    gsutil -m iam set -r iam.txt gs://cats\n\n  Note that\
    \ only object-level IAM applications are parallelized; you do not\n  gain any\
    \ additional performance when applying an IAM policy to a large\n  number of buckets\
    \ with the -m flag.\n\nSET OPTIONS\n  The \"set\" sub-command has the following\
    \ options\n\n    -R, -r      Performs \"iam set\" recursively to all objects under\
    \ the\n                specified bucket.\n\n    -a          Performs \"iam set\"\
    \ request on all object versions.\n\n    -e <etag>   Performs the precondition\
    \ check on each object with the\n                specified etag before setting\
    \ the policy.\n\n    -f          Default gsutil error handling is fail-fast. This\
    \ flag\n                changes the request to fail-silent mode. This is implicitly\n\
    \                set when invoking the gsutil -m option.\n\n\nCH\n  The \"iam\
    \ ch\" command incrementally updates IAM policies. You may specify\n  multiple\
    \ access grants and removals in a single command invocation, which\n  will be\
    \ batched and applied as a whole to each url via an IAM patch.\n  The patch will\
    \ be constructed by applying each access grant or removal in the\n  order in which\
    \ they appear in the command line arguments. Each access change\n  specifies a\
    \ member and the role that will be either granted or revoked.\n\n  The gsutil\
    \ -m option may be set to handle object-level operations more\n  efficiently.\n\
    \nCH EXAMPLES\n  Examples for the \"ch\" sub-command:\n\n  To grant a single role\
    \ to a single member for some targets:\n\n    gsutil iam ch user:john.doe@example.com:objectCreator\
    \ gs://ex-bucket\n\n  To make a bucket's objects publically readable:\n\n    gsutil\
    \ iam ch allUsers:objectViewer gs://ex-bucket\n\n  To grant multiple bindings\
    \ to a bucket:\n\n    gsutil iam ch user:john.doe@example.com:objectCreator \\\
    \n                  domain:www.my-domain.org:objectViewer gs://ex-bucket\n\n \
    \ To specify more than one role for a particular member:\n\n    gsutil iam ch\
    \ user:john.doe@example.com:objectCreator,objectViewer \\\n                  gs://ex-bucket\n\
    \n  To apply a grant and simultaneously remove a binding to a bucket:\n\n    gsutil\
    \ iam ch -d group:readers@example.com:legacyBucketReader \\\n                \
    \  group:viewers@example.com:objectViewer gs://ex-bucket\n\n  To remove a user\
    \ from all roles on a bucket:\n\n    gsutil iam ch -d user:john.doe@example.com\
    \ gs://ex-bucket\n\nCH OPTIONS\n  The \"ch\" sub-command has the following options\n\
    \n    -R, -r      Performs \"iam ch\" recursively to all objects under the\n \
    \               specified bucket.\n\n    -f          Default gsutil error handling\
    \ is fail-fast. This flag\n                changes the request to fail-silent\
    \ mode. This is implicitly\n                set when invoking the gsutil -m option.\n"
  generated_using: *id002
- !Command
  command:
  - gsutil
  - version
  positional: []
  named:
  - !Flag
    description: "Prints additional information, such as the version of Python\nbeing\
      \ used, the version of the Boto library, a checksum of the\ncode, the path to\
      \ gsutil, and the path to gsutil's configuration\nfile.\n"
    synonyms:
    - -l
    args: !EmptyFlagArg {}
    optional: true
  parent: *id001
  subcommands: []
  usage: []
  help_flag:
  usage_flag:
  version_flag:
  help_text: "NAME\n  version - Print version info about gsutil\n\n\nSYNOPSIS\n\n\
    \  gsutil version\n\n\n\nDESCRIPTION\n  Prints information about the version of\
    \ gsutil.\n\nOPTIONS\n  -l          Prints additional information, such as the\
    \ version of Python\n              being used, the version of the Boto library,\
    \ a checksum of the\n              code, the path to gsutil, and the path to gsutil's\
    \ configuration\n              file.\n"
  generated_using: *id002
- !Command
  command:
  - gsutil
  - rm
  positional:
  - !Positional
    description: You can also use the -r option to specify recursive object deletion.
      Thus, for
    position: 0
    name: subdirectories.
    optional: false
  named:
  - !Flag
    description: "Continues silently (without printing error messages) despite\nerrors\
      \ when removing multiple objects. If some of the objects\ncould not be removed,\
      \ gsutil's exit status will be non-zero even\nif this flag is set. Execution\
      \ will still halt if an inaccessible\nbucket is encountered. This option is\
      \ implicitly set when running\n\"gsutil -m rm ...\"."
    synonyms:
    - -f
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Causes gsutil to read the list of objects to remove from stdin.\n\
      This allows you to run a program that generates the list of\nobjects to remove."
    synonyms:
    - -I
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "The -R and -r options are synonymous. Causes bucket or bucket\n\
      subdirectory contents (all objects and subdirectories that it\ncontains) to\
      \ be removed recursively. If used with a bucket-only\nURL (like gs://bucket),\
      \ after deleting objects and subdirectories\ngsutil will delete the bucket.\
      \ This option implies the -a option\nand will delete all object versions."
    synonyms:
    - -R
    - -r
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: Delete all versions of an object.
    synonyms:
    - -a
    args: !EmptyFlagArg {}
    optional: true
  parent: *id001
  subcommands: []
  usage: []
  help_flag:
  usage_flag:
  version_flag:
  help_text: "NAME\n  rm - Remove objects\n\n\nSYNOPSIS\n\n  gsutil rm [-f] [-r] url...\n\
    \  gsutil rm [-f] [-r] -I\n\n\n\nDESCRIPTION\n  The gsutil rm command removes\
    \ objects.\n  For example, the command:\n\n    gsutil rm gs://bucket/subdir/*\n\
    \n  will remove all objects in gs://bucket/subdir, but not in any of its\n  sub-directories.\
    \ In contrast:\n\n    gsutil rm gs://bucket/subdir/**\n\n  will remove all objects\
    \ under gs://bucket/subdir or any of its\n  subdirectories.\n\n  You can also\
    \ use the -r option to specify recursive object deletion. Thus, for\n  example,\
    \ either of the following two commands will remove gs://bucket/subdir\n  and all\
    \ objects and subdirectories under it:\n\n    gsutil rm gs://bucket/subdir**\n\
    \    gsutil rm -r gs://bucket/subdir\n\n  The -r option will also delete all object\
    \ versions in the subdirectory for\n  versioning-enabled buckets, whereas the\
    \ ** command will only delete the live\n  version of each object in the subdirectory.\n\
    \n  Running gsutil rm -r on a bucket will delete all versions of all objects in\n\
    \  the bucket, and then delete the bucket:\n\n    gsutil rm -r gs://bucket\n\n\
    \  If you want to delete all objects in the bucket, but not the bucket itself,\n\
    \  this command will work:\n\n    gsutil rm gs://bucket/**\n\n  If you have a\
    \ large number of objects to remove you might want to use the\n  gsutil -m option,\
    \ to perform parallel (multi-threaded/multi-processing)\n  removes:\n\n    gsutil\
    \ -m rm -r gs://my_bucket/subdir\n\n  You can pass a list of URLs (one per line)\
    \ to remove on stdin instead of as\n  command line arguments by using the -I option.\
    \ This allows you to use gsutil\n  in a pipeline to remove objects identified\
    \ by a program, such as:\n\n    some_program | gsutil -m rm -I\n\n  The contents\
    \ of stdin can name cloud URLs and wildcards of cloud URLs.\n\n  Note that gsutil\
    \ rm will refuse to remove files from the local\n  file system. For example this\
    \ will fail:\n\n    gsutil rm *.txt\n\n  WARNING: Object removal cannot be undone.\
    \ Google Cloud Storage is designed\n  to give developers a high amount of flexibility\
    \ and control over their data,\n  and Google maintains strict controls over the\
    \ processing and purging of\n  deleted data. To protect yourself from mistakes,\
    \ you can configure object\n  versioning on your bucket(s). See 'gsutil help versions'\
    \ for details.\n\n\nDATA RESTORATION FROM ACCIDENTAL DELETION OR OVERWRITES\n\
    Google Cloud Storage does not provide support for restoring data lost\nor overwritten\
    \ due to customer errors. If you have concerns that your\napplication software\
    \ (or your users) may at some point erroneously delete or\noverwrite data, you\
    \ can protect yourself from that risk by enabling Object\nVersioning (see \"gsutil\
    \ help versioning\"). Doing so increases storage costs,\nwhich can be partially\
    \ mitigated by configuring Lifecycle Management to delete\nolder object versions\
    \ (see \"gsutil help lifecycle\").\n\n\nOPTIONS\n  -f          Continues silently\
    \ (without printing error messages) despite\n              errors when removing\
    \ multiple objects. If some of the objects\n              could not be removed,\
    \ gsutil's exit status will be non-zero even\n              if this flag is set.\
    \ Execution will still halt if an inaccessible\n              bucket is encountered.\
    \ This option is implicitly set when running\n              \"gsutil -m rm ...\"\
    .\n\n  -I          Causes gsutil to read the list of objects to remove from stdin.\n\
    \              This allows you to run a program that generates the list of\n \
    \             objects to remove.\n\n  -R, -r      The -R and -r options are synonymous.\
    \ Causes bucket or bucket\n              subdirectory contents (all objects and\
    \ subdirectories that it\n              contains) to be removed recursively. If\
    \ used with a bucket-only\n              URL (like gs://bucket), after deleting\
    \ objects and subdirectories\n              gsutil will delete the bucket. This\
    \ option implies the -a option\n              and will delete all object versions.\n\
    \n  -a          Delete all versions of an object.\n"
  generated_using: *id002
- !Command
  command:
  - gsutil
  - cat
  positional: []
  named:
  - !Flag
    description: "Causes gsutil to output just the specified byte range of the\nobject.\
      \ Ranges are can be of these forms:\nstart-end (e.g., -r 256-5939)\nstart- \
      \   (e.g., -r 256-)\n-numbytes (e.g., -r -5)\nwhere offsets start at 0, start-end\
      \ means to return bytes start\nthrough end (inclusive), start- means to return\
      \ bytes start\nthrough the end of the object, and -numbytes means to return\
      \ the\nlast numbytes of the object. For example:\ngsutil cat -r 256-939 gs://bucket/object\n\
      returns bytes 256 through 939, while:\ngsutil cat -r -5 gs://bucket/object\n\
      returns the final 5 bytes of the object.\n"
    synonyms:
    - -r
    args: !SimpleFlagArg
      name: range
    optional: true
  parent: *id001
  subcommands: []
  usage: []
  help_flag: !Flag
    description: "Prints short header for each object. For example:\ngsutil cat -h\
      \ gs://bucket/meeting_notes/2012_Feb/*.txt\nThis would print a header with the\
      \ object name before the contents\nof each text object that matched the wildcard."
    synonyms:
    - -h
    args: !EmptyFlagArg {}
    optional: true
  usage_flag:
  version_flag:
  help_text: "NAME\n  cat - Concatenate object content to stdout\n\n\nSYNOPSIS\n\n\
    \  gsutil cat [-h] url...\n\n\n\nDESCRIPTION\n  The cat command outputs the contents\
    \ of one or more URLs to stdout.\n  While the cat command does not compute a checksum,\
    \ it is otherwise\n  equivalent to doing:\n\n    gsutil cp url... -\n\n  (The\
    \ final '-' causes gsutil to stream the output to stdout.)\n\n\nWARNING: DATA\
    \ INTEGRITY CHECKING NOT DONE\n  The gsutil cat command does not compute a checksum\
    \ of the downloaded data.\n  Therefore, we recommend that users either perform\
    \ their own validation of the\n  output of gsutil cat or use gsutil cp or rsync\
    \ (both of which perform\n  integrity checking automatically).\n\n\nOPTIONS\n\
    \  -h          Prints short header for each object. For example:\n\n         \
    \       gsutil cat -h gs://bucket/meeting_notes/2012_Feb/*.txt\n\n           \
    \   This would print a header with the object name before the contents\n     \
    \         of each text object that matched the wildcard.\n\n  -r range    Causes\
    \ gsutil to output just the specified byte range of the\n              object.\
    \ Ranges are can be of these forms:\n\n                start-end (e.g., -r 256-5939)\n\
    \                start-    (e.g., -r 256-)\n                -numbytes (e.g., -r\
    \ -5)\n\n              where offsets start at 0, start-end means to return bytes\
    \ start\n              through end (inclusive), start- means to return bytes start\n\
    \              through the end of the object, and -numbytes means to return the\n\
    \              last numbytes of the object. For example:\n\n                gsutil\
    \ cat -r 256-939 gs://bucket/object\n\n              returns bytes 256 through\
    \ 939, while:\n\n                gsutil cat -r -5 gs://bucket/object\n\n     \
    \         returns the final 5 bytes of the object.\n"
  generated_using: *id002
- !Command
  command:
  - gsutil
  - cp
  positional:
  - !Positional
    description: Copies spanning locations and/or storage classes cause data to be
      rewritten
    position: 0
    name: metadata.
    optional: false
  - !Positional
    description: Note that by default, the gsutil cp command does not copy the object
    position: 0
    name: identical.
    optional: false
  - !Positional
    description: 'obj:'
    position: 0
    name: Hashing
    optional: false
  - !Positional
    description: 'gs://your-bucket/obj:                                182 b/182 B'
    position: 0
    name: Uploading
    optional: false
  - !Positional
    description: This feature is only available for Google Cloud Storage objects because
      it
    position: 0
    name: operation.
    optional: false
  - !Positional
    description: -I             Causes gsutil to read the list of files or objects
      to copy from
    position: 0
    name: copied.
    optional: false
  named:
  - !Flag
    description: "Sets named canned_acl when uploaded objects created. See\n\"gsutil\
      \ help acls\" for further details."
    synonyms:
    - -a
    args: !SimpleFlagArg
      name: canned_acl
    optional: true
  - !Flag
    description: "Copy all source versions from a source buckets/folders.\nIf not\
      \ set, only the live version of each source object is\ncopied. Note: this option\
      \ is only useful when the destination\nbucket has versioning enabled."
    synonyms:
    - -A
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "If an error occurs, continue to attempt to copy the remaining\n\
      files. If any copies were unsuccessful, gsutil's exit status\nwill be non-zero\
      \ even if this flag is set. This option is\nimplicitly set when running \"gsutil\
      \ -m cp...\". Note: -c only\napplies to the actual copying operation. If an\
      \ error occurs\nwhile iterating over the files in the local directory (e.g.,\n\
      invalid Unicode file name) gsutil will print an error message\nand abort."
    synonyms:
    - -c
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Copy in \"daisy chain\" mode, i.e., copying between two buckets\n\
      by hooking a download to an upload, via the machine where\ngsutil is run. This\
      \ stands in contrast to the default, where\ndata are copied between two buckets\
      \ \"in the cloud\", i.e.,\nwithout needing to copy via the machine where gsutil\
      \ runs.\nBy default, a \"copy in the cloud\" when the source is a\ncomposite\
      \ object will retain the composite nature of the\nobject. However, Daisy chain\
      \ mode can be used to change a\ncomposite object into a non-composite object.\
      \ For example:\ngsutil cp -D -p gs://bucket/obj gs://bucket/obj_tmp\ngsutil\
      \ mv -p gs://bucket/obj_tmp gs://bucket/obj\nNote: Daisy chain mode is automatically\
      \ used when copying\nbetween providers (e.g., to copy data from Google Cloud\
      \ Storage\nto another provider)."
    synonyms:
    - -D
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: Exclude symlinks. When specified, symbolic links will not be
    synonyms:
    - -e
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Outputs a manifest log file with detailed information about\neach\
      \ item that was copied. This manifest contains the following\ninformation for\
      \ each item:\n- Source path.\n- Destination path.\n- Source size.\n- Bytes transferred.\n\
      - MD5 hash.\n- UTC date and time transfer was started in ISO 8601 format.\n\
      - UTC date and time transfer was completed in ISO 8601 format.\n- Upload id,\
      \ if a resumable upload was performed.\n- Final result of the attempted transfer,\
      \ success or failure.\n- Failure details, if any.\nIf the log file already exists,\
      \ gsutil will use the file as an\ninput to the copy process, and will also append\
      \ log items to\nthe existing file. Files/objects that are marked in the\nexisting\
      \ log file as having been successfully copied (or\nskipped) will be ignored.\
      \ Files/objects without entries will be\ncopied and ones previously marked as\
      \ unsuccessful will be\nretried. This can be used in conjunction with the -c\
      \ option to\nbuild a script that copies a large number of objects reliably,\n\
      using a bash script like the following:\nuntil gsutil cp -c -L cp.log -r ./dir\
      \ gs://bucket; do\nsleep 1\ndone\nThe -c option will cause copying to continue\
      \ after failures\noccur, and the -L option will allow gsutil to pick up where\
      \ it\nleft off without duplicating work. The loop will continue\nrunning as\
      \ long as gsutil exits with a non-zero status (such a\nstatus indicates there\
      \ was at least one failure during the\ngsutil run).\nNote: If you're trying\
      \ to synchronize the contents of a\ndirectory and a bucket (or two buckets),\
      \ see\n\"gsutil help rsync\"."
    synonyms:
    - -L
    args: !SimpleFlagArg
      name: file
    optional: true
  - !Flag
    description: "No-clobber. When specified, existing files or objects at the\ndestination\
      \ will not be overwritten. Any items that are skipped\nby this option will be\
      \ reported as being skipped. This option\nwill perform an additional GET request\
      \ to check if an item\nexists before attempting to upload the data. This will\
      \ save\nretransmitting data, but the additional HTTP requests may make\nsmall\
      \ object transfers slower and more expensive."
    synonyms:
    - -n
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Causes ACLs to be preserved when copying in the cloud. Note\nthat\
      \ this option has performance and cost implications when\nusing  the XML API,\
      \ as it requires separate HTTP calls for\ninteracting with ACLs. (There are\
      \ no such performance or cost\nimplications when using the -p option with the\
      \ JSON API.) The\nperformance issue can be mitigated to some degree by using\n\
      gsutil -m cp to cause parallel copying. Note that this option\nonly works if\
      \ you have OWNER access to all of the objects that\nare copied.\nYou can avoid\
      \ the additional performance and cost of using\ncp -p if you want all objects\
      \ in the destination bucket to end\nup with the same ACL by setting a default\
      \ object ACL on that\nbucket instead of using cp -p. See \"gsutil help defacl\"\
      .\nNote that it's not valid to specify both the -a and -p options\ntogether."
    synonyms:
    - -p
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Causes POSIX attributes to be preserved when objects are\ncopied.\
      \ With this feature enabled, gsutil cp will copy fields\nprovided by stat. These\
      \ are the user ID of the owner, the group\nID of the owning group, the mode\
      \ (permissions) of the file, and\nthe access/modification time of the file.\
      \ For downloads, these\nattributes will only be set if the source objects were\
      \ uploaded\nwith this flag enabled.\nOn Windows, this flag will only set and\
      \ restore access time and\nmodification time. This is because Windows doesn't\
      \ have a\nnotion of POSIX uid/gid/mode."
    synonyms:
    - -P
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "The -R and -r options are synonymous. Causes directories,\nbuckets,\
      \ and bucket subdirectories to be copied recursively.\nIf you neglect to use\
      \ this option for an upload, gsutil will\ncopy any files it finds and skip any\
      \ directories. Similarly,\nneglecting to specify this option for a download\
      \ will cause\ngsutil to copy any objects at the current bucket directory\nlevel,\
      \ and skip any subdirectories."
    synonyms:
    - -R
    - -r
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "The storage class of the destination object(s). If not\nspecified,\
      \ the default storage class of the destination bucket\nis used. Not valid for\
      \ copying to non-cloud destinations."
    synonyms:
    - -s
    args: !SimpleFlagArg
      name: class
    optional: true
  - !Flag
    description: "Skip objects with unsupported object types instead of failing.\n\
      Unsupported object types are Amazon S3 Objects in the GLACIER\nstorage class."
    synonyms:
    - -U
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Requests that the version-specific URL for each uploaded object\n\
      be printed. Given this URL you can make future upload requests\nthat are safe\
      \ in the face of concurrent updates, because Google\nCloud Storage will refuse\
      \ to perform the update if the current\nobject version doesn't match the version-specific\
      \ URL. See\n\"gsutil help versions\" for more details."
    synonyms:
    - -v
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "<ext,...>   Applies gzip content-encoding to any file upload whose\n\
      extension matches the -z extension list. This is useful when\nuploading files\
      \ with compressible content (such as .js, .css,\nor .html files) because it\
      \ saves network bandwidth and space\nin Google Cloud Storage, which in turn\
      \ reduces storage costs.\nWhen you specify the -z option, the data from your\
      \ files is\ncompressed before it is uploaded, but your actual files are\nleft\
      \ uncompressed on the local disk. The uploaded objects\nretain the Content-Type\
      \ and name of the original files but are\ngiven a Content-Encoding header with\
      \ the value \"gzip\" to\nindicate that the object data stored are compressed\
      \ on the\nGoogle Cloud Storage servers.\nFor example, the following command:\n\
      gsutil cp -z html -a public-read \\\ncattypes.html tabby.jpeg gs://mycats\n\
      will do all of the following:\n- Upload the files cattypes.html and tabby.jpeg\
      \ to the bucket\ngs://mycats (cp command)\n- Set the Content-Type of cattypes.html\
      \ to text/html and\ntabby.jpeg to image/jpeg (based on file extensions)\n- Compress\
      \ the data in the file cattypes.html (-z option)\n- Set the Content-Encoding\
      \ for cattypes.html to gzip\n(-z option)\n- Set the ACL for both files to public-read\
      \ (-a option)\n- If a user tries to view cattypes.html in a browser, the\nbrowser\
      \ will know to uncompress the data based on the\nContent-Encoding header and\
      \ to render it as HTML based on\nthe Content-Type header.\nNote that if you\
      \ download an object with Content-Encoding:gzip\ngsutil will decompress the\
      \ content before writing the local\nfile."
    synonyms:
    - -z
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Applies gzip content-encoding to file uploads. This option\nworks\
      \ like the -z option described above, but it applies to\nall uploaded files,\
      \ regardless of extension.\nWarning: If you use this option and some of the\
      \ source files\ndon't compress well (e.g., that's often true of binary data),\n\
      this option may result in files taking up more space in the\ncloud than they\
      \ would if left uncompressed.\n"
    synonyms:
    - -Z
    args: !EmptyFlagArg {}
    optional: true
  parent: *id001
  subcommands: []
  usage: []
  help_flag:
  usage_flag:
  version_flag:
  help_text: "NAME\n  cp - Copy files and objects\n\n\nSYNOPSIS\n\n  gsutil cp [OPTION]...\
    \ src_url dst_url\n  gsutil cp [OPTION]... src_url... dst_url\n  gsutil cp [OPTION]...\
    \ -I dst_url\n\n\n\nDESCRIPTION\n  The gsutil cp command allows you to copy data\
    \ between your local file\n  system and the cloud, copy data within the cloud,\
    \ and copy data between\n  cloud storage providers. For example, to copy all text\
    \ files from the\n  local directory to a bucket you could do:\n\n    gsutil cp\
    \ *.txt gs://my-bucket\n\n  Similarly, you can download text files from a bucket\
    \ by doing:\n\n    gsutil cp gs://my-bucket/*.txt .\n\n  If you want to copy an\
    \ entire directory tree you need to use the -r option:\n\n    gsutil cp -r dir\
    \ gs://my-bucket\n\n  If you have a large number of files to transfer you might\
    \ want to use the\n  gsutil -m option, to perform a parallel (multi-threaded/multi-processing)\n\
    \  copy:\n\n    gsutil -m cp -r dir gs://my-bucket\n\n  You can pass a list of\
    \ URLs (one per line) to copy on stdin instead of as\n  command line arguments\
    \ by using the -I option. This allows you to use gsutil\n  in a pipeline to upload\
    \ or download files / objects as generated by a program,\n  such as:\n\n    some_program\
    \ | gsutil -m cp -I gs://my-bucket\n\n  or:\n\n    some_program | gsutil -m cp\
    \ -I ./download_dir\n\n  The contents of stdin can name files, cloud URLs, and\
    \ wildcards of files\n  and cloud URLs.\n\n\n\nHOW NAMES ARE CONSTRUCTED\n  The\
    \ gsutil cp command strives to name objects in a way consistent with how\n  Linux\
    \ cp works, which causes names to be constructed in varying ways depending\n \
    \ on whether you're performing a recursive directory copy or copying\n  individually\
    \ named objects; and whether you're copying to an existing or\n  non-existent\
    \ directory.\n\n  When performing recursive directory copies, object names are\
    \ constructed that\n  mirror the source directory structure starting at the point\
    \ of recursive\n  processing. For example, if dir1/dir2 contains the file a/b/c\
    \ then the\n  command:\n\n    gsutil cp -r dir1/dir2 gs://my-bucket\n\n  will\
    \ create the object gs://my-bucket/dir2/a/b/c.\n\n  In contrast, copying individually\
    \ named files will result in objects named by\n  the final path component of the\
    \ source files. For example, again assuming\n  dir1/dir2 contains a/b/c, the command:\n\
    \n    gsutil cp dir1/dir2/** gs://my-bucket\n\n  will create the object gs://my-bucket/c.\n\
    \n  The same rules apply for downloads: recursive copies of buckets and\n  bucket\
    \ subdirectories produce a mirrored filename structure, while copying\n  individually\
    \ (or wildcard) named objects produce flatly named files.\n\n  Note that in the\
    \ above example the '**' wildcard matches all names\n  anywhere under dir. The\
    \ wildcard '*' will match names just one level deep. For\n  more details see \"\
    gsutil help wildcards\".\n\n  There's an additional wrinkle when working with\
    \ subdirectories: the resulting\n  names depend on whether the destination subdirectory\
    \ exists. For example,\n  if gs://my-bucket/subdir exists as a subdirectory, the\
    \ command:\n\n    gsutil cp -r dir1/dir2 gs://my-bucket/subdir\n\n  will create\
    \ the object gs://my-bucket/subdir/dir2/a/b/c. In contrast, if\n  gs://my-bucket/subdir\
    \ does not exist, this same gsutil cp command will create\n  the object gs://my-bucket/subdir/a/b/c.\n\
    \n  Note: If you use the\n  `Google Cloud Platform Console <https://console.cloud.google.com>`_\n\
    \  to create folders, it does so by creating a \"placeholder\" object that ends\n\
    \  with a \"/\" character. gsutil skips these objects when downloading from the\n\
    \  cloud to the local file system, because attempting to create a file that\n\
    \  ends with a \"/\" is not allowed on Linux and MacOS. Because of this, it is\n\
    \  recommended that you not create objects that end with \"/\" (unless you don't\n\
    \  need to be able to download such objects using gsutil).\n\n\n\nCOPYING TO/FROM\
    \ SUBDIRECTORIES; DISTRIBUTING TRANSFERS ACROSS MACHINES\n  You can use gsutil\
    \ to copy to and from subdirectories by using a command\n  like:\n\n    gsutil\
    \ cp -r dir gs://my-bucket/data\n\n  This will cause dir and all of its files\
    \ and nested subdirectories to be\n  copied under the specified destination, resulting\
    \ in objects with names like\n  gs://my-bucket/data/dir/a/b/c. Similarly you can\
    \ download from bucket\n  subdirectories by using a command like:\n\n    gsutil\
    \ cp -r gs://my-bucket/data dir\n\n  This will cause everything nested under gs://my-bucket/data\
    \ to be downloaded\n  into dir, resulting in files with names like dir/data/a/b/c.\n\
    \n  Copying subdirectories is useful if you want to add data to an existing\n\
    \  bucket directory structure over time. It's also useful if you want\n  to parallelize\
    \ uploads and downloads across multiple machines (potentially\n  reducing overall\
    \ transfer time compared with simply running gsutil -m\n  cp on one machine).\
    \ For example, if your bucket contains this structure:\n\n    gs://my-bucket/data/result_set_01/\n\
    \    gs://my-bucket/data/result_set_02/\n    ...\n    gs://my-bucket/data/result_set_99/\n\
    \n  you could perform concurrent downloads across 3 machines by running these\n\
    \  commands on each machine, respectively:\n\n    gsutil -m cp -r gs://my-bucket/data/result_set_[0-3]*\
    \ dir\n    gsutil -m cp -r gs://my-bucket/data/result_set_[4-6]* dir\n    gsutil\
    \ -m cp -r gs://my-bucket/data/result_set_[7-9]* dir\n\n  Note that dir could\
    \ be a local directory on each machine, or it could be a\n  directory mounted\
    \ off of a shared file server; whether the latter performs\n  acceptably will\
    \ depend on a number of factors, so we recommend experimenting\n  to find out\
    \ what works best for your computing environment.\n\n\n\nCOPYING IN THE CLOUD\
    \ AND METADATA PRESERVATION\n  If both the source and destination URL are cloud\
    \ URLs from the same\n  provider, gsutil copies data \"in the cloud\" (i.e., without\
    \ downloading\n  to and uploading from the machine where you run gsutil). In addition\
    \ to\n  the performance and cost advantages of doing this, copying in the cloud\n\
    \  preserves metadata (like Content-Type and Cache-Control). In contrast,\n  when\
    \ you download data from the cloud it ends up in a file, which has\n  no associated\
    \ metadata. Thus, unless you have some way to hold on to\n  or re-create that\
    \ metadata, downloading to a file will not retain the\n  metadata.\n\n  Copies\
    \ spanning locations and/or storage classes cause data to be rewritten\n  in the\
    \ cloud, which may take some time (but still will be faster than\n  downloading\
    \ and re-uploading). Such operations can be resumed with the same\n  command if\
    \ they are interrupted, so long as the command parameters are\n  identical.\n\n\
    \  Note that by default, the gsutil cp command does not copy the object\n  ACL\
    \ to the new object, and instead will use the default bucket ACL (see\n  \"gsutil\
    \ help defacl\"). You can override this behavior with the -p\n  option (see OPTIONS\
    \ below).\n\n  One additional note about copying in the cloud: If the destination\
    \ bucket has\n  versioning enabled, by default gsutil cp will copy only live versions\
    \ of the\n  source object(s). For example:\n\n    gsutil cp gs://bucket1/obj gs://bucket2\n\
    \n  will cause only the single live version of gs://bucket1/obj to be copied to\n\
    \  gs://bucket2, even if there are archived versions of gs://bucket1/obj. To also\n\
    \  copy archived versions, use the -A flag:\n\n    gsutil cp -A gs://bucket1/obj\
    \ gs://bucket2\n\n  The gsutil -m flag is disallowed when using the cp -A flag,\
    \ to ensure that\n  version ordering is preserved.\n\n\n\nCHECKSUM VALIDATION\n\
    \  At the end of every upload or download the gsutil cp command validates that\n\
    \  the checksum it computes for the source file/object matches the checksum\n\
    \  the service computes. If the checksums do not match, gsutil will delete the\n\
    \  corrupted object and print a warning message. This very rarely happens, but\n\
    \  if it does, please contact gs-team@google.com.\n\n  If you know the MD5 of\
    \ a file before uploading you can specify it in the\n  Content-MD5 header, which\
    \ will cause the cloud storage service to reject the\n  upload if the MD5 doesn't\
    \ match the value computed by the service. For\n  example:\n\n    % gsutil hash\
    \ obj\n    Hashing     obj:\n    Hashes [base64] for obj:\n            Hash (crc32c):\
    \          lIMoIw==\n            Hash (md5):             VgyllJgiiaRAbyUUIqDMmw==\n\
    \n    % gsutil -h Content-MD5:VgyllJgiiaRAbyUUIqDMmw== cp obj gs://your-bucket/obj\n\
    \    Copying file://obj [Content-Type=text/plain]...\n    Uploading   gs://your-bucket/obj:\
    \                                182 b/182 B\n\n    If the checksum didn't match\
    \ the service would instead reject the upload and\n    gsutil would print a message\
    \ like:\n\n    BadRequestException: 400 Provided MD5 hash \"VgyllJgiiaRAbyUUIqDMmw==\"\
    \n    doesn't match calculated MD5 hash \"7gyllJgiiaRAbyUUIqDMmw==\".\n\n  Even\
    \ if you don't do this gsutil will delete the object if the computed\n  checksum\
    \ mismatches, but specifying the Content-MD5 header has several\n  advantages:\n\
    \n      1. It prevents the corrupted object from becoming visible at all, whereas\n\
    \      otherwise it would be visible for 1-3 seconds before gsutil deletes it.\n\
    \n      2. If an object already exists with the given name, specifying the\n \
    \     Content-MD5 header will cause the existing object never to be replaced,\n\
    \      whereas otherwise it would be replaced by the corrupted object and then\n\
    \      deleted a few seconds later.\n\n      3. It will definitively prevent the\
    \ corrupted object from being left in\n      the cloud, whereas the gsutil approach\
    \ of deleting after the upload\n      completes could fail if (for example) the\
    \ gsutil process gets ^C'd\n      between upload and deletion request.\n\n   \
    \   4. It supports a customer-to-service integrity check handoff. For example,\n\
    \      if you have a content production pipeline that generates data to be\n \
    \     uploaded to the cloud along with checksums of that data, specifying the\n\
    \      MD5 computed by your content pipeline when you run gsutil cp will ensure\n\
    \      that the checksums match all the way through the process (e.g., detecting\n\
    \      if data gets corrupted on your local disk between the time it was written\n\
    \      by your content pipeline and the time it was uploaded to GCS).\n\n  Note:\
    \ The Content-MD5 header is ignored for composite objects, because such\n  objects\
    \ only have a CRC32C checksum.\n\n\n\nRETRY HANDLING\n  The cp command will retry\
    \ when failures occur, but if enough failures happen\n  during a particular copy\
    \ or delete operation the cp command will skip that\n  object and move on. At\
    \ the end of the copy run if any failures were not\n  successfully retried, the\
    \ cp command will report the count of failures, and\n  exit with non-zero status.\n\
    \n  Note that there are cases where retrying will never succeed, such as if you\n\
    \  don't have write permission to the destination bucket or if the destination\n\
    \  path for some objects is longer than the maximum allowed length.\n\n  For more\
    \ details about gsutil's retry handling, please see\n  \"gsutil help retries\"\
    .\n\n\n\nRESUMABLE TRANSFERS\n  gsutil automatically performs a resumable upload\
    \ whenever you use the cp\n  command to upload an object that is larger than 8\
    \ MiB. You do not need to\n  specify any special command line options to make\
    \ this happen. If your upload\n  is interrupted you can restart the upload by\
    \ running the same cp command that\n  you ran to start the upload. Until the upload\
    \ has completed successfully, it\n  will not be visible at the destination object\
    \ and will not replace any\n  existing object the upload is intended to overwrite.\
    \ However, see the section\n  on PARALLEL COMPOSITE UPLOADS, which may leave temporary\
    \ component objects in\n  place during the upload process.\n\n  Similarly, gsutil\
    \ automatically performs resumable downloads (using standard\n  HTTP Range GET\
    \ operations) whenever you use the cp command, unless the\n  destination is a\
    \ stream. In this case, a partially downloaded temporary file\n  will be visible\
    \ in the destination directory. Upon completion, the original\n  file is deleted\
    \ and overwritten with the downloaded contents.\n\n  Resumable uploads and downloads\
    \ store state information in files under\n  ~/.gsutil, named by the destination\
    \ object or file. If you attempt to resume a\n  transfer from a machine with a\
    \ different directory, the transfer will start\n  over from scratch.\n\n  See\
    \ also \"gsutil help prod\" for details on using resumable transfers\n  in production.\n\
    \n\n\nSTREAMING TRANSFERS\n  Use '-' in place of src_url or dst_url to perform\
    \ a streaming\n  transfer. For example:\n\n    long_running_computation | gsutil\
    \ cp - gs://my-bucket/obj\n\n  Streaming uploads using the JSON API (see \"gsutil\
    \ help apis\") are buffered in\n  memory part-way back into the file and can thus\
    \ retry in the event of network\n  or service problems.\n\n  Streaming transfers\
    \ using the XML API do not support resumable\n  uploads/downloads. If you have\
    \ a large amount of data to upload (say, more\n  than 100 MiB) it is recommended\
    \ that you write the data to a local file and\n  then copy that file to the cloud\
    \ rather than streaming it (and similarly for\n  large downloads).\n\n  WARNING:\
    \ When performing streaming transfers gsutil does not compute a\n  checksum of\
    \ the uploaded or downloaded data. Therefore, we recommend that\n  users either\
    \ perform their own validation of the data or use non-streaming\n  transfers (which\
    \ perform integrity checking automatically).\n\n\n\nSLICED OBJECT DOWNLOADS\n\
    \  gsutil uses HTTP Range GET requests to perform \"sliced\" downloads in parallel\n\
    \  when downloading large objects from Google Cloud Storage. This means that disk\n\
    \  space for the temporary download destination file will be pre-allocated and\n\
    \  byte ranges (slices) within the file will be downloaded in parallel. Once all\n\
    \  slices have completed downloading, the temporary file will be renamed to the\n\
    \  destination file. No additional local disk space is required for this\n  operation.\n\
    \n  This feature is only available for Google Cloud Storage objects because it\n\
    \  requires a fast composable checksum (CRC32C) that can be used to verify the\n\
    \  data integrity of the slices. And because it depends on CRC32C, using sliced\n\
    \  object downloads also requires a compiled crcmod (see \"gsutil help crcmod\"\
    ) on\n  the machine performing the download. If compiled crcmod is not available,\n\
    \  a non-sliced object download will instead be performed.\n\n  Note: since sliced\
    \ object downloads cause multiple writes to occur at various\n  locations on disk,\
    \ this mechanism can degrade performance for disks with slow\n  seek times, especially\
    \ for large numbers of slices. While the default number\n  of slices is set small\
    \ to avoid this problem, you can disable sliced object\n  download if necessary\
    \ by setting the \"sliced_object_download_threshold\"\n  variable in the .boto\
    \ config file to 0.\n\n\n\n\n\nPARALLEL COMPOSITE UPLOADS\n  gsutil can automatically\
    \ use\n  `object composition <https://cloud.google.com/storage/docs/composite-objects>`_\n\
    \  to perform uploads in parallel for large, local files being uploaded to Google\n\
    \  Cloud Storage. If enabled (see below), a large file will be split into\n  component\
    \ pieces that are uploaded in parallel and then composed in the cloud\n  (and\
    \ the temporary components finally deleted). A file can be broken into as\n  many\
    \ as 32 component pieces; until this piece limit is reached, the maximum\n  size\
    \ of each component piece is determined by the variable\n  \"parallel_composite_upload_component_size,\"\
    \ specified in the [GSUtil] section\n  of your .boto configuration file (for files\
    \ that are otherwise too big,\n  components are as large as needed to fit into\
    \ 32 pieces). No additional local\n  disk space is required for this operation.\n\
    \n  Using parallel composite uploads presents a tradeoff between upload\n  performance\
    \ and download configuration: If you enable parallel composite\n  uploads your\
    \ uploads will run faster, but someone will need to install a\n  compiled crcmod\
    \ (see \"gsutil help crcmod\") on every machine where objects are\n  downloaded\
    \ by gsutil or other Python applications. Note that for such uploads,\n  crcmod\
    \ is required for downloading regardless of whether the parallel\n  composite\
    \ upload option is on or not. For some distributions this is easy\n  (e.g., it\
    \ comes pre-installed on MacOS), but in other cases some users have\n  found it\
    \ difficult. Because of this, at present parallel composite uploads are\n  disabled\
    \ by default. Google is actively working with a number of the Linux\n  distributions\
    \ to get crcmod included with the stock distribution. Once that is\n  done we\
    \ will re-enable parallel composite uploads by default in gsutil.\n\n  Warning:\
    \ Parallel composite uploads should not be used with NEARLINE or\n  COLDLINE storage\
    \ class buckets, because doing so incurs an early deletion\n  charge for each\
    \ component object.\n\n  To try parallel composite uploads you can run the command:\n\
    \n    gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp bigfile gs://your-bucket\n\
    \n  where bigfile is larger than 150 MiB. When you do this notice that the upload\n\
    \  progress indicator continuously updates for several different uploads at once\n\
    \  (corresponding to each of the sections of the file being uploaded in\n  parallel),\
    \ until the parallel upload completes. If after trying this you want\n  to enable\
    \ parallel composite uploads for all of your future uploads\n  (notwithstanding\
    \ the caveats mentioned earlier), you can uncomment and set the\n  \"parallel_composite_upload_threshold\"\
    \ config value in your .boto configuration\n  file to this value.\n\n  Note that\
    \ the crcmod problem only impacts downloads via Python applications\n  (such as\
    \ gsutil). If all users who need to download the data using gsutil or\n  other\
    \ Python applications can install crcmod, or if no Python users will\n  need to\
    \ download your objects, it makes sense to enable parallel composite\n  uploads\
    \ (see above). For example, if you use gsutil to upload video assets,\n  and those\
    \ assets will only ever be served via a Java application, it would\n  make sense\
    \ to enable parallel composite uploads on your machine (there are\n  efficient\
    \ CRC32C implementations available in Java).\n\n  If a parallel composite upload\
    \ fails prior to composition, re-running the\n  gsutil command will take advantage\
    \ of resumable uploads for the components\n  that failed, and the component objects\
    \ will be deleted after the first\n  successful attempt. Any temporary objects\
    \ that were uploaded successfully\n  before gsutil failed will still exist until\
    \ the upload is completed\n  successfully. The temporary objects will be named\
    \ in the following fashion:\n\n    <random ID>/gsutil/tmp/parallel_composite_uploads/for_details_see/gsutil_help_cp/<hash>\n\
    \n  where <random ID> is a numerical value, and <hash> is an MD5 hash (not related\n\
    \  to the hash of the contents of the file or object).\n\n  To avoid leaving temporary\
    \ objects around, you should make sure to check the\n  exit status from the gsutil\
    \ command.  This can be done in a bash script, for\n  example, by doing:\n\n \
    \   if ! gsutil cp ./local-file gs://your-bucket/your-object; then\n      << Code\
    \ that handles failures >>\n    fi\n\n  Or, for copying a directory, use this\
    \ instead:\n\n    if ! gsutil cp -c -L cp.log -r ./dir gs://bucket; then\n   \
    \   << Code that handles failures >>\n    fi\n\n  One important caveat is that\
    \ files uploaded using parallel composite uploads\n  are subject to a maximum\
    \ number of components limit. For example, if you\n  upload a large file that\
    \ gets split into 10 components, and try to compose it\n  with another object\
    \ with 1015 components, the operation will fail because it\n  exceeds the 1024\
    \ component limit. If you wish to compose an object later and the\n  component\
    \ limit is a concern, it is recommended that you disable parallel\n  composite\
    \ uploads for that transfer.\n\n  Also note that an object uploaded using parallel\
    \ composite uploads will have a\n  CRC32C hash, but it will not have an MD5 hash\
    \ (and because of that, users who\n  download the object must have crcmod installed,\
    \ as noted earlier). For details\n  see \"gsutil help crc32c\".\n\n  Parallel\
    \ composite uploads can be disabled by setting the\n  \"parallel_composite_upload_threshold\"\
    \ variable in the .boto config file to 0.\n\n\n\nCHANGING TEMP DIRECTORIES\n \
    \ gsutil writes data to a temporary directory in several cases:\n\n  - when compressing\
    \ data to be uploaded (see the -z and -Z options)\n  - when decompressing data\
    \ being downloaded (when the data has\n    Content-Encoding:gzip, e.g., as happens\
    \ when uploaded using gsutil cp -z\n    or gsutil cp -Z)\n  - when running integration\
    \ tests (using the gsutil test command)\n\n  In these cases it's possible the\
    \ temp file location on your system that\n  gsutil selects by default may not\
    \ have enough space. If gsutil runs out of\n  space during one of these operations\
    \ (e.g., raising\n  \"CommandException: Inadequate temp space available to compress\
    \ <your file>\"\n  during a gsutil cp -z operation), you can change where it writes\
    \ these\n  temp files by setting the TMPDIR environment variable. On Linux and\
    \ MacOS\n  you can do this either by running gsutil this way:\n\n    TMPDIR=/some/directory\
    \ gsutil cp ...\n\n  or by adding this line to your ~/.bashrc file and then restarting\
    \ the shell\n  before running gsutil:\n\n    export TMPDIR=/some/directory\n\n\
    \  On Windows 7 you can change the TMPDIR environment variable from Start ->\n\
    \  Computer -> System -> Advanced System Settings -> Environment Variables.\n\
    \  You need to reboot after making this change for it to take effect. (Rebooting\n\
    \  is not necessary after running the export command on Linux and MacOS.)\n\n\n\
    \nCOPYING SPECIAL FILES\n  gsutil cp does not support copying special file types\
    \ such as sockets, device\n  files, named pipes, or any other non-standard files\
    \ intended to represent an\n  operating system resource. You should not run gsutil\
    \ cp with sources that\n  include such files (for example, recursively copying\
    \ the root directory on\n  Linux that includes /dev ). If you do, gsutil cp may\
    \ fail or hang.\n\n\n\nOPTIONS\n  -a canned_acl  Sets named canned_acl when uploaded\
    \ objects created. See\n                 \"gsutil help acls\" for further details.\n\
    \n  -A             Copy all source versions from a source buckets/folders.\n \
    \                If not set, only the live version of each source object is\n\
    \                 copied. Note: this option is only useful when the destination\n\
    \                 bucket has versioning enabled.\n\n  -c             If an error\
    \ occurs, continue to attempt to copy the remaining\n                 files. If\
    \ any copies were unsuccessful, gsutil's exit status\n                 will be\
    \ non-zero even if this flag is set. This option is\n                 implicitly\
    \ set when running \"gsutil -m cp...\". Note: -c only\n                 applies\
    \ to the actual copying operation. If an error occurs\n                 while\
    \ iterating over the files in the local directory (e.g.,\n                 invalid\
    \ Unicode file name) gsutil will print an error message\n                 and\
    \ abort.\n\n  -D             Copy in \"daisy chain\" mode, i.e., copying between\
    \ two buckets\n                 by hooking a download to an upload, via the machine\
    \ where\n                 gsutil is run. This stands in contrast to the default,\
    \ where\n                 data are copied between two buckets \"in the cloud\"\
    , i.e.,\n                 without needing to copy via the machine where gsutil\
    \ runs.\n\n                 By default, a \"copy in the cloud\" when the source\
    \ is a\n                 composite object will retain the composite nature of\
    \ the\n                 object. However, Daisy chain mode can be used to change\
    \ a\n                 composite object into a non-composite object. For example:\n\
    \n                     gsutil cp -D -p gs://bucket/obj gs://bucket/obj_tmp\n \
    \                    gsutil mv -p gs://bucket/obj_tmp gs://bucket/obj\n\n    \
    \             Note: Daisy chain mode is automatically used when copying\n    \
    \             between providers (e.g., to copy data from Google Cloud Storage\n\
    \                 to another provider).\n\n  -e             Exclude symlinks.\
    \ When specified, symbolic links will not be\n                 copied.\n\n  -I\
    \             Causes gsutil to read the list of files or objects to copy from\n\
    \                 stdin. This allows you to run a program that generates the list\n\
    \                 of files to upload/download.\n\n  -L <file>      Outputs a manifest\
    \ log file with detailed information about\n                 each item that was\
    \ copied. This manifest contains the following\n                 information for\
    \ each item:\n\n                 - Source path.\n                 - Destination\
    \ path.\n                 - Source size.\n                 - Bytes transferred.\n\
    \                 - MD5 hash.\n                 - UTC date and time transfer was\
    \ started in ISO 8601 format.\n                 - UTC date and time transfer was\
    \ completed in ISO 8601 format.\n                 - Upload id, if a resumable\
    \ upload was performed.\n                 - Final result of the attempted transfer,\
    \ success or failure.\n                 - Failure details, if any.\n\n       \
    \          If the log file already exists, gsutil will use the file as an\n  \
    \               input to the copy process, and will also append log items to\n\
    \                 the existing file. Files/objects that are marked in the\n  \
    \               existing log file as having been successfully copied (or\n   \
    \              skipped) will be ignored. Files/objects without entries will be\n\
    \                 copied and ones previously marked as unsuccessful will be\n\
    \                 retried. This can be used in conjunction with the -c option\
    \ to\n                 build a script that copies a large number of objects reliably,\n\
    \                 using a bash script like the following:\n\n                \
    \   until gsutil cp -c -L cp.log -r ./dir gs://bucket; do\n                  \
    \   sleep 1\n                   done\n\n                 The -c option will cause\
    \ copying to continue after failures\n                 occur, and the -L option\
    \ will allow gsutil to pick up where it\n                 left off without duplicating\
    \ work. The loop will continue\n                 running as long as gsutil exits\
    \ with a non-zero status (such a\n                 status indicates there was\
    \ at least one failure during the\n                 gsutil run).\n\n         \
    \        Note: If you're trying to synchronize the contents of a\n           \
    \      directory and a bucket (or two buckets), see\n                 \"gsutil\
    \ help rsync\".\n\n  -n             No-clobber. When specified, existing files\
    \ or objects at the\n                 destination will not be overwritten. Any\
    \ items that are skipped\n                 by this option will be reported as\
    \ being skipped. This option\n                 will perform an additional GET\
    \ request to check if an item\n                 exists before attempting to upload\
    \ the data. This will save\n                 retransmitting data, but the additional\
    \ HTTP requests may make\n                 small object transfers slower and more\
    \ expensive.\n\n  -p             Causes ACLs to be preserved when copying in the\
    \ cloud. Note\n                 that this option has performance and cost implications\
    \ when\n                 using  the XML API, as it requires separate HTTP calls\
    \ for\n                 interacting with ACLs. (There are no such performance\
    \ or cost\n                 implications when using the -p option with the JSON\
    \ API.) The\n                 performance issue can be mitigated to some degree\
    \ by using\n                 gsutil -m cp to cause parallel copying. Note that\
    \ this option\n                 only works if you have OWNER access to all of\
    \ the objects that\n                 are copied.\n\n                 You can avoid\
    \ the additional performance and cost of using\n                 cp -p if you\
    \ want all objects in the destination bucket to end\n                 up with\
    \ the same ACL by setting a default object ACL on that\n                 bucket\
    \ instead of using cp -p. See \"gsutil help defacl\".\n\n                 Note\
    \ that it's not valid to specify both the -a and -p options\n                \
    \ together.\n\n  -P             Causes POSIX attributes to be preserved when objects\
    \ are\n                 copied. With this feature enabled, gsutil cp will copy\
    \ fields\n                 provided by stat. These are the user ID of the owner,\
    \ the group\n                 ID of the owning group, the mode (permissions) of\
    \ the file, and\n                 the access/modification time of the file. For\
    \ downloads, these\n                 attributes will only be set if the source\
    \ objects were uploaded\n                 with this flag enabled.\n\n        \
    \         On Windows, this flag will only set and restore access time and\n  \
    \               modification time. This is because Windows doesn't have a\n  \
    \               notion of POSIX uid/gid/mode.\n\n  -R, -r         The -R and -r\
    \ options are synonymous. Causes directories,\n                 buckets, and bucket\
    \ subdirectories to be copied recursively.\n                 If you neglect to\
    \ use this option for an upload, gsutil will\n                 copy any files\
    \ it finds and skip any directories. Similarly,\n                 neglecting to\
    \ specify this option for a download will cause\n                 gsutil to copy\
    \ any objects at the current bucket directory\n                 level, and skip\
    \ any subdirectories.\n\n  -s <class>     The storage class of the destination\
    \ object(s). If not\n                 specified, the default storage class of\
    \ the destination bucket\n                 is used. Not valid for copying to non-cloud\
    \ destinations.\n\n  -U             Skip objects with unsupported object types\
    \ instead of failing.\n                 Unsupported object types are Amazon S3\
    \ Objects in the GLACIER\n                 storage class.\n\n  -v            \
    \ Requests that the version-specific URL for each uploaded object\n          \
    \       be printed. Given this URL you can make future upload requests\n     \
    \            that are safe in the face of concurrent updates, because Google\n\
    \                 Cloud Storage will refuse to perform the update if the current\n\
    \                 object version doesn't match the version-specific URL. See\n\
    \                 \"gsutil help versions\" for more details.\n\n  -z <ext,...>\
    \   Applies gzip content-encoding to any file upload whose\n                 extension\
    \ matches the -z extension list. This is useful when\n                 uploading\
    \ files with compressible content (such as .js, .css,\n                 or .html\
    \ files) because it saves network bandwidth and space\n                 in Google\
    \ Cloud Storage, which in turn reduces storage costs.\n\n                 When\
    \ you specify the -z option, the data from your files is\n                 compressed\
    \ before it is uploaded, but your actual files are\n                 left uncompressed\
    \ on the local disk. The uploaded objects\n                 retain the Content-Type\
    \ and name of the original files but are\n                 given a Content-Encoding\
    \ header with the value \"gzip\" to\n                 indicate that the object\
    \ data stored are compressed on the\n                 Google Cloud Storage servers.\n\
    \n                 For example, the following command:\n\n                   gsutil\
    \ cp -z html -a public-read \\\n                     cattypes.html tabby.jpeg\
    \ gs://mycats\n\n                 will do all of the following:\n\n          \
    \       - Upload the files cattypes.html and tabby.jpeg to the bucket\n      \
    \             gs://mycats (cp command)\n                 - Set the Content-Type\
    \ of cattypes.html to text/html and\n                   tabby.jpeg to image/jpeg\
    \ (based on file extensions)\n                 - Compress the data in the file\
    \ cattypes.html (-z option)\n                 - Set the Content-Encoding for cattypes.html\
    \ to gzip\n                   (-z option)\n                 - Set the ACL for\
    \ both files to public-read (-a option)\n                 - If a user tries to\
    \ view cattypes.html in a browser, the\n                   browser will know to\
    \ uncompress the data based on the\n                   Content-Encoding header\
    \ and to render it as HTML based on\n                   the Content-Type header.\n\
    \n                 Note that if you download an object with Content-Encoding:gzip\n\
    \                 gsutil will decompress the content before writing the local\n\
    \                 file.\n\n  -Z             Applies gzip content-encoding to file\
    \ uploads. This option\n                 works like the -z option described above,\
    \ but it applies to\n                 all uploaded files, regardless of extension.\n\
    \n                 Warning: If you use this option and some of the source files\n\
    \                 don't compress well (e.g., that's often true of binary data),\n\
    \                 this option may result in files taking up more space in the\n\
    \                 cloud than they would if left uncompressed.\n"
  generated_using: *id002
- !Command
  command:
  - gsutil
  - notification
  positional:
  - !Positional
    description: OPTIONS
    position: 0
    name: projects/_/buckets/example-bucket/notificationConfigs/1
    optional: false
  - !Positional
    description: The optional token parameter can be used to validate notifications
      events.
    position: 0
    name: generated.
    optional: false
  named:
  - !Flag
    description: "Specify an event type filter for this notification config. Cloud\n\
      Storage will only send notifications of this type. You may specify\nthis parameter\
      \ multiple times to allow multiple event types. If not\nspecified, Cloud Storage\
      \ will send notifications for all event\ntypes. The valid types are:\nOBJECT_FINALIZE\
      \ - An object has been created.\nOBJECT_METADATA_UPDATE - The metadata of an\
      \ object has changed.\nOBJECT_DELETE - An object has been permanently deleted.\n\
      OBJECT_ARCHIVE - A live Cloud Storage object has been archived."
    synonyms:
    - -e
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Specifies the payload format of notification messages. Must be\n\
      either \"json\" for a payload matches the object metadata for the\nJSON API,\
      \ or \"none\" to specify no payload at all. In either case,\nnotification details\
      \ are available in the message attributes."
    synonyms:
    - -f
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Specifies a key:value attribute that will be appended to the set\n\
      of attributes sent to Cloud Pub/Sub for all events associated with\nthis notification\
      \ config. You may specify this parameter multiple\ntimes to set multiple attributes."
    synonyms:
    - -m
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Specifies a prefix path filter for this notification config. Cloud\n\
      Storage will only send notifications for objects in this bucket\nwhose names\
      \ begin with the specified prefix."
    synonyms:
    - -p
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Skips creation and permission assignment of the Cloud Pub/Sub topic.\n\
      This is useful if the caller does not have permission to access\nthe topic in\
      \ question, or if the topic already exists and has the\nappropriate publish\
      \ permission assigned."
    synonyms:
    - -s
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "The Cloud Pub/Sub topic to which notifications should be sent. If\n\
      not specified, this command will choose a topic whose project is\nyour default\
      \ project and whose ID is the same as the Cloud Storage\nbucket name."
    synonyms:
    - -t
    args: !EmptyFlagArg {}
    optional: true
  parent: *id001
  subcommands: []
  usage: []
  help_flag:
  usage_flag:
  version_flag:
  help_text: "NAME\n  notification - Configure object change notification\n\n\nSYNOPSIS\n\
    \  gsutil notification create -f (json|none) [-p prefix] [-t topic] \\\n     \
    \ [-m key:value]... [-e eventType]... bucket_url\n  gsutil notification delete\
    \ (notificationConfigName|bucket_url)...\n  gsutil notification list bucket_url...\n\
    \n  gsutil notification watchbucket [-i id] [-t token] app_url bucket_url\n  gsutil\
    \ notification stopchannel channel_id resource_id\n\n\nDESCRIPTION\n  The notification\
    \ command is used to configure Google Cloud Storage support for\n  sending notifications\
    \ to Cloud Pub/Sub as well as to configure the object\n  change notification feature.\n\
    \nCLOUD PUB/SUB\n  The \"create\", \"list\", and \"delete\" sub-commands deal\
    \ with configuring Cloud\n  Storage integration with Google Cloud Pub/Sub.\n\n\
    CREATE\n  The create sub-command creates a notification config on a bucket, establishing\n\
    \  a flow of event notifications from Cloud Storage to a Cloud Pub/Sub topic.\
    \ As\n  part of creating this flow, the create command also verifies that the\n\
    \  destination Cloud Pub/Sub topic exists, creating it if necessary, and verifies\n\
    \  that the Cloud Storage bucket has permission to publish events to that topic,\n\
    \  granting the permission if necessary.\n\n  If a destination Cloud Pub/Sub topic\
    \ is not specified with the -t flag, Cloud\n  Storage will by default choose a\
    \ topic name in the default project whose ID is\n  the same the bucket name. For\
    \ example, if the default project ID specified is\n  'default-project' and the\
    \ bucket being configured is gs://example-bucket, the\n  create command will use\
    \ the Cloud Pub/Sub topic\n  \"projects/default-project/topics/example-bucket\"\
    .\n\n  In order to enable notifications, a special Cloud Storage service account\n\
    \  unique to each project must have the IAM permission \"projects.topics.publish\"\
    .\n  This command will check to see if that permission exists and, if not, will\n\
    \  attempt to grant it.\n\n  You can create multiple notification configurations\
    \ for a bucket, but their\n  triggers cannot overlap such that a single event\
    \ could send multiple\n  notifications. Attempting to create a notification configuration\
    \ that\n  overlaps with an exisitng notification configuration results in an error.\n\
    \nCREATE EXAMPLES\n  Begin sending notifications of all changes to the bucket\
    \ example-bucket\n  to the Cloud Pub/Sub topic projects/default-project/topics/example-bucket:\n\
    \n    gsutil notification create -f json gs://example-bucket\n\n  The same as\
    \ above, but specifies the destination topic ID 'files-to-process'\n  in the default\
    \ project:\n\n    gsutil notification create -f json \\\n      -t files-to-process\
    \ gs://example-bucket\n\n  The same as above, but specifies a Cloud Pub/Sub topic\
    \ belonging to the\n  specific cloud project 'example-project':\n\n    gsutil\
    \ notification create -f json \\\n      -t projects/example-project/topics/files-to-process\
    \ gs://example-bucket\n\n  Create a notification config that will only send an\
    \ event when a new object\n  has been created:\n\n    gsutil notification create\
    \ -f json -t OBJECT_FINALIZE gs://example-bucket\n\n  Create a topic and notification\
    \ config that will only send an event when\n  an object beginning with \"photos/\"\
    \ is affected:\n\n    gsutil notification create -p photos/ gs://example-bucket\n\
    \n  List all of the notificationConfigs in bucket example-bucket:\n\n    gsutil\
    \ notification list gs://example-bucket\n\n  Delete all notitificationConfigs\
    \ for bucket example-bucket:\n\n    gsutil notification delete gs://example-bucket\n\
    \n  Delete one specific notificationConfig for bucket example-bucket:\n\n    gsutil\
    \ notification delete \\\n      projects/_/buckets/example-bucket/notificationConfigs/1\n\
    \nOPTIONS\n  The create sub-command has the following options\n\n  -e        Specify\
    \ an event type filter for this notification config. Cloud\n            Storage\
    \ will only send notifications of this type. You may specify\n            this\
    \ parameter multiple times to allow multiple event types. If not\n           \
    \ specified, Cloud Storage will send notifications for all event\n           \
    \ types. The valid types are:\n\n              OBJECT_FINALIZE - An object has\
    \ been created.\n              OBJECT_METADATA_UPDATE - The metadata of an object\
    \ has changed.\n              OBJECT_DELETE - An object has been permanently deleted.\n\
    \              OBJECT_ARCHIVE - A live Cloud Storage object has been archived.\n\
    \n  -f        Specifies the payload format of notification messages. Must be\n\
    \            either \"json\" for a payload matches the object metadata for the\n\
    \            JSON API, or \"none\" to specify no payload at all. In either case,\n\
    \            notification details are available in the message attributes.\n\n\
    \  -m        Specifies a key:value attribute that will be appended to the set\n\
    \            of attributes sent to Cloud Pub/Sub for all events associated with\n\
    \            this notification config. You may specify this parameter multiple\n\
    \            times to set multiple attributes.\n\n  -p        Specifies a prefix\
    \ path filter for this notification config. Cloud\n            Storage will only\
    \ send notifications for objects in this bucket\n            whose names begin\
    \ with the specified prefix.\n\n  -s        Skips creation and permission assignment\
    \ of the Cloud Pub/Sub topic.\n            This is useful if the caller does not\
    \ have permission to access\n            the topic in question, or if the topic\
    \ already exists and has the\n            appropriate publish permission assigned.\n\
    \n  -t        The Cloud Pub/Sub topic to which notifications should be sent. If\n\
    \            not specified, this command will choose a topic whose project is\n\
    \            your default project and whose ID is the same as the Cloud Storage\n\
    \            bucket name.\n\nLIST\n  The list sub-command provides a list of notification\
    \ configs belonging to a\n  given bucket. The listed name of each notification\
    \ config can be used with\n  the delete sub-command to delete that specific notification\
    \ config.\n\n  No object change notifications will be listed. Only Cloud Pub/Sub\
    \ notification\n  subscription configs will be listed.\n\nLIST EXAMPLES\n  Fetch\
    \ the list of notification configs for the bucket example-bucket:\n\n    gsutil\
    \ notification list gs://example-bucket\n\n  Fetch the notification configs in\
    \ all buckets matching a wildcard:\n\n    gsutil notification list gs://example-*\n\
    \n  Fetch all of the notification configs for buckets in the default project:\n\
    \n    gsutil notification list gs://*\n\nDELETE\n  The delete sub-command deletes\
    \ notification configs from a bucket. If a\n  notification config name is passed\
    \ as a parameter, that notification config\n  alone will be deleted. If a bucket\
    \ name is passed, all notification configs\n  associated with that bucket will\
    \ be deleted.\n\n  Cloud Pub/Sub topics associated with this notification config\
    \ will not be\n  deleted by this command. Those must be deleted separately, for\
    \ example with\n  the gcloud command `gcloud beta pubsub topics delete`.\n\n \
    \ Object Change Notification subscriptions cannot be deleted with this command.\n\
    \  For that, see the command `gsutil notification stopchannel`.\n\nDELETE EXAMPLES\n\
    \  Delete a single notification config (with ID 3) in the bucket example-bucket:\n\
    \n    gsutil notification delete projects/_/buckets/example-bucket/notificationConfigs/3\n\
    \n  Delete all notification configs in the bucket example-bucket:\n\n    gsutil\
    \ notification delete gs://example-bucket\n\nOBJECT CHANGE NOTIFICATIONS\n  For\
    \ more information on the Object Change Notification feature, please see:\n  https://cloud.google.com/storage/docs/object-change-notification\n\
    \n  The \"watchbucket\" and \"stopchannel\" sub-commands enable and disable Object\n\
    \  Change Notifications.\n\nWATCHBUCKET\n  The watchbucket sub-command can be\
    \ used to watch a bucket for object changes.\n  A service account must be used\
    \ when running this command.\n\n  The app_url parameter must be an HTTPS URL to\
    \ an application that will be\n  notified of changes to any object in the bucket.\
    \ The URL endpoint must be\n  a verified domain on your project. See\n  `Notification\
    \ Authorization <https://cloud.google.com/storage/docs/object-change-notification#_Authorization>`_\n\
    \  for details.\n\n  The optional id parameter can be used to assign a unique\
    \ identifier to the\n  created notification channel. If not provided, a random\
    \ UUID string will be\n  generated.\n\n  The optional token parameter can be used\
    \ to validate notifications events.\n  To do this, set this custom token and store\
    \ it to later verify that\n  notification events contain the client token you\
    \ expect.\n\nWATCHBUCKET EXAMPLES\n  Watch the bucket example-bucket for changes\
    \ and send notifications to an\n  application server running at example.com:\n\
    \n    gsutil notification watchbucket https://example.com/notify \\\n      gs://example-bucket\n\
    \n  Assign identifier my-channel-id to the created notification channel:\n\n \
    \   gsutil notification watchbucket -i my-channel-id \\\n      https://example.com/notify\
    \ gs://example-bucket\n\n  Set a custom client token that will be included with\
    \ each notification event:\n\n    gsutil notification watchbucket -t my-client-token\
    \ \\\n      https://example.com/notify gs://example-bucket\n\nSTOPCHANNEL\n  The\
    \ stopchannel sub-command can be used to stop sending change events to a\n  notification\
    \ channel.\n\n  The channel_id and resource_id parameters should match the values\
    \ from the\n  response of a bucket watch request.\n\nSTOPCHANNEL EXAMPLES\n  Stop\
    \ the notification event channel with channel identifier channel1 and\n  resource\
    \ identifier SoGqan08XDIFWr1Fv_nGpRJBHh8:\n\n    gsutil notification stopchannel\
    \ channel1 SoGqan08XDIFWr1Fv_nGpRJBHh8\n\nNOTIFICATIONS AND PARALLEL COMPOSITE\
    \ UPLOADS\n  By default, gsutil enables parallel composite uploads for large files\
    \ (see\n  \"gsutil help cp\"), which means that an upload of a large object can\
    \ result\n  in multiple temporary component objects being uploaded before the\
    \ actual\n  intended object is created. Any subscriber to notifications for this\
    \ bucket\n  will then see a notification for each of these components being created\
    \ and\n  deleted. If this is a concern for you, note that parallel composite uploads\n\
    \  can be disabled by setting \"parallel_composite_upload_threshold = 0\" in your\n\
    \  boto config file.\n"
  generated_using: *id002
- !Command
  command:
  - gsutil
  - web
  positional:
  - !Positional
    description: For example, suppose your company's Domain name is example.com. You
      could set
    position: 0
    name: page.
    optional: false
  named:
  - !Flag
    description: "Specifies the object name to serve when a bucket\nlisting is requested\
      \ via the CNAME alias to\nc.storage.googleapis.com."
    synonyms:
    - -m
    args: !SimpleFlagArg
      name: index.html
    optional: true
  - !Flag
    description: "Specifies the error page to serve when a request is made\nfor a\
      \ non-existent object via the CNAME alias to\nc.storage.googleapis.com."
    synonyms:
    - -e
    args: !SimpleFlagArg
      name: 404.html
    optional: true
  parent: *id001
  subcommands: []
  usage: []
  help_flag:
  usage_flag:
  version_flag:
  help_text: "NAME\n  web - Set a main page and/or error page for one or more buckets\n\
    \n\nSYNOPSIS\n  gsutil web set [-m main_page_suffix] [-e error_page] bucket_url...\n\
    \  gsutil web get bucket_url\n\nDESCRIPTION\n  The Website Configuration feature\
    \ enables you to configure a Google Cloud\n  Storage bucket to behave like a static\
    \ website. This means requests made via a\n  domain-named bucket aliased using\
    \ a Domain Name System \"CNAME\" to\n  c.storage.googleapis.com will work like\
    \ any other website, i.e., a GET to the\n  bucket will serve the configured \"\
    main\" page instead of the usual bucket\n  listing and a GET for a non-existent\
    \ object will serve the configured error\n  page.\n\n  For example, suppose your\
    \ company's Domain name is example.com. You could set\n  up a website bucket as\
    \ follows:\n\n  1. Create a bucket called example.com (see the \"DOMAIN NAMED\
    \ BUCKETS\"\n     section of \"gsutil help naming\" for details about creating\
    \ such buckets).\n\n  2. Create index.html and 404.html files and upload them\
    \ to the bucket.\n\n  3. Configure the bucket to have website behavior using the\
    \ command:\n\n       gsutil web set -m index.html -e 404.html gs://www.example.com\n\
    \n  4. Add a DNS CNAME record for example.com pointing to c.storage.googleapis.com\n\
    \     (ask your DNS administrator for help with this).\n\n  Now if you open a\
    \ browser and navigate to http://www.example.com, it will\n  display the main\
    \ page instead of the default bucket listing. Note: It can\n  take time for DNS\
    \ updates to propagate because of caching used by the DNS,\n  so it may take up\
    \ to a day for the domain-named bucket website to work after\n  you create the\
    \ CNAME DNS record.\n\n  Additional notes:\n\n  1. Because the main page is only\
    \ served when a bucket listing request is made\n     via the CNAME alias, you\
    \ can continue to use \"gsutil ls\" to list the bucket\n     and get the normal\
    \ bucket listing (rather than the main page).\n\n  2. The main_page_suffix applies\
    \ to each subdirectory of the bucket. For\n     example, with the main_page_suffix\
    \ configured to be index.html, a GET\n     request for http://www.example.com\
    \ would retrieve\n     http://www.example.com/index.html, and a GET request for\n\
    \     http://www.example.com/photos would retrieve\n     http://www.example.com/photos/index.html.\n\
    \n  3. There is just one 404.html page: For example, a GET request for\n     http://www.example.com/photos/missing\
    \ would retrieve\n     http://www.example.com/404.html, not\n     http://www.example.com/photos/404.html.\n\
    \n  4. For additional details see\n     https://cloud.google.com/storage/docs/website-configuration.\n\
    \n  The web command has two sub-commands:\n\nSET\n  The \"gsutil web set\" command\
    \ will allow you to configure or disable\n  Website Configuration on your bucket(s).\
    \ The \"set\" sub-command has the\n  following options (leave both options blank\
    \ to disable):\n\nSET OPTIONS\n  -m <index.html>      Specifies the object name\
    \ to serve when a bucket\n                       listing is requested via the\
    \ CNAME alias to\n                       c.storage.googleapis.com.\n\n  -e <404.html>\
    \        Specifies the error page to serve when a request is made\n          \
    \             for a non-existent object via the CNAME alias to\n             \
    \          c.storage.googleapis.com.\n\n\nGET\n  The \"gsutil web get\" command\
    \ will gets the web semantics configuration for\n  a bucket and displays a JSON\
    \ representation of the configuration.\n\n  In Google Cloud Storage, this would\
    \ look like:\n\n    {\n      \"notFoundPage\": \"404.html\",\n      \"mainPageSuffix\"\
    : \"index.html\"\n    }\n"
  generated_using: *id002
- !Command
  command:
  - gsutil
  - du
  positional: []
  named:
  - !Flag
    description: "Ends each output line with a 0 byte rather than a newline. This\n\
      can be useful to make the output more easily machine-readable."
    synonyms:
    - '-0'
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Includes non-current object versions / generations in the listing\n\
      (only useful with a versioning-enabled bucket). Also prints\ngeneration and\
      \ metageneration for each listed object."
    synonyms:
    - -a
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: Includes a grand total at the end of the output.
    synonyms:
    - -c
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "A pattern to exclude from reporting. Example: -e \"*.o\" would\n\
      exclude any object that ends in \".o\". Can be specified multiple\ntimes."
    synonyms:
    - -e
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: Displays only the grand total for each argument.
    synonyms:
    - -s
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Similar to -e, but excludes patterns from the given file. The\n\
      patterns to exclude should be one per line."
    synonyms:
    - -X
    args: !EmptyFlagArg {}
    optional: true
  parent: *id001
  subcommands: []
  usage: []
  help_flag: !Flag
    description: "Prints object sizes in human-readable format (e.g., 1 KiB,\n234\
      \ MiB, 2GiB, etc.)"
    synonyms:
    - -h
    args: !EmptyFlagArg {}
    optional: true
  usage_flag:
  version_flag:
  help_text: "NAME\n  du - Display object size usage\n\n\nSYNOPSIS\n\n  gsutil du\
    \ url...\n\n\n\nDESCRIPTION\n  The du command displays the amount of space (in\
    \ bytes) being used by the\n  objects in the file or object hierarchy under a\
    \ given URL. The syntax emulates\n  the Linux du command (which stands for disk\
    \ usage). For example, the command:\n\n  gsutil du -s gs://your-bucket/dir\n\n\
    \  will report the total space used by all objects under gs://your-bucket/dir\
    \ and\n  any sub-directories.\n\n\nOPTIONS\n  -0          Ends each output line\
    \ with a 0 byte rather than a newline. This\n              can be useful to make\
    \ the output more easily machine-readable.\n\n  -a          Includes non-current\
    \ object versions / generations in the listing\n              (only useful with\
    \ a versioning-enabled bucket). Also prints\n              generation and metageneration\
    \ for each listed object.\n\n  -c          Includes a grand total at the end of\
    \ the output.\n\n  -e          A pattern to exclude from reporting. Example: -e\
    \ \"*.o\" would\n              exclude any object that ends in \".o\". Can be\
    \ specified multiple\n              times.\n\n  -h          Prints object sizes\
    \ in human-readable format (e.g., 1 KiB,\n              234 MiB, 2GiB, etc.)\n\
    \n  -s          Displays only the grand total for each argument.\n\n  -X     \
    \     Similar to -e, but excludes patterns from the given file. The\n        \
    \      patterns to exclude should be one per line.\n\n\nEXAMPLES\n  To list the\
    \ size of all objects in a bucket:\n\n    gsutil du gs://bucketname\n\n  To list\
    \ the size of all objects underneath a prefix:\n\n    gsutil du gs://bucketname/prefix/*\n\
    \n  To print the total number of bytes in a bucket, in human-readable form:\n\n\
    \    gsutil du -ch gs://bucketname\n\n  To see a summary of the total bytes in\
    \ the two given buckets:\n\n    gsutil du -s gs://bucket1 gs://bucket2\n\n  To\
    \ list the size of all objects in a versioned bucket, including objects that\n\
    \  are not the latest:\n\n    gsutil du -a gs://bucketname\n\n  To list all objects\
    \ in a bucket, except objects that end in \".bak\",\n  with each object printed\
    \ ending in a null byte:\n\n    gsutil du -e \"*.bak\" -0 gs://bucketname\n\n\
    \  To get a total of all buckets in a project with a grand total for an entire\n\
    \  project:\n\n      gsutil -o GSUtil:default_project_id=project-name du -shc\n"
  generated_using: *id002
- !Command
  command:
  - gsutil
  - mb
  positional: []
  named:
  - !Flag
    description: Specifies the default storage class. Default is "Standard".
    synonyms:
    - -c
    args: !SimpleFlagArg
      name: class
    optional: true
  - !Flag
    description: "Can be any multi-regional or regional location. See\nhttps://cloud.google.com/storage/docs/storage-classes\n\
      for a discussion of this distinction. Default is US.\nLocations are case insensitive."
    synonyms:
    - -l
    args: !SimpleFlagArg
      name: location
    optional: true
  - !Flag
    description: Specifies the project ID under which to create the bucket.
    synonyms:
    - -p
    args: !SimpleFlagArg
      name: proj_id
    optional: true
  - !Flag
    description: Same as -c.
    synonyms:
    - -s
    args: !SimpleFlagArg
      name: class
    optional: true
  parent: *id001
  subcommands: []
  usage: []
  help_flag:
  usage_flag:
  version_flag:
  help_text: "NAME\n  mb - Make buckets\n\n\nSYNOPSIS\n\n  gsutil mb [-c class] [-l\
    \ location] [-p proj_id] url...\n\n\n\nDESCRIPTION\n  The mb command creates a\
    \ new bucket. Google Cloud Storage has a single\n  namespace, so you are not allowed\
    \ to create a bucket with a name already\n  in use by another user. You can, however,\
    \ carve out parts of the bucket name\n  space corresponding to your company's\
    \ domain name (see \"gsutil help naming\").\n\n  If you don't specify a project\
    \ ID using the -p option, the bucket is created\n  using the default project ID\
    \ specified in your gsutil configuration file\n  (see \"gsutil help config\").\
    \ For more details about projects see \"gsutil help\n  projects\".\n\n  The -c\
    \ and -l options specify the storage class and location, respectively,\n  for\
    \ the bucket. Once a bucket is created in a given location and with a\n  given\
    \ storage class, it cannot be moved to a different location, and the\n  storage\
    \ class cannot be changed. Instead, you would need to create a new\n  bucket and\
    \ move the data over and then delete the original bucket.\n\n\nBUCKET STORAGE\
    \ CLASSES\n  You can specify one of the `storage classes\n  <https://cloud.google.com/storage/docs/storage-classes>`_\
    \ for a bucket\n  with the -c option.\n\n  Example:\n\n    gsutil mb -c nearline\
    \ gs://some-bucket\n\n  See online documentation for\n  `pricing <https://cloud.google.com/storage/pricing>`_\
    \ and\n  `SLA <https://cloud.google.com/storage/sla>`_ details.\n\n  If you don't\
    \ specify a -c option, the bucket is created with the\n  default storage class\
    \ Standard Storage, which is equivalent to Multi-Regional\n  Storage or Regional\
    \ Storage, depending on whether the bucket was created in\n  a multi-regional\
    \ location or regional location, respectively.\n\nBUCKET LOCATIONS\n  You can\
    \ specify one of the 'available locations\n  <https://cloud.google.com/storage/docs/bucket-locations>`_\
    \ for a bucket\n  with the -l option.\n\n  Examples:\n\n    gsutil mb -l asia\
    \ gs://some-bucket\n\n    gsutil mb -c regional -l us-east1 gs://some-bucket\n\
    \n  If you don't specify a -l option, the bucket is created in the default\n \
    \ location (US).\n\nOPTIONS\n  -c class          Specifies the default storage\
    \ class. Default is \"Standard\".\n\n  -l location       Can be any multi-regional\
    \ or regional location. See\n                    https://cloud.google.com/storage/docs/storage-classes\n\
    \                    for a discussion of this distinction. Default is US.\n  \
    \                  Locations are case insensitive.\n\n  -p proj_id        Specifies\
    \ the project ID under which to create the bucket.\n\n  -s class          Same\
    \ as -c.\n"
  generated_using: *id002
- !Command
  command:
  - gsutil
  - rewrite
  positional:
  - !Positional
    description: "For example, the command:\ngsutil rewrite -k gs://bucket/**"
    position: 0
    name: objects.
    optional: false
  named:
  - !Flag
    description: "Continues silently (without printing error messages) despite\nerrors\
      \ when rewriting multiple objects. If some of the objects\ncould not be rewritten,\
      \ gsutil's exit status will be non-zero\neven if this flag is set. This option\
      \ is implicitly set when\nrunning \"gsutil -m rewrite ...\"."
    synonyms:
    - -f
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Causes gsutil to read the list of objects to rewrite from stdin.\n\
      This allows you to run a program that generates the list of\nobjects to rewrite."
    synonyms:
    - -I
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Rewrite the objects to the current encryption key specific in\n\
      your boto configuration file. If encryption_key is specified,\nencrypt all objects\
      \ with this key. If encryption_key is\nunspecified, decrypt all objects. See\
      \ `gsutil help encryption`\nfor details on encryption configuration."
    synonyms:
    - -k
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Rewrite objects with the bucket's default object ACL instead of\n\
      the existing object ACL. This is needed if you do not have\nOWNER permission\
      \ on the object."
    synonyms:
    - -O
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "The -R and -r options are synonymous. Causes bucket or bucket\n\
      subdirectory contents to be rewritten recursively."
    synonyms:
    - -R
    - -r
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: Rewrite objects using the specified storage class.
    synonyms:
    - -s
    args: !SimpleFlagArg
      name: class
    optional: true
  parent: *id001
  subcommands: []
  usage: []
  help_flag:
  usage_flag:
  version_flag:
  help_text: "NAME\n  rewrite - Rewrite objects\n\n\nSYNOPSIS\n\n  gsutil rewrite\
    \ -k [-f] [-r] url...\n  gsutil rewrite -k [-f] [-r] -I\n\n\n\nDESCRIPTION\n \
    \ The gsutil rewrite command rewrites cloud objects, applying the specified\n\
    \  transformations to them. The transformation(s) are atomic and\n  applied based\
    \ on the input transformation flags. Object metadata values are\n  preserved unless\
    \ altered by a transformation.\n\n  The -k flag is supported to add, rotate, or\
    \ remove encryption keys on\n  objects.  For example, the command:\n\n    gsutil\
    \ rewrite -k gs://bucket/**\n\n  will update all objects in gs://bucket with the\
    \ current encryption key\n  from your boto config file.\n\n  You can also use\
    \ the -r option to specify recursive object transform; this is\n  synonymous with\
    \ the ** wildcard. Thus, either of the following two commands\n  will perform\
    \ encryption key transforms on gs://bucket/subdir and all objects\n  and subdirectories\
    \ under it:\n\n    gsutil rewrite -k gs://bucket/subdir**\n    gsutil rewrite\
    \ -k -r gs://bucket/subdir\n\n  The rewrite command acts only on live object versions,\
    \ so specifying a\n  URL with a generation will fail. If you want to rewrite an\
    \ archived\n  generation, first copy it to the live version, then rewrite it,\
    \ for example:\n\n    gsutil cp gs://bucket/object#123 gs://bucket/object\n  \
    \  gsutil rewrite -k gs://bucket/object\n\n  You can use the -s option to specify\
    \ a new storage class for objects.  For\n  example, the command:\n\n    gsutil\
    \ rewrite -s nearline gs://bucket/foo\n\n  will rewrite the object, changing its\
    \ storage class to nearline.\n\n  The rewrite command will skip objects that are\
    \ already in the desired state.\n  For example, if you run:\n\n    gsutil rewrite\
    \ -k gs://bucket/**\n\n  and gs://bucket contains objects that already match the\
    \ encryption\n  configuration, gsutil will skip rewriting those objects and only\
    \ rewrite\n  objects that do not match the encryption configuration. If you specify\n\
    \  multiple transformations, gsutil will only skip those that would not change\n\
    \  the object's state. For example, if you run:\n\n    gsutil rewrite -s nearline\
    \ -k gs://bucket/**\n\n  and gs://bucket contains objects that already match the\
    \ encryption\n  configuration but have a storage class of standard, the only transformation\n\
    \  applied to those objects would be the change in storage class.\n\n  You can\
    \ pass a list of URLs (one per line) to rewrite on stdin instead of as\n  command\
    \ line arguments by using the -I option. This allows you to use gsutil\n  in a\
    \ pipeline to rewrite objects identified by a program, such as:\n\n    some_program\
    \ | gsutil -m rewrite -k -I\n\n  The contents of stdin can name cloud URLs and\
    \ wildcards of cloud URLs.\n\n  The rewrite command requires OWNER permissions\
    \ on each object to preserve\n  object ACLs. You can bypass this by using the\
    \ -O flag, which will cause\n  gsutil not to read the object's ACL and instead\
    \ apply the default object ACL\n  to the rewritten object:\n\n    gsutil rewrite\
    \ -k -O gs://bucket/**\n\n\nOPTIONS\n  -f          Continues silently (without\
    \ printing error messages) despite\n              errors when rewriting multiple\
    \ objects. If some of the objects\n              could not be rewritten, gsutil's\
    \ exit status will be non-zero\n              even if this flag is set. This option\
    \ is implicitly set when\n              running \"gsutil -m rewrite ...\".\n\n\
    \  -I          Causes gsutil to read the list of objects to rewrite from stdin.\n\
    \              This allows you to run a program that generates the list of\n \
    \             objects to rewrite.\n\n  -k          Rewrite the objects to the\
    \ current encryption key specific in\n              your boto configuration file.\
    \ If encryption_key is specified,\n              encrypt all objects with this\
    \ key. If encryption_key is\n              unspecified, decrypt all objects. See\
    \ `gsutil help encryption`\n              for details on encryption configuration.\n\
    \n  -O          Rewrite objects with the bucket's default object ACL instead of\n\
    \              the existing object ACL. This is needed if you do not have\n  \
    \            OWNER permission on the object.\n\n  -R, -r      The -R and -r options\
    \ are synonymous. Causes bucket or bucket\n              subdirectory contents\
    \ to be rewritten recursively.\n\n  -s <class>  Rewrite objects using the specified\
    \ storage class.\n"
  generated_using: *id002
- !Command
  command:
  - gsutil
  - hash
  positional: []
  named:
  - !Flag
    description: Calculate a CRC32c hash for the file.
    synonyms:
    - -c
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: Calculate a MD5 hash for the file.
    synonyms:
    - -m
    args: !EmptyFlagArg {}
    optional: true
  parent: *id001
  subcommands: []
  usage: []
  help_flag: !Flag
    description: Output hashes in hex format. By default, gsutil uses base64.
    synonyms:
    - -h
    args: !EmptyFlagArg {}
    optional: true
  usage_flag:
  version_flag:
  help_text: "NAME\n  hash - Calculate file hashes\n\n\nSYNOPSIS\n\n  gsutil hash\
    \ [-c] [-h] [-m] filename...\n\n\n\nDESCRIPTION\n  The hash command calculates\
    \ hashes on a local file that can be used to compare\n  with gsutil ls -L output.\
    \ If a specific hash option is not provided, this\n  command calculates all gsutil-supported\
    \ hashes for the file.\n\n  Note that gsutil automatically performs hash validation\
    \ when uploading or\n  downloading files, so this command is only needed if you\
    \ want to write a\n  script that separately checks the hash for some reason.\n\
    \n  If you calculate a CRC32c hash for the file without a precompiled crcmod\n\
    \  installation, hashing will be very slow. See \"gsutil help crcmod\" for details.\n\
    \nOPTIONS\n  -c          Calculate a CRC32c hash for the file.\n\n  -h       \
    \   Output hashes in hex format. By default, gsutil uses base64.\n\n  -m     \
    \     Calculate a MD5 hash for the file.\n"
  generated_using: *id002
- !Command
  command:
  - gsutil
  - perfdiag
  positional: []
  named:
  - !Flag
    description: "Sets the number of objects to use when downloading and uploading\n\
      files during tests. Defaults to 5."
    synonyms:
    - -n
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Sets the number of processes to use while running throughput\nexperiments.\
      \ The default value is 1."
    synonyms:
    - -c
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Sets the number of threads per process to use while running\nthroughput\
      \ experiments. Each process will receive an equal number\nof threads. The default\
      \ value is 1.\nNote: All specified threads and processes will be created, but\
      \ may\nnot by saturated with work if too few objects (specified with -n)\nand\
      \ too few components (specified with -y) are specified."
    synonyms:
    - -k
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Sets the type of parallelism to be used (only applicable when\n\
      threads or processes are specified and threads * processes > 1).\nThe default\
      \ is to use fan. Must be one of the following:\nfan\nUse one thread per object.\
      \ This is akin to using gsutil -m cp,\nwith sliced object download / parallel\
      \ composite upload\ndisabled.\nslice\nUse Y (specified with -y) threads for\
      \ each object, transferring\none object at a time. This is akin to using parallel\
      \ object\ndownload / parallel composite upload, without -m. Sliced\nuploads\
      \ not supported for s3.\nboth\nUse Y (specified with -y) threads for each object,\
      \ transferring\nmultiple objects at a time. This is akin to simultaneously\n\
      using sliced object download / parallel composite upload and\ngsutil -m cp.\
      \ Sliced uploads not supported for s3."
    synonyms:
    - -p
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Sets the number of slices to divide each file/object into while\n\
      transferring data. Only applicable with the slice (or both)\nparallelism type.\
      \ The default is 4 slices."
    synonyms:
    - -y
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Sets the size (in bytes) for each of the N (set with -n) objects\n\
      used in the read and write throughput tests. The default is 1 MiB.\nThis can\
      \ also be specified using byte suffixes such as 500K or 1M.\nNote: these values\
      \ are interpreted as multiples of 1024 (K=1024,\nM=1024*1024, etc.)\nNote: If\
      \ rthru_file or wthru_file are performed, N (set with -n)\ntimes as much disk\
      \ space as specified will be required for the\noperation."
    synonyms:
    - -s
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Sets the directory to store temporary local files in. If not\nspecified,\
      \ a default temporary directory will be used."
    synonyms:
    - -d
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Sets the list of diagnostic tests to perform. The default is to\n\
      run the lat, rthru, and wthru diagnostic tests. Must be a\ncomma-separated list\
      \ containing one or more of the following:\nlat\nFor N (set with -n) objects,\
      \ write the object, retrieve its\nmetadata, read the object, and finally delete\
      \ the object.\nRecord the latency of each operation.\nlist\nWrite N (set with\
      \ -n) objects to the bucket, record how long\nit takes for the eventually consistent\
      \ listing call to return\nthe N objects in its result, delete the N objects,\
      \ then record\nhow long it takes listing to stop returning the N objects.\n\
      rthru\nRuns N (set with -n) read operations, with at most C\n(set with -c) reads\
      \ outstanding at any given time.\nrthru_file\nThe same as rthru, but simultaneously\
      \ writes data to the disk,\nto gauge the performance impact of the local disk\
      \ on downloads.\nwthru\nRuns N (set with -n) write operations, with at most\
      \ C\n(set with -c) writes outstanding at any given time.\nwthru_file\nThe same\
      \ as wthru, but simultaneously reads data from the disk,\nto gauge the performance\
      \ impact of the local disk on uploads."
    synonyms:
    - -t
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Adds metadata to the result JSON file. Multiple -m values can be\n\
      specified. Example:\ngsutil perfdiag -m \"key1:val1\" -m \"key2:val2\" gs://bucketname\n\
      Each metadata key will be added to the top-level \"metadata\"\ndictionary in\
      \ the output JSON file."
    synonyms:
    - -m
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Writes the results of the diagnostic to an output file. The output\n\
      is a JSON file containing system information and performance\ndiagnostic results.\
      \ The file can be read and reported later using\nthe -i option."
    synonyms:
    - -o
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Reads the JSON output file created using the -o command and prints\n\
      a formatted description of the results."
    synonyms:
    - -i
    args: !EmptyFlagArg {}
    optional: true
  parent: *id001
  subcommands: []
  usage: []
  help_flag:
  usage_flag:
  version_flag:
  help_text: "NAME\n  perfdiag - Run performance diagnostic\n\n\nSYNOPSIS\n\n  gsutil\
    \ perfdiag [-i in.json]\n  gsutil perfdiag [-o out.json] [-n objects] [-c processes]\n\
    \      [-k threads] [-p parallelism type] [-y slices] [-s size] [-d directory]\n\
    \      [-t tests] url...\n\n\n\nDESCRIPTION\n  The perfdiag command runs a suite\
    \ of diagnostic tests for a given Google\n  Storage bucket.\n\n  The 'url' parameter\
    \ must name an existing bucket (e.g. gs://foo) to which\n  the user has write\
    \ permission. Several test files will be uploaded to and\n  downloaded from this\
    \ bucket. All test files will be deleted at the completion\n  of the diagnostic\
    \ if it finishes successfully.\n\n  gsutil performance can be impacted by many\
    \ factors at the client, server,\n  and in-between, such as: CPU speed; available\
    \ memory; the access path to the\n  local disk; network bandwidth; contention\
    \ and error rates along the path\n  between gsutil and Google; operating system\
    \ buffering configuration; and\n  firewalls and other network elements. The perfdiag\
    \ command is provided so\n  that customers can run a known measurement suite when\
    \ troubleshooting\n  performance problems.\n\n\nPROVIDING DIAGNOSTIC OUTPUT TO\
    \ GOOGLE CLOUD STORAGE TEAM\n  If the Google Cloud Storage Team asks you to run\
    \ a performance diagnostic\n  please use the following command, and email the\
    \ output file (output.json)\n  to gs-team@google.com:\n\n    gsutil perfdiag -o\
    \ output.json gs://your-bucket\n\n\nOPTIONS\n  -n          Sets the number of\
    \ objects to use when downloading and uploading\n              files during tests.\
    \ Defaults to 5.\n\n  -c          Sets the number of processes to use while running\
    \ throughput\n              experiments. The default value is 1.\n\n  -k     \
    \     Sets the number of threads per process to use while running\n          \
    \    throughput experiments. Each process will receive an equal number\n     \
    \         of threads. The default value is 1.\n\n              Note: All specified\
    \ threads and processes will be created, but may\n              not by saturated\
    \ with work if too few objects (specified with -n)\n              and too few\
    \ components (specified with -y) are specified.\n\n  -p          Sets the type\
    \ of parallelism to be used (only applicable when\n              threads or processes\
    \ are specified and threads * processes > 1).\n              The default is to\
    \ use fan. Must be one of the following:\n\n              fan\n              \
    \   Use one thread per object. This is akin to using gsutil -m cp,\n         \
    \        with sliced object download / parallel composite upload\n           \
    \      disabled.\n\n              slice\n                 Use Y (specified with\
    \ -y) threads for each object, transferring\n                 one object at a\
    \ time. This is akin to using parallel object\n                 download / parallel\
    \ composite upload, without -m. Sliced\n                 uploads not supported\
    \ for s3.\n\n              both\n                 Use Y (specified with -y) threads\
    \ for each object, transferring\n                 multiple objects at a time.\
    \ This is akin to simultaneously\n                 using sliced object download\
    \ / parallel composite upload and\n                 gsutil -m cp. Sliced uploads\
    \ not supported for s3.\n\n  -y          Sets the number of slices to divide each\
    \ file/object into while\n              transferring data. Only applicable with\
    \ the slice (or both)\n              parallelism type. The default is 4 slices.\n\
    \n  -s          Sets the size (in bytes) for each of the N (set with -n) objects\n\
    \              used in the read and write throughput tests. The default is 1 MiB.\n\
    \              This can also be specified using byte suffixes such as 500K or\
    \ 1M.\n              Note: these values are interpreted as multiples of 1024 (K=1024,\n\
    \              M=1024*1024, etc.)\n              Note: If rthru_file or wthru_file\
    \ are performed, N (set with -n)\n              times as much disk space as specified\
    \ will be required for the\n              operation.\n\n  -d          Sets the\
    \ directory to store temporary local files in. If not\n              specified,\
    \ a default temporary directory will be used.\n\n  -t          Sets the list of\
    \ diagnostic tests to perform. The default is to\n              run the lat, rthru,\
    \ and wthru diagnostic tests. Must be a\n              comma-separated list containing\
    \ one or more of the following:\n\n              lat\n                 For N (set\
    \ with -n) objects, write the object, retrieve its\n                 metadata,\
    \ read the object, and finally delete the object.\n                 Record the\
    \ latency of each operation.\n\n              list\n                 Write N (set\
    \ with -n) objects to the bucket, record how long\n                 it takes for\
    \ the eventually consistent listing call to return\n                 the N objects\
    \ in its result, delete the N objects, then record\n                 how long\
    \ it takes listing to stop returning the N objects.\n\n              rthru\n \
    \                Runs N (set with -n) read operations, with at most C\n      \
    \           (set with -c) reads outstanding at any given time.\n\n           \
    \   rthru_file\n                 The same as rthru, but simultaneously writes\
    \ data to the disk,\n                 to gauge the performance impact of the local\
    \ disk on downloads.\n\n              wthru\n                 Runs N (set with\
    \ -n) write operations, with at most C\n                 (set with -c) writes\
    \ outstanding at any given time.\n\n              wthru_file\n               \
    \  The same as wthru, but simultaneously reads data from the disk,\n         \
    \        to gauge the performance impact of the local disk on uploads.\n\n  -m\
    \          Adds metadata to the result JSON file. Multiple -m values can be\n\
    \              specified. Example:\n\n                  gsutil perfdiag -m \"\
    key1:val1\" -m \"key2:val2\" gs://bucketname\n\n              Each metadata key\
    \ will be added to the top-level \"metadata\"\n              dictionary in the\
    \ output JSON file.\n\n  -o          Writes the results of the diagnostic to an\
    \ output file. The output\n              is a JSON file containing system information\
    \ and performance\n              diagnostic results. The file can be read and\
    \ reported later using\n              the -i option.\n\n  -i          Reads the\
    \ JSON output file created using the -o command and prints\n              a formatted\
    \ description of the results.\n\n\nMEASURING AVAILABILITY\n  The perfdiag command\
    \ ignores the boto num_retries configuration parameter.\n  Instead, it always\
    \ retries on HTTP errors in the 500 range and keeps track of\n  how many 500 errors\
    \ were encountered during the test. The availability\n  measurement is reported\
    \ at the end of the test.\n\n  Note that HTTP responses are only recorded when\
    \ the request was made in a\n  single process. When using multiple processes or\
    \ threads, read and write\n  throughput measurements are performed in an external\
    \ process, so the\n  availability numbers reported won't include the throughput\
    \ measurements.\n\n\nNOTE\n  The perfdiag command collects system information.\
    \ It collects your IP address,\n  executes DNS queries to Google servers and collects\
    \ the results, and collects\n  network statistics information from the output\
    \ of netstat -s. It will also\n  attempt to connect to your proxy server if you\
    \ have one configured. None of\n  this information will be sent to Google unless\
    \ you choose to send it.\n"
  generated_using: *id002
- !Command
  command:
  - gsutil
  - rb
  positional:
  - !Positional
    description: Be certain you want to delete a bucket before you do so, as once
      it is
    position: 0
    name: them.
    optional: false
  named:
  - !Flag
    description: "Continues silently (without printing error messages) despite\nerrors\
      \ when removing buckets. If some buckets couldn't be removed,\ngsutil's exit\
      \ status will be non-zero even if this flag is set.\n"
    synonyms:
    - -f
    args: !EmptyFlagArg {}
    optional: true
  parent: *id001
  subcommands: []
  usage: []
  help_flag:
  usage_flag:
  version_flag:
  help_text: "NAME\n  rb - Remove buckets\n\n\nSYNOPSIS\n\n  gsutil rb [-f] url...\n\
    \n\n\nDESCRIPTION\n  The rb command deletes a bucket. Buckets must be empty before\
    \ you can delete\n  them.\n\n  Be certain you want to delete a bucket before you\
    \ do so, as once it is\n  deleted the name becomes available and another user\
    \ may create a bucket with\n  that name. (But see also \"DOMAIN NAMED BUCKETS\"\
    \ under \"gsutil help naming\"\n  for help carving out parts of the bucket name\
    \ space.)\n\n\nOPTIONS\n  -f          Continues silently (without printing error\
    \ messages) despite\n              errors when removing buckets. If some buckets\
    \ couldn't be removed,\n              gsutil's exit status will be non-zero even\
    \ if this flag is set.\n"
  generated_using: *id002
- !Command
  command:
  - gsutil
  - ls
  positional:
  - !Positional
    description: 2012-03-02T19:25:17Z  gs://bucket/obj1
    position: 0
    name: '2276224'
    optional: false
  - !Positional
    description: 2012-03-02T19:30:27Z  gs://bucket/obj2
    position: 1
    name: '3914624'
    optional: false
  named:
  - !Flag
    description: Prints long listing (owner, length).
    synonyms:
    - -l
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Prints even more detail than -l.  Note: If you use this option\n\
      with the (non-default) XML API it will generate an additional\nrequest per object\
      \ being listed, which makes the -L option run\nmuch more slowly (and cost more)\
      \ using the XML API than the\ndefault JSON API."
    synonyms:
    - -L
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "List matching subdirectory names instead of contents, and do not\n\
      recurse into matching subdirectories even if the -R option is\nspecified."
    synonyms:
    - -d
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: Prints info about the bucket when used with a bucket URL.
    synonyms:
    - -b
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: Specifies the project ID to use for listing buckets.
    synonyms:
    - -p
    args: !SimpleFlagArg
      name: proj_id
    optional: true
  - !Flag
    description: "Requests a recursive listing, performing at least one listing\n\
      operation per subdirectory. If you have a large number of\nsubdirectories and\
      \ do not require recursive-style output ordering,\nyou may be able to instead\
      \ use wildcards to perform a flat\nlisting, e.g.  `gsutil ls gs://mybucket/**`,\
      \ which will generally\nperform fewer listing operations."
    synonyms:
    - -R
    - -r
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Includes non-current object versions / generations in the listing\n\
      (only useful with a versioning-enabled bucket). If combined with\n-l option\
      \ also prints metageneration for each listed object."
    synonyms:
    - -a
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: Include ETag in long listing (-l) output.
    synonyms:
    - -e
    args: !EmptyFlagArg {}
    optional: true
  parent: *id001
  subcommands: []
  usage: []
  help_flag: !Flag
    description: "When used with -l, prints object sizes in human readable format\n\
      (e.g., 1 KiB, 234 MiB, 2 GiB, etc.)"
    synonyms:
    - -h
    args: !EmptyFlagArg {}
    optional: true
  usage_flag:
  version_flag:
  help_text: "NAME\n  ls - List providers, buckets, or objects\n\n\nSYNOPSIS\n\n \
    \ gsutil ls [-a] [-b] [-d] [-l] [-L] [-r] [-p proj_id] url...\n\n\n\nLISTING PROVIDERS,\
    \ BUCKETS, SUBDIRECTORIES, AND OBJECTS\n  If you run gsutil ls without URLs, it\
    \ lists all of the Google Cloud Storage\n  buckets under your default project\
    \ ID:\n\n    gsutil ls\n\n  (For details about projects, see \"gsutil help projects\"\
    \ and also the -p\n  option in the OPTIONS section below.)\n\n  If you specify\
    \ one or more provider URLs, gsutil ls will list buckets at\n  each listed provider:\n\
    \n    gsutil ls gs://\n\n  If you specify bucket URLs, gsutil ls will list objects\
    \ at the top level of\n  each bucket, along with the names of each subdirectory.\
    \ For example:\n\n    gsutil ls gs://bucket\n\n  might produce output like:\n\n\
    \    gs://bucket/obj1.htm\n    gs://bucket/obj2.htm\n    gs://bucket/images1/\n\
    \    gs://bucket/images2/\n\n  The \"/\" at the end of the last 2 URLs tells you\
    \ they are subdirectories,\n  which you can list using:\n\n    gsutil ls gs://bucket/images*\n\
    \n  If you specify object URLs, gsutil ls will list the specified objects. For\n\
    \  example:\n\n    gsutil ls gs://bucket/*.txt\n\n  will list all files whose\
    \ name matches the above wildcard at the top level\n  of the bucket.\n\n  See\
    \ \"gsutil help wildcards\" for more details on working with wildcards.\n\n\n\
    DIRECTORY BY DIRECTORY, FLAT, and RECURSIVE LISTINGS\n  Listing a bucket or subdirectory\
    \ (as illustrated near the end of the previous\n  section) only shows the objects\
    \ and names of subdirectories it contains. You\n  can list all objects in a bucket\
    \ by using the -r option. For example:\n\n    gsutil ls -r gs://bucket\n\n  will\
    \ list the top-level objects and buckets, then the objects and\n  buckets under\
    \ gs://bucket/images1, then those under gs://bucket/images2, etc.\n\n  If you\
    \ want to see all objects in the bucket in one \"flat\" listing use the\n  recursive\
    \ (\"**\") wildcard, like:\n\n    gsutil ls -r gs://bucket/**\n\n  or, for a flat\
    \ listing of a subdirectory:\n\n    gsutil ls -r gs://bucket/dir/**\n\n  If you\
    \ want to see only the subdirectory itself, use the -d option:\n\n    gsutil ls\
    \ -d gs://bucket/dir\n\n\nLISTING OBJECT DETAILS\n  If you specify the -l option,\
    \ gsutil will output additional information\n  about each matching provider, bucket,\
    \ subdirectory, or object. For example:\n\n    gsutil ls -l gs://bucket/*.txt\n\
    \n  will print the object size, creation time stamp, and name of each matching\n\
    \  object, along with the total count and sum of sizes of all matching objects:\n\
    \n       2276224  2012-03-02T19:25:17Z  gs://bucket/obj1\n       3914624  2012-03-02T19:30:27Z\
    \  gs://bucket/obj2\n    TOTAL: 2 objects, 6190848 bytes (5.9 MiB)\n\n  Note that\
    \ the total listed in parentheses above is in mebibytes (or gibibytes,\n  tebibytes,\
    \ etc.), which corresponds to the unit of billing measurement for\n  Google Cloud\
    \ Storage.\n\n  You can get a listing of all the objects in the top-level bucket\
    \ directory\n  (along with the total count and sum of sizes) using a command like:\n\
    \n    gsutil ls -l gs://bucket\n\n  To print additional detail about objects and\
    \ buckets use the gsutil ls -L\n  option. For example:\n\n    gsutil ls -L gs://bucket/obj1\n\
    \n  will print something like:\n\n    gs://bucket/obj1:\n            Creation\
    \ time:                    Fri, 21 Oct 2016 19:25:17 GMT\n            Update time:\
    \                      Fri, 21 Oct 2016 21:17:59 GMT\n            Storage class\
    \ update time:        Fri, 21 Oct 2016 22:12:32 GMT\n            Size:       \
    \                      2276224\n            Cache-Control:                   \
    \ private, max-age=0\n            Content-Type:                     application/x-executable\n\
    \            ETag:                             5ca6796417570a586723b7344afffc81\n\
    \            Generation:                       1378862725952000\n            Metageneration:\
    \                   1\n            ACL:\n    [\n      {\n        \"entity\": \"\
    group-00b4903a97163d99003117abe64d292561d2b4074fc90ce5c0e35ac45f66ad70\",\n  \
    \      \"entityId\": \"00b4903a97163d99003117abe64d292561d2b4074fc90ce5c0e35ac45f66ad70\"\
    ,\n        \"role\": \"OWNER\"\n      }\n    ]\n    TOTAL: 1 objects, 2276224\
    \ bytes (2.17 MiB)\n\n  Note that some fields above (time updated, storage class\
    \ update time) are\n  not available with the (non-default) XML API.\n\n  Also\
    \ note that the Storage class update time field does not display unless it\n \
    \ differs from Creation time.\n\n  See also \"gsutil help acl\" for getting a\
    \ more readable version of the ACL.\n\n\nLISTING BUCKET DETAILS\n  If you want\
    \ to see information about the bucket itself, use the -b\n  option. For example:\n\
    \n    gsutil ls -L -b gs://bucket\n\n  will print something like:\n\n    gs://bucket/\
    \ :\n            Storage class:                MULTI_REGIONAL\n            Location\
    \ constraint:          US\n            Versioning enabled:           True\n  \
    \          Logging configuration:        None\n            Website configuration:\
    \        None\n            CORS configuration:           Present\n           \
    \ Lifecycle configuration:      None\n            Labels:                    \
    \   None\n            Time created:                 Fri, 21 Oct 2016 19:25:17\
    \ GMT\n            Time updated:                 Fri, 21 Oct 2016 21:17:59 GMT\n\
    \            Metageneration:               1\n            ACL:\n    [\n      {\n\
    \        \"entity\": \"group-00b4903a97163d99003117abe64d292561d2b4074fc90ce5c0e35ac45f66ad70\"\
    ,\n        \"entityId\": \"00b4903a97163d99003117abe64d292561d2b4074fc90ce5c0e35ac45f66ad70\"\
    ,\n        \"role\": \"OWNER\"\n      }\n    ]\n            Default ACL:\n   \
    \ [\n      {\n        \"entity\": \"group-00b4903a97163d99003117abe64d292561d2b4074fc90ce5c0e35ac45f66ad70\"\
    ,\n        \"entityId\": \"00b4903a97163d99003117abe64d292561d2b4074fc90ce5c0e35ac45f66ad70\"\
    ,\n        \"role\": \"OWNER\"\n      }\n    ]\n\n  Note that some fields above\
    \ (time created, time updated, metageneration) are\n  not available with the (non-default)\
    \ XML API.\n\n\nOPTIONS\n  -l          Prints long listing (owner, length).\n\n\
    \  -L          Prints even more detail than -l.  Note: If you use this option\n\
    \              with the (non-default) XML API it will generate an additional\n\
    \              request per object being listed, which makes the -L option run\n\
    \              much more slowly (and cost more) using the XML API than the\n \
    \             default JSON API.\n\n  -d          List matching subdirectory names\
    \ instead of contents, and do not\n              recurse into matching subdirectories\
    \ even if the -R option is\n              specified.\n\n  -b          Prints info\
    \ about the bucket when used with a bucket URL.\n\n  -h          When used with\
    \ -l, prints object sizes in human readable format\n              (e.g., 1 KiB,\
    \ 234 MiB, 2 GiB, etc.)\n\n  -p proj_id  Specifies the project ID to use for listing\
    \ buckets.\n\n  -R, -r      Requests a recursive listing, performing at least\
    \ one listing\n              operation per subdirectory. If you have a large number\
    \ of\n              subdirectories and do not require recursive-style output ordering,\n\
    \              you may be able to instead use wildcards to perform a flat\n  \
    \            listing, e.g.  `gsutil ls gs://mybucket/**`, which will generally\n\
    \              perform fewer listing operations.\n\n  -a          Includes non-current\
    \ object versions / generations in the listing\n              (only useful with\
    \ a versioning-enabled bucket). If combined with\n              -l option also\
    \ prints metageneration for each listed object.\n\n  -e          Include ETag\
    \ in long listing (-l) output.\n"
  generated_using: *id002
- !Command
  command:
  - gsutil
  - config
  positional:
  - !Positional
    description: If you want to use credentials based on access key and secret (the
      older
    position: 0
    name: scopes.
    optional: false
  - !Positional
    description: aws_secret_access_key
    position: 0
    name: aws_access_key_id
    optional: false
  - !Positional
    description: gs_host
    position: 1
    name: gs_access_key_id
    optional: false
  - !Positional
    description: gs_json_port
    position: 2
    name: gs_json_host
    optional: false
  - !Positional
    description: gs_port
    position: 3
    name: gs_oauth2_refresh_token
    optional: false
  - !Positional
    description: s3_host
    position: 4
    name: gs_secret_access_key
    optional: false
  - !Positional
    description: '[Boto]'
    position: 5
    name: s3_port
    optional: false
  - !Positional
    description: proxy_port
    position: 6
    name: proxy
    optional: false
  - !Positional
    description: proxy_pass
    position: 7
    name: proxy_user
    optional: false
  - !Positional
    description: http_socket_timeout
    position: 8
    name: proxy_rdns
    optional: false
  - !Positional
    description: debug
    position: 9
    name: https_validate_certificates
    optional: false
  - !Positional
    description: num_retries
    position: 10
    name: max_retry_delay
    optional: false
  - !Positional
    description: '[GSUtil]'
    position: 0
    name: service_account
    optional: false
  - !Positional
    description: content_language
    position: 1
    name: check_hashes
    optional: false
  - !Positional
    description: default_project_id
    position: 0
    name: default_api_version
    optional: false
  - !Positional
    description: encryption_key
    position: 1
    name: disable_analytics_prompt
    optional: false
  - !Positional
    description: parallel_composite_upload_component_size
    position: 2
    name: json_api_version
    optional: false
  - !Positional
    description: sliced_object_download_component_size
    position: 3
    name: parallel_composite_upload_threshold
    optional: false
  - !Positional
    description: sliced_object_download_threshold
    position: 4
    name: sliced_object_download_max_components
    optional: false
  - !Positional
    description: parallel_thread_count
    position: 5
    name: parallel_process_count
    optional: false
  - !Positional
    description: resumable_threshold
    position: 6
    name: prefer_api
    optional: false
  - !Positional
    description: software_update_check_period
    position: 0
    name: rsync_buffer_lines
    optional: false
  - !Positional
    description: tab_completion_time_logs
    position: 1
    name: state_dir
    optional: false
  - !Positional
    description: task_estimation_threshold
    position: 2
    name: tab_completion_timeout
    optional: false
  - !Positional
    description: '[OAuth2]'
    position: 3
    name: use_magicfile
    optional: false
  - !Positional
    description: client_secret
    position: 4
    name: client_id
    optional: false
  - !Positional
    description: provider_authorization_uri
    position: 5
    name: oauth2_refresh_retries
    optional: false
  - !Positional
    description: provider_token_uri
    position: 6
    name: provider_label
    optional: false
  - !Positional
    description: UPDATING TO THE LATEST CONFIGURATION FILE
    position: 7
    name: token_cache
    optional: false
  named:
  - !Flag
    description: (see OPTIONS below) cause gsutil config to request a token
    synonyms:
    - -f
    args: !SimpleFlagArg
      name: options
    optional: true
  - !Flag
    description: "Prompt for Google Cloud Storage access key and secret (the older\n\
      authentication method before OAuth2 was supported) instead of\nobtaining an\
      \ OAuth2 token."
    synonyms:
    - -a
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Causes gsutil config to launch a browser to obtain OAuth2 approval\n\
      and the project ID instead of showing the URL for each and asking\nthe user\
      \ to open the browser. This will probably not work as\nexpected if you are running\
      \ gsutil from an ssh window, or using\ngsutil on Windows."
    synonyms:
    - -b
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: Prompt for service account credentials. This option requires that
    synonyms:
    - -e
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Write the configuration file without authentication configured.\n\
      This flag is mutually exlusive with all flags other than -o."
    synonyms:
    - -n
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: "Write the configuration to <file> instead of ~/.boto.\nUse '-' for\
      \ stdout."
    synonyms:
    - -o
    args: !SimpleFlagArg
      name: file
    optional: true
  - !Flag
    description: Request token restricted to read-only access.
    synonyms:
    - -r
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: Request additional OAuth2 <scope>.
    synonyms:
    - -s
    args: !SimpleFlagArg
      name: scope
    optional: true
  - !Flag
    description: Request token restricted to read-write access.
    synonyms:
    - -w
    args: !EmptyFlagArg {}
    optional: true
  parent: *id001
  subcommands: []
  usage: []
  help_flag:
  usage_flag:
  version_flag:
  help_text: "NAME\n  config - Obtain credentials and create configuration file\n\n\
    \nSYNOPSIS\n\n  gsutil [-D] config [-a] [-b] [-e] [-f] [-n] [-o <file>] [-r] [-s\
    \ <scope>] [-w]\n\n\n\nDESCRIPTION\n  The gsutil config command obtains access\
    \ credentials for Google Cloud\n  Storage and writes a boto/gsutil configuration\
    \ file containing the obtained\n  credentials along with a number of other configuration-controllable\
    \ values.\n\n  Unless specified otherwise (see OPTIONS), the configuration file\
    \ is written\n  to ~/.boto (i.e., the file .boto under the user's home directory).\
    \ If the\n  default file already exists, an attempt is made to rename the existing\
    \ file\n  to ~/.boto.bak; if that attempt fails the command will exit. A different\n\
    \  destination file can be specified with the -o option (see OPTIONS).\n\n  Because\
    \ the boto configuration file contains your credentials you should\n  keep its\
    \ file permissions set so no one but you has read access. (The file\n  is created\
    \ read-only when you run gsutil config.)\n\n\nCREDENTIALS\n  By default gsutil\
    \ config obtains OAuth2 credentials, and writes them\n  to the [Credentials] section\
    \ of the configuration file. The -r, -w,\n  -f options (see OPTIONS below) cause\
    \ gsutil config to request a token\n  with restricted scope; the resulting token\
    \ will be restricted to read-only\n  operations, read-write operations, or all\
    \ operations (including acl get/set,\n  defacl get/set, and logging get/'set on'/'set\
    \ off' operations). In\n  addition, -s <scope> can be used to request additional\
    \ (non-Google-Storage)\n  scopes.\n\n  If you want to use credentials based on\
    \ access key and secret (the older\n  authentication method before OAuth2 was\
    \ supported) instead of OAuth2,\n  see help about the -a option in the OPTIONS\
    \ section.\n\n  If you wish to use gsutil with other providers (or to copy data\
    \ back and\n  forth between multiple providers) you can edit their credentials\
    \ into the\n  [Credentials] section after creating the initial configuration file.\n\
    \n\nCONFIGURING SERVICE ACCOUNT CREDENTIALS\n  You can configure credentials for\
    \ service accounts using the gsutil config -e\n  option. Service accounts are\
    \ useful for authenticating on behalf of a service\n  or application (as opposed\
    \ to a user).\n\n  When you run gsutil config -e, you will be prompted for your\
    \ service account\n  email address and the path to your private key file. To get\
    \ this data,\n  follow the instructions on\n  `Service Accounts <https://cloud.google.com/storage/docs/authentication#generating-a-private-key>`_.\n\
    \n  Note that your service account will NOT be considered an Owner for the\n \
    \ purposes of API access (see \"gsutil help creds\" for more information about\n\
    \  this). See https://developers.google.com/identity/protocols/OAuth2ServiceAccount\n\
    \  for further information on service account authentication.\n\n\nCONFIGURATION\
    \ FILE SELECTION PROCEDURE\n  By default, gsutil will look for the configuration\
    \ file in /etc/boto.cfg and\n  ~/.boto. You can override this choice by setting\
    \ the BOTO_CONFIG environment\n  variable. This is also useful if you have several\
    \ different identities or\n  cloud storage environments: By setting up the credentials\
    \ and any additional\n  configuration in separate files for each, you can switch\
    \ environments by\n  changing environment variables.\n\n  You can also set up\
    \ a path of configuration files, by setting the BOTO_PATH\n  environment variable\
    \ to contain a \":\" delimited path (or \";\" for Windows).\n  For example setting\
    \ the BOTO_PATH environment variable to:\n\n    /etc/projects/my_group_project.boto.cfg:/home/mylogin/.boto\n\
    \n  will cause gsutil to load each configuration file found in the path in\n \
    \ order. This is useful if you want to set up some shared configuration\n  state\
    \ among many users: The shared state can go in the central shared file\n  ( /etc/projects/my_group_project.boto.cfg)\
    \ and each user's individual\n  credentials can be placed in the configuration\
    \ file in each of their home\n  directories. For security reasons, users should\
    \ never share credentials\n  via a shared configuration file.\n\n\nCONFIGURATION\
    \ FILE STRUCTURE\n  The configuration file contains a number of sections: [Credentials],\n\
    \  [Boto], [GSUtil], and [OAuth2]. If you edit the file, make sure to edit the\n\
    \  appropriate section (discussed below), and to be careful not to mis-edit\n\
    \  any of the setting names (like \"gs_access_key_id\") and not to remove the\n\
    \  section delimiters (like \"[Credentials]\").\n\n\nADDITIONAL CONFIGURATION-CONTROLLABLE\
    \ FEATURES\n  With the exception of setting up gsutil to work through a proxy,\
    \ most users\n  won't need to edit values in the boto configuration file; values\
    \ found in\n  the file tend to be of more specialized use than command line\n\
    \  option-controllable features. For information on setting up gsutil to work\n\
    \  through a proxy, see the comments preceding the proxy settings in your\n  .boto\
    \ file.\n\n  The following are the currently defined configuration settings, broken\n\
    \  down by section. Their use is documented in comments preceding each, in\n \
    \ the configuration file. If you see a setting you want to change that's not\n\
    \  listed in your current file, see the section below on Updating to the Latest\n\
    \  Configuration File.\n\n  The currently supported settings, are, by section:\n\
    \n    [Credentials]\n      aws_access_key_id\n      aws_secret_access_key\n  \
    \    gs_access_key_id\n      gs_host\n      gs_json_host\n      gs_json_port\n\
    \      gs_oauth2_refresh_token\n      gs_port\n      gs_secret_access_key\n  \
    \    s3_host\n      s3_port\n\n    [Boto]\n      proxy\n      proxy_port\n   \
    \   proxy_user\n      proxy_pass\n      proxy_rdns\n      http_socket_timeout\n\
    \      https_validate_certificates\n      debug\n      max_retry_delay\n     \
    \ num_retries\n\n    [GoogleCompute]\n      service_account\n\n    [GSUtil]\n\
    \      check_hashes\n      content_language\n      decryption_key1 ... 100\n \
    \     default_api_version\n      default_project_id\n      disable_analytics_prompt\n\
    \      encryption_key\n      json_api_version\n      parallel_composite_upload_component_size\n\
    \      parallel_composite_upload_threshold\n      sliced_object_download_component_size\n\
    \      sliced_object_download_max_components\n      sliced_object_download_threshold\n\
    \      parallel_process_count\n      parallel_thread_count\n      prefer_api\n\
    \      resumable_threshold\n      resumable_tracker_dir (deprecated in 4.6, use\
    \ state_dir)\n      rsync_buffer_lines\n      software_update_check_period\n \
    \     state_dir\n      tab_completion_time_logs\n      tab_completion_timeout\n\
    \      task_estimation_threshold\n      use_magicfile\n\n    [OAuth2]\n      client_id\n\
    \      client_secret\n      oauth2_refresh_retries\n      provider_authorization_uri\n\
    \      provider_label\n      provider_token_uri\n      token_cache\n\n\nUPDATING\
    \ TO THE LATEST CONFIGURATION FILE\n  We add new configuration controllable features\
    \ to the boto configuration file\n  over time, but most gsutil users create a\
    \ configuration file once and then\n  keep it for a long time, so new features\
    \ aren't apparent when you update\n  to a newer version of gsutil. If you want\
    \ to get the latest configuration\n  file (which includes all the latest settings\
    \ and documentation about each)\n  you can rename your current file (e.g., to\
    \ '.boto_old'), run gsutil config,\n  and then edit any configuration settings\
    \ you wanted from your old file\n  into the newly created file. Note, however,\
    \ that if you're using OAuth2\n  credentials and you go back through the OAuth2\
    \ configuration dialog it will\n  invalidate your previous OAuth2 credentials.\n\
    \n  If no explicit scope option is given, -f (full control) is assumed by default.\n\
    \n\nOPTIONS\n  -a          Prompt for Google Cloud Storage access key and secret\
    \ (the older\n              authentication method before OAuth2 was supported)\
    \ instead of\n              obtaining an OAuth2 token.\n\n  -b          Causes\
    \ gsutil config to launch a browser to obtain OAuth2 approval\n              and\
    \ the project ID instead of showing the URL for each and asking\n            \
    \  the user to open the browser. This will probably not work as\n            \
    \  expected if you are running gsutil from an ssh window, or using\n         \
    \     gsutil on Windows.\n\n  -e          Prompt for service account credentials.\
    \ This option requires that\n              -a is not set.\n\n  -f          Request\
    \ token with full-control access (default).\n\n  -n          Write the configuration\
    \ file without authentication configured.\n              This flag is mutually\
    \ exlusive with all flags other than -o.\n\n  -o <file>   Write the configuration\
    \ to <file> instead of ~/.boto.\n              Use '-' for stdout.\n\n  -r   \
    \       Request token restricted to read-only access.\n\n  -s <scope>  Request\
    \ additional OAuth2 <scope>.\n\n  -w          Request token restricted to read-write\
    \ access.\n"
  generated_using: *id002
usage: []
help_flag:
usage_flag:
version_flag:
help_text: "Usage: gsutil [-D] [-DD] [-h header]... [-m] [-o] [-q] [command [opts...]\
  \ args...]\nAvailable commands:\n  acl             Get, set, or change bucket and/or\
  \ object ACLs\n  cat             Concatenate object content to stdout\n  compose\
  \         Concatenate a sequence of objects into a new composite object.\n  config\
  \          Obtain credentials and create configuration file\n  cors            Get\
  \ or set a CORS JSON document for one or more buckets\n  cp              Copy files\
  \ and objects\n  defacl          Get, set, or change default ACL on buckets\n  defstorageclass\
  \ Get or set the default storage class on buckets\n  du              Display object\
  \ size usage\n  hash            Calculate file hashes\n  help            Get help\
  \ about commands and topics\n  iam             Get, set, or change bucket and/or\
  \ object IAM permissions.\n  label           Get, set, or change the label configuration\
  \ of a bucket.\n  lifecycle       Get or set lifecycle configuration for a bucket\n\
  \  logging         Configure or retrieve logging on buckets\n  ls              List\
  \ providers, buckets, or objects\n  mb              Make buckets\n  mv         \
  \     Move/rename objects and/or subdirectories\n  notification    Configure object\
  \ change notification\n  perfdiag        Run performance diagnostic\n  rb      \
  \        Remove buckets\n  rewrite         Rewrite objects\n  rm              Remove\
  \ objects\n  rsync           Synchronize content of two buckets/directories\n  setmeta\
  \         Set metadata on already uploaded objects\n  signurl         Create a signed\
  \ url\n  stat            Display object status\n  test            Run gsutil unit/integration\
  \ tests (for developers)\n  update          Update to the latest gsutil release\n\
  \  version         Print version info about gsutil\n  versioning      Enable or\
  \ suspend versioning for one or more buckets\n  web             Set a main page\
  \ and/or error page for one or more buckets\n\nAdditional help topics:\n  acls \
  \           Working With Access Control Lists\n  anon            Accessing Public\
  \ Data Without Credentials\n  apis            Cloud Storage APIs\n  crc32c     \
  \     CRC32C and Installing crcmod\n  creds           Credential Types Supporting\
  \ Various Use Cases\n  csek            Supplying Your Own Encryption Keys\n  dev\
  \             Contributing Code to gsutil\n  encoding        Filename encoding and\
  \ interoperability problems\n  metadata        Working With Object Metadata\n  naming\
  \          Object and Bucket Naming\n  options         Top-Level Command-Line Options\n\
  \  prod            Scripting Production Transfers\n  projects        Working With\
  \ Projects\n  retries         Retry Handling Strategy\n  security        Security\
  \ and Privacy Considerations\n  subdirs         How Subdirectories Work\n  support\
  \         Google Cloud Storage Support\n  throttling      Throttling gsutil\n  versions\
  \        Object Versioning and Concurrency Control\n  wildcards       Wildcard Names\n\
  \nUse gsutil help <command or topic> for detailed help.\n"
generated_using: *id002
