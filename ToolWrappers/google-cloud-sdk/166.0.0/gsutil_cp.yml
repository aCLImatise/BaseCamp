!Command
command:
- gsutil
- cp
positional:
- !Positional
  optional: false
  position: 0
  name: metadata.
  description: Copies spanning locations and/or storage classes cause data to be rewritten
- !Positional
  optional: false
  position: 0
  name: identical.
  description: Note that by default, the gsutil cp command does not copy the object
- !Positional
  optional: false
  position: 0
  name: Hashing
  description: 'obj:'
- !Positional
  optional: false
  position: 0
  name: Uploading
  description: 'gs://your-bucket/obj:                                182 b/182 B'
- !Positional
  optional: false
  position: 0
  name: operation.
  description: This feature is only available for Google Cloud Storage objects because
    it
- !Positional
  optional: false
  position: 0
  name: copied.
  description: -I             Causes gsutil to read the list of files or objects to
    copy from
named:
- !Flag
  optional: true
  synonyms:
  - -a
  description: "Sets named canned_acl when uploaded objects created. See\n\"gsutil\
    \ help acls\" for further details."
  args: !SimpleFlagArg
    name: canned_acl
- !Flag
  optional: true
  synonyms:
  - -A
  description: "Copy all source versions from a source buckets/folders.\nIf not set,\
    \ only the live version of each source object is\ncopied. Note: this option is\
    \ only useful when the destination\nbucket has versioning enabled."
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - -c
  description: "If an error occurs, continue to attempt to copy the remaining\nfiles.\
    \ If any copies were unsuccessful, gsutil's exit status\nwill be non-zero even\
    \ if this flag is set. This option is\nimplicitly set when running \"gsutil -m\
    \ cp...\". Note: -c only\napplies to the actual copying operation. If an error\
    \ occurs\nwhile iterating over the files in the local directory (e.g.,\ninvalid\
    \ Unicode file name) gsutil will print an error message\nand abort."
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - -D
  description: "Copy in \"daisy chain\" mode, i.e., copying between two buckets\n\
    by hooking a download to an upload, via the machine where\ngsutil is run. This\
    \ stands in contrast to the default, where\ndata are copied between two buckets\
    \ \"in the cloud\", i.e.,\nwithout needing to copy via the machine where gsutil\
    \ runs.\nBy default, a \"copy in the cloud\" when the source is a\ncomposite object\
    \ will retain the composite nature of the\nobject. However, Daisy chain mode can\
    \ be used to change a\ncomposite object into a non-composite object. For example:\n\
    gsutil cp -D -p gs://bucket/obj gs://bucket/obj_tmp\ngsutil mv -p gs://bucket/obj_tmp\
    \ gs://bucket/obj\nNote: Daisy chain mode is automatically used when copying\n\
    between providers (e.g., to copy data from Google Cloud Storage\nto another provider)."
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - -e
  description: Exclude symlinks. When specified, symbolic links will not be
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - -L
  description: "Outputs a manifest log file with detailed information about\neach\
    \ item that was copied. This manifest contains the following\ninformation for\
    \ each item:\n- Source path.\n- Destination path.\n- Source size.\n- Bytes transferred.\n\
    - MD5 hash.\n- UTC date and time transfer was started in ISO 8601 format.\n- UTC\
    \ date and time transfer was completed in ISO 8601 format.\n- Upload id, if a\
    \ resumable upload was performed.\n- Final result of the attempted transfer, success\
    \ or failure.\n- Failure details, if any.\nIf the log file already exists, gsutil\
    \ will use the file as an\ninput to the copy process, and will also append log\
    \ items to\nthe existing file. Files/objects that are marked in the\nexisting\
    \ log file as having been successfully copied (or\nskipped) will be ignored. Files/objects\
    \ without entries will be\ncopied and ones previously marked as unsuccessful will\
    \ be\nretried. This can be used in conjunction with the -c option to\nbuild a\
    \ script that copies a large number of objects reliably,\nusing a bash script\
    \ like the following:\nuntil gsutil cp -c -L cp.log -r ./dir gs://bucket; do\n\
    sleep 1\ndone\nThe -c option will cause copying to continue after failures\noccur,\
    \ and the -L option will allow gsutil to pick up where it\nleft off without duplicating\
    \ work. The loop will continue\nrunning as long as gsutil exits with a non-zero\
    \ status (such a\nstatus indicates there was at least one failure during the\n\
    gsutil run).\nNote: If you're trying to synchronize the contents of a\ndirectory\
    \ and a bucket (or two buckets), see\n\"gsutil help rsync\"."
  args: !SimpleFlagArg
    name: file
- !Flag
  optional: true
  synonyms:
  - -n
  description: "No-clobber. When specified, existing files or objects at the\ndestination\
    \ will not be overwritten. Any items that are skipped\nby this option will be\
    \ reported as being skipped. This option\nwill perform an additional GET request\
    \ to check if an item\nexists before attempting to upload the data. This will\
    \ save\nretransmitting data, but the additional HTTP requests may make\nsmall\
    \ object transfers slower and more expensive."
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - -p
  description: "Causes ACLs to be preserved when copying in the cloud. Note\nthat\
    \ this option has performance and cost implications when\nusing  the XML API,\
    \ as it requires separate HTTP calls for\ninteracting with ACLs. (There are no\
    \ such performance or cost\nimplications when using the -p option with the JSON\
    \ API.) The\nperformance issue can be mitigated to some degree by using\ngsutil\
    \ -m cp to cause parallel copying. Note that this option\nonly works if you have\
    \ OWNER access to all of the objects that\nare copied.\nYou can avoid the additional\
    \ performance and cost of using\ncp -p if you want all objects in the destination\
    \ bucket to end\nup with the same ACL by setting a default object ACL on that\n\
    bucket instead of using cp -p. See \"gsutil help defacl\".\nNote that it's not\
    \ valid to specify both the -a and -p options\ntogether."
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - -P
  description: "Causes POSIX attributes to be preserved when objects are\ncopied.\
    \ With this feature enabled, gsutil cp will copy fields\nprovided by stat. These\
    \ are the user ID of the owner, the group\nID of the owning group, the mode (permissions)\
    \ of the file, and\nthe access/modification time of the file. For downloads, these\n\
    attributes will only be set if the source objects were uploaded\nwith this flag\
    \ enabled.\nOn Windows, this flag will only set and restore access time and\n\
    modification time. This is because Windows doesn't have a\nnotion of POSIX uid/gid/mode."
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - -R
  - -r
  description: "The -R and -r options are synonymous. Causes directories,\nbuckets,\
    \ and bucket subdirectories to be copied recursively.\nIf you neglect to use this\
    \ option for an upload, gsutil will\ncopy any files it finds and skip any directories.\
    \ Similarly,\nneglecting to specify this option for a download will cause\ngsutil\
    \ to copy any objects at the current bucket directory\nlevel, and skip any subdirectories."
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - -s
  description: "The storage class of the destination object(s). If not\nspecified,\
    \ the default storage class of the destination bucket\nis used. Not valid for\
    \ copying to non-cloud destinations."
  args: !SimpleFlagArg
    name: class
- !Flag
  optional: true
  synonyms:
  - -U
  description: "Skip objects with unsupported object types instead of failing.\nUnsupported\
    \ object types are Amazon S3 Objects in the GLACIER\nstorage class."
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - -v
  description: "Requests that the version-specific URL for each uploaded object\n\
    be printed. Given this URL you can make future upload requests\nthat are safe\
    \ in the face of concurrent updates, because Google\nCloud Storage will refuse\
    \ to perform the update if the current\nobject version doesn't match the version-specific\
    \ URL. See\n\"gsutil help versions\" for more details."
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - -z
  description: "<ext,...>   Applies gzip content-encoding to any file upload whose\n\
    extension matches the -z extension list. This is useful when\nuploading files\
    \ with compressible content (such as .js, .css,\nor .html files) because it saves\
    \ network bandwidth and space\nin Google Cloud Storage, which in turn reduces\
    \ storage costs.\nWhen you specify the -z option, the data from your files is\n\
    compressed before it is uploaded, but your actual files are\nleft uncompressed\
    \ on the local disk. The uploaded objects\nretain the Content-Type and name of\
    \ the original files but are\ngiven a Content-Encoding header with the value \"\
    gzip\" to\nindicate that the object data stored are compressed on the\nGoogle\
    \ Cloud Storage servers.\nFor example, the following command:\ngsutil cp -z html\
    \ -a public-read \\\ncattypes.html tabby.jpeg gs://mycats\nwill do all of the\
    \ following:\n- Upload the files cattypes.html and tabby.jpeg to the bucket\n\
    gs://mycats (cp command)\n- Set the Content-Type of cattypes.html to text/html\
    \ and\ntabby.jpeg to image/jpeg (based on file extensions)\n- Compress the data\
    \ in the file cattypes.html (-z option)\n- Set the Content-Encoding for cattypes.html\
    \ to gzip\n(-z option)\n- Set the ACL for both files to public-read (-a option)\n\
    - If a user tries to view cattypes.html in a browser, the\nbrowser will know to\
    \ uncompress the data based on the\nContent-Encoding header and to render it as\
    \ HTML based on\nthe Content-Type header.\nNote that if you download an object\
    \ with Content-Encoding:gzip\ngsutil will decompress the content before writing\
    \ the local\nfile."
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - -Z
  description: "Applies gzip content-encoding to file uploads. This option\nworks\
    \ like the -z option described above, but it applies to\nall uploaded files, regardless\
    \ of extension.\nWarning: If you use this option and some of the source files\n\
    don't compress well (e.g., that's often true of binary data),\nthis option may\
    \ result in files taking up more space in the\ncloud than they would if left uncompressed.\n"
  args: !EmptyFlagArg {}
parent:
subcommands: []
usage: []
help_flag:
usage_flag:
version_flag:
help_text: "NAME\n  cp - Copy files and objects\n\n\nSYNOPSIS\n\n  gsutil cp [OPTION]...\
  \ src_url dst_url\n  gsutil cp [OPTION]... src_url... dst_url\n  gsutil cp [OPTION]...\
  \ -I dst_url\n\n\n\nDESCRIPTION\n  The gsutil cp command allows you to copy data\
  \ between your local file\n  system and the cloud, copy data within the cloud, and\
  \ copy data between\n  cloud storage providers. For example, to copy all text files\
  \ from the\n  local directory to a bucket you could do:\n\n    gsutil cp *.txt gs://my-bucket\n\
  \n  Similarly, you can download text files from a bucket by doing:\n\n    gsutil\
  \ cp gs://my-bucket/*.txt .\n\n  If you want to copy an entire directory tree you\
  \ need to use the -r option:\n\n    gsutil cp -r dir gs://my-bucket\n\n  If you\
  \ have a large number of files to transfer you might want to use the\n  gsutil -m\
  \ option, to perform a parallel (multi-threaded/multi-processing)\n  copy:\n\n \
  \   gsutil -m cp -r dir gs://my-bucket\n\n  You can pass a list of URLs (one per\
  \ line) to copy on stdin instead of as\n  command line arguments by using the -I\
  \ option. This allows you to use gsutil\n  in a pipeline to upload or download files\
  \ / objects as generated by a program,\n  such as:\n\n    some_program | gsutil\
  \ -m cp -I gs://my-bucket\n\n  or:\n\n    some_program | gsutil -m cp -I ./download_dir\n\
  \n  The contents of stdin can name files, cloud URLs, and wildcards of files\n \
  \ and cloud URLs.\n\n\n\nHOW NAMES ARE CONSTRUCTED\n  The gsutil cp command strives\
  \ to name objects in a way consistent with how\n  Linux cp works, which causes names\
  \ to be constructed in varying ways depending\n  on whether you're performing a\
  \ recursive directory copy or copying\n  individually named objects; and whether\
  \ you're copying to an existing or\n  non-existent directory.\n\n  When performing\
  \ recursive directory copies, object names are constructed that\n  mirror the source\
  \ directory structure starting at the point of recursive\n  processing. For example,\
  \ if dir1/dir2 contains the file a/b/c then the\n  command:\n\n    gsutil cp -r\
  \ dir1/dir2 gs://my-bucket\n\n  will create the object gs://my-bucket/dir2/a/b/c.\n\
  \n  In contrast, copying individually named files will result in objects named by\n\
  \  the final path component of the source files. For example, again assuming\n \
  \ dir1/dir2 contains a/b/c, the command:\n\n    gsutil cp dir1/dir2/** gs://my-bucket\n\
  \n  will create the object gs://my-bucket/c.\n\n  The same rules apply for downloads:\
  \ recursive copies of buckets and\n  bucket subdirectories produce a mirrored filename\
  \ structure, while copying\n  individually (or wildcard) named objects produce flatly\
  \ named files.\n\n  Note that in the above example the '**' wildcard matches all\
  \ names\n  anywhere under dir. The wildcard '*' will match names just one level\
  \ deep. For\n  more details see \"gsutil help wildcards\".\n\n  There's an additional\
  \ wrinkle when working with subdirectories: the resulting\n  names depend on whether\
  \ the destination subdirectory exists. For example,\n  if gs://my-bucket/subdir\
  \ exists as a subdirectory, the command:\n\n    gsutil cp -r dir1/dir2 gs://my-bucket/subdir\n\
  \n  will create the object gs://my-bucket/subdir/dir2/a/b/c. In contrast, if\n \
  \ gs://my-bucket/subdir does not exist, this same gsutil cp command will create\n\
  \  the object gs://my-bucket/subdir/a/b/c.\n\n  Note: If you use the\n  `Google\
  \ Cloud Platform Console <https://console.cloud.google.com>`_\n  to create folders,\
  \ it does so by creating a \"placeholder\" object that ends\n  with a \"/\" character.\
  \ gsutil skips these objects when downloading from the\n  cloud to the local file\
  \ system, because attempting to create a file that\n  ends with a \"/\" is not allowed\
  \ on Linux and MacOS. Because of this, it is\n  recommended that you not create\
  \ objects that end with \"/\" (unless you don't\n  need to be able to download such\
  \ objects using gsutil).\n\n\n\nCOPYING TO/FROM SUBDIRECTORIES; DISTRIBUTING TRANSFERS\
  \ ACROSS MACHINES\n  You can use gsutil to copy to and from subdirectories by using\
  \ a command\n  like:\n\n    gsutil cp -r dir gs://my-bucket/data\n\n  This will\
  \ cause dir and all of its files and nested subdirectories to be\n  copied under\
  \ the specified destination, resulting in objects with names like\n  gs://my-bucket/data/dir/a/b/c.\
  \ Similarly you can download from bucket\n  subdirectories by using a command like:\n\
  \n    gsutil cp -r gs://my-bucket/data dir\n\n  This will cause everything nested\
  \ under gs://my-bucket/data to be downloaded\n  into dir, resulting in files with\
  \ names like dir/data/a/b/c.\n\n  Copying subdirectories is useful if you want to\
  \ add data to an existing\n  bucket directory structure over time. It's also useful\
  \ if you want\n  to parallelize uploads and downloads across multiple machines (potentially\n\
  \  reducing overall transfer time compared with simply running gsutil -m\n  cp on\
  \ one machine). For example, if your bucket contains this structure:\n\n    gs://my-bucket/data/result_set_01/\n\
  \    gs://my-bucket/data/result_set_02/\n    ...\n    gs://my-bucket/data/result_set_99/\n\
  \n  you could perform concurrent downloads across 3 machines by running these\n\
  \  commands on each machine, respectively:\n\n    gsutil -m cp -r gs://my-bucket/data/result_set_[0-3]*\
  \ dir\n    gsutil -m cp -r gs://my-bucket/data/result_set_[4-6]* dir\n    gsutil\
  \ -m cp -r gs://my-bucket/data/result_set_[7-9]* dir\n\n  Note that dir could be\
  \ a local directory on each machine, or it could be a\n  directory mounted off of\
  \ a shared file server; whether the latter performs\n  acceptably will depend on\
  \ a number of factors, so we recommend experimenting\n  to find out what works best\
  \ for your computing environment.\n\n\n\nCOPYING IN THE CLOUD AND METADATA PRESERVATION\n\
  \  If both the source and destination URL are cloud URLs from the same\n  provider,\
  \ gsutil copies data \"in the cloud\" (i.e., without downloading\n  to and uploading\
  \ from the machine where you run gsutil). In addition to\n  the performance and\
  \ cost advantages of doing this, copying in the cloud\n  preserves metadata (like\
  \ Content-Type and Cache-Control). In contrast,\n  when you download data from the\
  \ cloud it ends up in a file, which has\n  no associated metadata. Thus, unless\
  \ you have some way to hold on to\n  or re-create that metadata, downloading to\
  \ a file will not retain the\n  metadata.\n\n  Copies spanning locations and/or\
  \ storage classes cause data to be rewritten\n  in the cloud, which may take some\
  \ time (but still will be faster than\n  downloading and re-uploading). Such operations\
  \ can be resumed with the same\n  command if they are interrupted, so long as the\
  \ command parameters are\n  identical.\n\n  Note that by default, the gsutil cp\
  \ command does not copy the object\n  ACL to the new object, and instead will use\
  \ the default bucket ACL (see\n  \"gsutil help defacl\"). You can override this\
  \ behavior with the -p\n  option (see OPTIONS below).\n\n  One additional note about\
  \ copying in the cloud: If the destination bucket has\n  versioning enabled, by\
  \ default gsutil cp will copy only live versions of the\n  source object(s). For\
  \ example:\n\n    gsutil cp gs://bucket1/obj gs://bucket2\n\n  will cause only the\
  \ single live version of gs://bucket1/obj to be copied to\n  gs://bucket2, even\
  \ if there are archived versions of gs://bucket1/obj. To also\n  copy archived versions,\
  \ use the -A flag:\n\n    gsutil cp -A gs://bucket1/obj gs://bucket2\n\n  The gsutil\
  \ -m flag is disallowed when using the cp -A flag, to ensure that\n  version ordering\
  \ is preserved.\n\n\n\nCHECKSUM VALIDATION\n  At the end of every upload or download\
  \ the gsutil cp command validates that\n  the checksum it computes for the source\
  \ file/object matches the checksum\n  the service computes. If the checksums do\
  \ not match, gsutil will delete the\n  corrupted object and print a warning message.\
  \ This very rarely happens, but\n  if it does, please contact gs-team@google.com.\n\
  \n  If you know the MD5 of a file before uploading you can specify it in the\n \
  \ Content-MD5 header, which will cause the cloud storage service to reject the\n\
  \  upload if the MD5 doesn't match the value computed by the service. For\n  example:\n\
  \n    % gsutil hash obj\n    Hashing     obj:\n    Hashes [base64] for obj:\n  \
  \          Hash (crc32c):          lIMoIw==\n            Hash (md5):           \
  \  VgyllJgiiaRAbyUUIqDMmw==\n\n    % gsutil -h Content-MD5:VgyllJgiiaRAbyUUIqDMmw==\
  \ cp obj gs://your-bucket/obj\n    Copying file://obj [Content-Type=text/plain]...\n\
  \    Uploading   gs://your-bucket/obj:                                182 b/182\
  \ B\n\n    If the checksum didn't match the service would instead reject the upload\
  \ and\n    gsutil would print a message like:\n\n    BadRequestException: 400 Provided\
  \ MD5 hash \"VgyllJgiiaRAbyUUIqDMmw==\"\n    doesn't match calculated MD5 hash \"\
  7gyllJgiiaRAbyUUIqDMmw==\".\n\n  Even if you don't do this gsutil will delete the\
  \ object if the computed\n  checksum mismatches, but specifying the Content-MD5\
  \ header has several\n  advantages:\n\n      1. It prevents the corrupted object\
  \ from becoming visible at all, whereas\n      otherwise it would be visible for\
  \ 1-3 seconds before gsutil deletes it.\n\n      2. If an object already exists\
  \ with the given name, specifying the\n      Content-MD5 header will cause the existing\
  \ object never to be replaced,\n      whereas otherwise it would be replaced by\
  \ the corrupted object and then\n      deleted a few seconds later.\n\n      3.\
  \ It will definitively prevent the corrupted object from being left in\n      the\
  \ cloud, whereas the gsutil approach of deleting after the upload\n      completes\
  \ could fail if (for example) the gsutil process gets ^C'd\n      between upload\
  \ and deletion request.\n\n      4. It supports a customer-to-service integrity\
  \ check handoff. For example,\n      if you have a content production pipeline that\
  \ generates data to be\n      uploaded to the cloud along with checksums of that\
  \ data, specifying the\n      MD5 computed by your content pipeline when you run\
  \ gsutil cp will ensure\n      that the checksums match all the way through the\
  \ process (e.g., detecting\n      if data gets corrupted on your local disk between\
  \ the time it was written\n      by your content pipeline and the time it was uploaded\
  \ to GCS).\n\n  Note: The Content-MD5 header is ignored for composite objects, because\
  \ such\n  objects only have a CRC32C checksum.\n\n\n\nRETRY HANDLING\n  The cp command\
  \ will retry when failures occur, but if enough failures happen\n  during a particular\
  \ copy or delete operation the cp command will skip that\n  object and move on.\
  \ At the end of the copy run if any failures were not\n  successfully retried, the\
  \ cp command will report the count of failures, and\n  exit with non-zero status.\n\
  \n  Note that there are cases where retrying will never succeed, such as if you\n\
  \  don't have write permission to the destination bucket or if the destination\n\
  \  path for some objects is longer than the maximum allowed length.\n\n  For more\
  \ details about gsutil's retry handling, please see\n  \"gsutil help retries\".\n\
  \n\n\nRESUMABLE TRANSFERS\n  gsutil automatically performs a resumable upload whenever\
  \ you use the cp\n  command to upload an object that is larger than 8 MiB. You do\
  \ not need to\n  specify any special command line options to make this happen. If\
  \ your upload\n  is interrupted you can restart the upload by running the same cp\
  \ command that\n  you ran to start the upload. Until the upload has completed successfully,\
  \ it\n  will not be visible at the destination object and will not replace any\n\
  \  existing object the upload is intended to overwrite. However, see the section\n\
  \  on PARALLEL COMPOSITE UPLOADS, which may leave temporary component objects in\n\
  \  place during the upload process.\n\n  Similarly, gsutil automatically performs\
  \ resumable downloads (using standard\n  HTTP Range GET operations) whenever you\
  \ use the cp command, unless the\n  destination is a stream. In this case, a partially\
  \ downloaded temporary file\n  will be visible in the destination directory. Upon\
  \ completion, the original\n  file is deleted and overwritten with the downloaded\
  \ contents.\n\n  Resumable uploads and downloads store state information in files\
  \ under\n  ~/.gsutil, named by the destination object or file. If you attempt to\
  \ resume a\n  transfer from a machine with a different directory, the transfer will\
  \ start\n  over from scratch.\n\n  See also \"gsutil help prod\" for details on\
  \ using resumable transfers\n  in production.\n\n\n\nSTREAMING TRANSFERS\n  Use\
  \ '-' in place of src_url or dst_url to perform a streaming\n  transfer. For example:\n\
  \n    long_running_computation | gsutil cp - gs://my-bucket/obj\n\n  Streaming uploads\
  \ using the JSON API (see \"gsutil help apis\") are buffered in\n  memory part-way\
  \ back into the file and can thus retry in the event of network\n  or service problems.\n\
  \n  Streaming transfers using the XML API do not support resumable\n  uploads/downloads.\
  \ If you have a large amount of data to upload (say, more\n  than 100 MiB) it is\
  \ recommended that you write the data to a local file and\n  then copy that file\
  \ to the cloud rather than streaming it (and similarly for\n  large downloads).\n\
  \n  WARNING: When performing streaming transfers gsutil does not compute a\n  checksum\
  \ of the uploaded or downloaded data. Therefore, we recommend that\n  users either\
  \ perform their own validation of the data or use non-streaming\n  transfers (which\
  \ perform integrity checking automatically).\n\n\n\nSLICED OBJECT DOWNLOADS\n  gsutil\
  \ uses HTTP Range GET requests to perform \"sliced\" downloads in parallel\n  when\
  \ downloading large objects from Google Cloud Storage. This means that disk\n  space\
  \ for the temporary download destination file will be pre-allocated and\n  byte\
  \ ranges (slices) within the file will be downloaded in parallel. Once all\n  slices\
  \ have completed downloading, the temporary file will be renamed to the\n  destination\
  \ file. No additional local disk space is required for this\n  operation.\n\n  This\
  \ feature is only available for Google Cloud Storage objects because it\n  requires\
  \ a fast composable checksum (CRC32C) that can be used to verify the\n  data integrity\
  \ of the slices. And because it depends on CRC32C, using sliced\n  object downloads\
  \ also requires a compiled crcmod (see \"gsutil help crcmod\") on\n  the machine\
  \ performing the download. If compiled crcmod is not available,\n  a non-sliced\
  \ object download will instead be performed.\n\n  Note: since sliced object downloads\
  \ cause multiple writes to occur at various\n  locations on disk, this mechanism\
  \ can degrade performance for disks with slow\n  seek times, especially for large\
  \ numbers of slices. While the default number\n  of slices is set small to avoid\
  \ this problem, you can disable sliced object\n  download if necessary by setting\
  \ the \"sliced_object_download_threshold\"\n  variable in the .boto config file\
  \ to 0.\n\n\n\n\n\nPARALLEL COMPOSITE UPLOADS\n  gsutil can automatically use\n\
  \  `object composition <https://cloud.google.com/storage/docs/composite-objects>`_\n\
  \  to perform uploads in parallel for large, local files being uploaded to Google\n\
  \  Cloud Storage. If enabled (see below), a large file will be split into\n  component\
  \ pieces that are uploaded in parallel and then composed in the cloud\n  (and the\
  \ temporary components finally deleted). A file can be broken into as\n  many as\
  \ 32 component pieces; until this piece limit is reached, the maximum\n  size of\
  \ each component piece is determined by the variable\n  \"parallel_composite_upload_component_size,\"\
  \ specified in the [GSUtil] section\n  of your .boto configuration file (for files\
  \ that are otherwise too big,\n  components are as large as needed to fit into 32\
  \ pieces). No additional local\n  disk space is required for this operation.\n\n\
  \  Using parallel composite uploads presents a tradeoff between upload\n  performance\
  \ and download configuration: If you enable parallel composite\n  uploads your uploads\
  \ will run faster, but someone will need to install a\n  compiled crcmod (see \"\
  gsutil help crcmod\") on every machine where objects are\n  downloaded by gsutil\
  \ or other Python applications. Note that for such uploads,\n  crcmod is required\
  \ for downloading regardless of whether the parallel\n  composite upload option\
  \ is on or not. For some distributions this is easy\n  (e.g., it comes pre-installed\
  \ on MacOS), but in other cases some users have\n  found it difficult. Because of\
  \ this, at present parallel composite uploads are\n  disabled by default. Google\
  \ is actively working with a number of the Linux\n  distributions to get crcmod\
  \ included with the stock distribution. Once that is\n  done we will re-enable parallel\
  \ composite uploads by default in gsutil.\n\n  Warning: Parallel composite uploads\
  \ should not be used with NEARLINE or\n  COLDLINE storage class buckets, because\
  \ doing so incurs an early deletion\n  charge for each component object.\n\n  To\
  \ try parallel composite uploads you can run the command:\n\n    gsutil -o GSUtil:parallel_composite_upload_threshold=150M\
  \ cp bigfile gs://your-bucket\n\n  where bigfile is larger than 150 MiB. When you\
  \ do this notice that the upload\n  progress indicator continuously updates for\
  \ several different uploads at once\n  (corresponding to each of the sections of\
  \ the file being uploaded in\n  parallel), until the parallel upload completes.\
  \ If after trying this you want\n  to enable parallel composite uploads for all\
  \ of your future uploads\n  (notwithstanding the caveats mentioned earlier), you\
  \ can uncomment and set the\n  \"parallel_composite_upload_threshold\" config value\
  \ in your .boto configuration\n  file to this value.\n\n  Note that the crcmod problem\
  \ only impacts downloads via Python applications\n  (such as gsutil). If all users\
  \ who need to download the data using gsutil or\n  other Python applications can\
  \ install crcmod, or if no Python users will\n  need to download your objects, it\
  \ makes sense to enable parallel composite\n  uploads (see above). For example,\
  \ if you use gsutil to upload video assets,\n  and those assets will only ever be\
  \ served via a Java application, it would\n  make sense to enable parallel composite\
  \ uploads on your machine (there are\n  efficient CRC32C implementations available\
  \ in Java).\n\n  If a parallel composite upload fails prior to composition, re-running\
  \ the\n  gsutil command will take advantage of resumable uploads for the components\n\
  \  that failed, and the component objects will be deleted after the first\n  successful\
  \ attempt. Any temporary objects that were uploaded successfully\n  before gsutil\
  \ failed will still exist until the upload is completed\n  successfully. The temporary\
  \ objects will be named in the following fashion:\n\n    <random ID>/gsutil/tmp/parallel_composite_uploads/for_details_see/gsutil_help_cp/<hash>\n\
  \n  where <random ID> is a numerical value, and <hash> is an MD5 hash (not related\n\
  \  to the hash of the contents of the file or object).\n\n  To avoid leaving temporary\
  \ objects around, you should make sure to check the\n  exit status from the gsutil\
  \ command.  This can be done in a bash script, for\n  example, by doing:\n\n   \
  \ if ! gsutil cp ./local-file gs://your-bucket/your-object; then\n      << Code\
  \ that handles failures >>\n    fi\n\n  Or, for copying a directory, use this instead:\n\
  \n    if ! gsutil cp -c -L cp.log -r ./dir gs://bucket; then\n      << Code that\
  \ handles failures >>\n    fi\n\n  One important caveat is that files uploaded using\
  \ parallel composite uploads\n  are subject to a maximum number of components limit.\
  \ For example, if you\n  upload a large file that gets split into 10 components,\
  \ and try to compose it\n  with another object with 1015 components, the operation\
  \ will fail because it\n  exceeds the 1024 component limit. If you wish to compose\
  \ an object later and the\n  component limit is a concern, it is recommended that\
  \ you disable parallel\n  composite uploads for that transfer.\n\n  Also note that\
  \ an object uploaded using parallel composite uploads will have a\n  CRC32C hash,\
  \ but it will not have an MD5 hash (and because of that, users who\n  download the\
  \ object must have crcmod installed, as noted earlier). For details\n  see \"gsutil\
  \ help crc32c\".\n\n  Parallel composite uploads can be disabled by setting the\n\
  \  \"parallel_composite_upload_threshold\" variable in the .boto config file to\
  \ 0.\n\n\n\nCHANGING TEMP DIRECTORIES\n  gsutil writes data to a temporary directory\
  \ in several cases:\n\n  - when compressing data to be uploaded (see the -z and\
  \ -Z options)\n  - when decompressing data being downloaded (when the data has\n\
  \    Content-Encoding:gzip, e.g., as happens when uploaded using gsutil cp -z\n\
  \    or gsutil cp -Z)\n  - when running integration tests (using the gsutil test\
  \ command)\n\n  In these cases it's possible the temp file location on your system\
  \ that\n  gsutil selects by default may not have enough space. If gsutil runs out\
  \ of\n  space during one of these operations (e.g., raising\n  \"CommandException:\
  \ Inadequate temp space available to compress <your file>\"\n  during a gsutil cp\
  \ -z operation), you can change where it writes these\n  temp files by setting the\
  \ TMPDIR environment variable. On Linux and MacOS\n  you can do this either by running\
  \ gsutil this way:\n\n    TMPDIR=/some/directory gsutil cp ...\n\n  or by adding\
  \ this line to your ~/.bashrc file and then restarting the shell\n  before running\
  \ gsutil:\n\n    export TMPDIR=/some/directory\n\n  On Windows 7 you can change\
  \ the TMPDIR environment variable from Start ->\n  Computer -> System -> Advanced\
  \ System Settings -> Environment Variables.\n  You need to reboot after making this\
  \ change for it to take effect. (Rebooting\n  is not necessary after running the\
  \ export command on Linux and MacOS.)\n\n\n\nCOPYING SPECIAL FILES\n  gsutil cp\
  \ does not support copying special file types such as sockets, device\n  files,\
  \ named pipes, or any other non-standard files intended to represent an\n  operating\
  \ system resource. You should not run gsutil cp with sources that\n  include such\
  \ files (for example, recursively copying the root directory on\n  Linux that includes\
  \ /dev ). If you do, gsutil cp may fail or hang.\n\n\n\nOPTIONS\n  -a canned_acl\
  \  Sets named canned_acl when uploaded objects created. See\n                 \"\
  gsutil help acls\" for further details.\n\n  -A             Copy all source versions\
  \ from a source buckets/folders.\n                 If not set, only the live version\
  \ of each source object is\n                 copied. Note: this option is only useful\
  \ when the destination\n                 bucket has versioning enabled.\n\n  -c\
  \             If an error occurs, continue to attempt to copy the remaining\n  \
  \               files. If any copies were unsuccessful, gsutil's exit status\n \
  \                will be non-zero even if this flag is set. This option is\n   \
  \              implicitly set when running \"gsutil -m cp...\". Note: -c only\n\
  \                 applies to the actual copying operation. If an error occurs\n\
  \                 while iterating over the files in the local directory (e.g.,\n\
  \                 invalid Unicode file name) gsutil will print an error message\n\
  \                 and abort.\n\n  -D             Copy in \"daisy chain\" mode, i.e.,\
  \ copying between two buckets\n                 by hooking a download to an upload,\
  \ via the machine where\n                 gsutil is run. This stands in contrast\
  \ to the default, where\n                 data are copied between two buckets \"\
  in the cloud\", i.e.,\n                 without needing to copy via the machine\
  \ where gsutil runs.\n\n                 By default, a \"copy in the cloud\" when\
  \ the source is a\n                 composite object will retain the composite nature\
  \ of the\n                 object. However, Daisy chain mode can be used to change\
  \ a\n                 composite object into a non-composite object. For example:\n\
  \n                     gsutil cp -D -p gs://bucket/obj gs://bucket/obj_tmp\n   \
  \                  gsutil mv -p gs://bucket/obj_tmp gs://bucket/obj\n\n        \
  \         Note: Daisy chain mode is automatically used when copying\n          \
  \       between providers (e.g., to copy data from Google Cloud Storage\n      \
  \           to another provider).\n\n  -e             Exclude symlinks. When specified,\
  \ symbolic links will not be\n                 copied.\n\n  -I             Causes\
  \ gsutil to read the list of files or objects to copy from\n                 stdin.\
  \ This allows you to run a program that generates the list\n                 of\
  \ files to upload/download.\n\n  -L <file>      Outputs a manifest log file with\
  \ detailed information about\n                 each item that was copied. This manifest\
  \ contains the following\n                 information for each item:\n\n      \
  \           - Source path.\n                 - Destination path.\n             \
  \    - Source size.\n                 - Bytes transferred.\n                 - MD5\
  \ hash.\n                 - UTC date and time transfer was started in ISO 8601 format.\n\
  \                 - UTC date and time transfer was completed in ISO 8601 format.\n\
  \                 - Upload id, if a resumable upload was performed.\n          \
  \       - Final result of the attempted transfer, success or failure.\n        \
  \         - Failure details, if any.\n\n                 If the log file already\
  \ exists, gsutil will use the file as an\n                 input to the copy process,\
  \ and will also append log items to\n                 the existing file. Files/objects\
  \ that are marked in the\n                 existing log file as having been successfully\
  \ copied (or\n                 skipped) will be ignored. Files/objects without entries\
  \ will be\n                 copied and ones previously marked as unsuccessful will\
  \ be\n                 retried. This can be used in conjunction with the -c option\
  \ to\n                 build a script that copies a large number of objects reliably,\n\
  \                 using a bash script like the following:\n\n                  \
  \ until gsutil cp -c -L cp.log -r ./dir gs://bucket; do\n                     sleep\
  \ 1\n                   done\n\n                 The -c option will cause copying\
  \ to continue after failures\n                 occur, and the -L option will allow\
  \ gsutil to pick up where it\n                 left off without duplicating work.\
  \ The loop will continue\n                 running as long as gsutil exits with\
  \ a non-zero status (such a\n                 status indicates there was at least\
  \ one failure during the\n                 gsutil run).\n\n                 Note:\
  \ If you're trying to synchronize the contents of a\n                 directory\
  \ and a bucket (or two buckets), see\n                 \"gsutil help rsync\".\n\n\
  \  -n             No-clobber. When specified, existing files or objects at the\n\
  \                 destination will not be overwritten. Any items that are skipped\n\
  \                 by this option will be reported as being skipped. This option\n\
  \                 will perform an additional GET request to check if an item\n \
  \                exists before attempting to upload the data. This will save\n \
  \                retransmitting data, but the additional HTTP requests may make\n\
  \                 small object transfers slower and more expensive.\n\n  -p    \
  \         Causes ACLs to be preserved when copying in the cloud. Note\n        \
  \         that this option has performance and cost implications when\n        \
  \         using  the XML API, as it requires separate HTTP calls for\n         \
  \        interacting with ACLs. (There are no such performance or cost\n       \
  \          implications when using the -p option with the JSON API.) The\n     \
  \            performance issue can be mitigated to some degree by using\n      \
  \           gsutil -m cp to cause parallel copying. Note that this option\n    \
  \             only works if you have OWNER access to all of the objects that\n \
  \                are copied.\n\n                 You can avoid the additional performance\
  \ and cost of using\n                 cp -p if you want all objects in the destination\
  \ bucket to end\n                 up with the same ACL by setting a default object\
  \ ACL on that\n                 bucket instead of using cp -p. See \"gsutil help\
  \ defacl\".\n\n                 Note that it's not valid to specify both the -a\
  \ and -p options\n                 together.\n\n  -P             Causes POSIX attributes\
  \ to be preserved when objects are\n                 copied. With this feature enabled,\
  \ gsutil cp will copy fields\n                 provided by stat. These are the user\
  \ ID of the owner, the group\n                 ID of the owning group, the mode\
  \ (permissions) of the file, and\n                 the access/modification time\
  \ of the file. For downloads, these\n                 attributes will only be set\
  \ if the source objects were uploaded\n                 with this flag enabled.\n\
  \n                 On Windows, this flag will only set and restore access time and\n\
  \                 modification time. This is because Windows doesn't have a\n  \
  \               notion of POSIX uid/gid/mode.\n\n  -R, -r         The -R and -r\
  \ options are synonymous. Causes directories,\n                 buckets, and bucket\
  \ subdirectories to be copied recursively.\n                 If you neglect to use\
  \ this option for an upload, gsutil will\n                 copy any files it finds\
  \ and skip any directories. Similarly,\n                 neglecting to specify this\
  \ option for a download will cause\n                 gsutil to copy any objects\
  \ at the current bucket directory\n                 level, and skip any subdirectories.\n\
  \n  -s <class>     The storage class of the destination object(s). If not\n    \
  \             specified, the default storage class of the destination bucket\n \
  \                is used. Not valid for copying to non-cloud destinations.\n\n \
  \ -U             Skip objects with unsupported object types instead of failing.\n\
  \                 Unsupported object types are Amazon S3 Objects in the GLACIER\n\
  \                 storage class.\n\n  -v             Requests that the version-specific\
  \ URL for each uploaded object\n                 be printed. Given this URL you\
  \ can make future upload requests\n                 that are safe in the face of\
  \ concurrent updates, because Google\n                 Cloud Storage will refuse\
  \ to perform the update if the current\n                 object version doesn't\
  \ match the version-specific URL. See\n                 \"gsutil help versions\"\
  \ for more details.\n\n  -z <ext,...>   Applies gzip content-encoding to any file\
  \ upload whose\n                 extension matches the -z extension list. This is\
  \ useful when\n                 uploading files with compressible content (such\
  \ as .js, .css,\n                 or .html files) because it saves network bandwidth\
  \ and space\n                 in Google Cloud Storage, which in turn reduces storage\
  \ costs.\n\n                 When you specify the -z option, the data from your\
  \ files is\n                 compressed before it is uploaded, but your actual files\
  \ are\n                 left uncompressed on the local disk. The uploaded objects\n\
  \                 retain the Content-Type and name of the original files but are\n\
  \                 given a Content-Encoding header with the value \"gzip\" to\n \
  \                indicate that the object data stored are compressed on the\n  \
  \               Google Cloud Storage servers.\n\n                 For example, the\
  \ following command:\n\n                   gsutil cp -z html -a public-read \\\n\
  \                     cattypes.html tabby.jpeg gs://mycats\n\n                 will\
  \ do all of the following:\n\n                 - Upload the files cattypes.html\
  \ and tabby.jpeg to the bucket\n                   gs://mycats (cp command)\n  \
  \               - Set the Content-Type of cattypes.html to text/html and\n     \
  \              tabby.jpeg to image/jpeg (based on file extensions)\n           \
  \      - Compress the data in the file cattypes.html (-z option)\n             \
  \    - Set the Content-Encoding for cattypes.html to gzip\n                   (-z\
  \ option)\n                 - Set the ACL for both files to public-read (-a option)\n\
  \                 - If a user tries to view cattypes.html in a browser, the\n  \
  \                 browser will know to uncompress the data based on the\n      \
  \             Content-Encoding header and to render it as HTML based on\n      \
  \             the Content-Type header.\n\n                 Note that if you download\
  \ an object with Content-Encoding:gzip\n                 gsutil will decompress\
  \ the content before writing the local\n                 file.\n\n  -Z         \
  \    Applies gzip content-encoding to file uploads. This option\n              \
  \   works like the -z option described above, but it applies to\n              \
  \   all uploaded files, regardless of extension.\n\n                 Warning: If\
  \ you use this option and some of the source files\n                 don't compress\
  \ well (e.g., that's often true of binary data),\n                 this option may\
  \ result in files taking up more space in the\n                 cloud than they\
  \ would if left uncompressed.\n"
generated_using:
- --help
docker_image:
