!Command
command:
- adam-shell
positional: []
named:
- !Flag
  description: preload <file>, enforcing line-by-line interpretation
  synonyms:
  - -I
  args: !SimpleFlagArg
    name: file
  optional: true
- !Flag
  description: "spark://host:port, mesos://host:port, yarn,\nk8s://https://host:port,\
    \ or local (Default: local[*])."
  synonyms:
  - --master
  args: !SimpleFlagArg
    name: MASTER_URL
  optional: true
- !Flag
  description: "Whether to launch the driver program locally (\"client\") or\non one\
    \ of the worker machines inside the cluster (\"cluster\")\n(Default: client)."
  synonyms:
  - --deploy-mode
  args: !SimpleFlagArg
    name: DEPLOY_MODE
  optional: true
- !Flag
  description: Your application's main class (for Java / Scala apps).
  synonyms:
  - --class
  args: !SimpleFlagArg
    name: CLASS_NAME
  optional: true
- !Flag
  description: A name of your application.
  synonyms:
  - --name
  args: !SimpleFlagArg
    name: NAME
  optional: true
- !Flag
  description: "Comma-separated list of jars to include on the driver\nand executor\
    \ classpaths."
  synonyms:
  - --jars
  args: !SimpleFlagArg
    name: JARS
  optional: true
- !Flag
  description: "Comma-separated list of maven coordinates of jars to include\non the\
    \ driver and executor classpaths. Will search the local\nmaven repo, then maven\
    \ central and any additional remote\nrepositories given by --repositories. The\
    \ format for the\ncoordinates should be groupId:artifactId:version."
  synonyms:
  - --packages
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Comma-separated list of groupId:artifactId, to exclude while\nresolving\
    \ the dependencies provided in --packages to avoid\ndependency conflicts."
  synonyms:
  - --exclude-packages
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Comma-separated list of additional remote repositories to\nsearch\
    \ for the maven coordinates given with --packages."
  synonyms:
  - --repositories
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Comma-separated list of .zip, .egg, or .py files to place\non the\
    \ PYTHONPATH for Python apps."
  synonyms:
  - --py-files
  args: !SimpleFlagArg
    name: PY_FILES
  optional: true
- !Flag
  description: "Comma-separated list of files to be placed in the working\ndirectory\
    \ of each executor. File paths of these files\nin executors can be accessed via\
    \ SparkFiles.get(fileName)."
  synonyms:
  - --files
  args: !SimpleFlagArg
    name: FILES
  optional: true
- !Flag
  description: =VALUE           Arbitrary Spark configuration property.
  synonyms:
  - --conf
  args: !SimpleFlagArg
    name: PROP
  optional: true
- !Flag
  description: "Path to a file from which to load extra properties. If not\nspecified,\
    \ this will look for conf/spark-defaults.conf."
  synonyms:
  - --properties-file
  args: !SimpleFlagArg
    name: FILE
  optional: true
- !Flag
  description: 'Memory for driver (e.g. 1000M, 2G) (Default: 1024M).'
  synonyms:
  - --driver-memory
  args: !SimpleFlagArg
    name: MEM
  optional: true
- !Flag
  description: Extra Java options to pass to the driver.
  synonyms:
  - --driver-java-options
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: Extra library path entries to pass to the driver.
  synonyms:
  - --driver-library-path
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Extra class path entries to pass to the driver. Note that\njars added\
    \ with --jars are automatically included in the\nclasspath."
  synonyms:
  - --driver-class-path
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: 'Memory per executor (e.g. 1000M, 2G) (Default: 1G).'
  synonyms:
  - --executor-memory
  args: !SimpleFlagArg
    name: MEM
  optional: true
- !Flag
  description: "User to impersonate when submitting the application.\nThis argument\
    \ does not work with --principal / --keytab."
  synonyms:
  - --proxy-user
  args: !SimpleFlagArg
    name: NAME
  optional: true
- !Flag
  description: Print additional debug output.
  synonyms:
  - --verbose
  - -v
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Number of cores used by the driver, only in cluster mode\n(Default:\
    \ 1)."
  synonyms:
  - --driver-cores
  args: !SimpleFlagArg
    name: NUM
  optional: true
- !Flag
  description: If given, restarts the driver on failure.
  synonyms:
  - --supervise
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: If given, kills the driver specified.
  synonyms:
  - --kill
  args: !SimpleFlagArg
    name: SUBMISSION_ID
  optional: true
- !Flag
  description: If given, requests the status of the driver specified.
  synonyms:
  - --status
  args: !SimpleFlagArg
    name: SUBMISSION_ID
  optional: true
- !Flag
  description: Total cores for all executors.
  synonyms:
  - --total-executor-cores
  args: !SimpleFlagArg
    name: NUM
  optional: true
- !Flag
  description: "Number of cores per executor. (Default: 1 in YARN mode,\nor all available\
    \ cores on the worker in standalone mode)"
  synonyms:
  - --executor-cores
  args: !SimpleFlagArg
    name: NUM
  optional: true
- !Flag
  description: 'The YARN queue to submit to (Default: "default").'
  synonyms:
  - --queue
  args: !SimpleFlagArg
    name: QUEUE_NAME
  optional: true
- !Flag
  description: "Number of executors to launch (Default: 2).\nIf dynamic allocation\
    \ is enabled, the initial number of\nexecutors will be at least NUM."
  synonyms:
  - --num-executors
  args: !SimpleFlagArg
    name: NUM
  optional: true
- !Flag
  description: "Comma separated list of archives to be extracted into the\nworking\
    \ directory of each executor."
  synonyms:
  - --archives
  args: !SimpleFlagArg
    name: ARCHIVES
  optional: true
- !Flag
  description: "Principal to be used to login to KDC, while running on\nsecure HDFS."
  synonyms:
  - --principal
  args: !SimpleFlagArg
    name: PRINCIPAL
  optional: true
- !Flag
  description: "The full path to the file that contains the keytab for the\nprincipal\
    \ specified above. This keytab will be copied to\nthe node running the Application\
    \ Master via the Secure\nDistributed Cache, for renewing the login tickets and\
    \ the\ndelegation tokens periodically.\n"
  synonyms:
  - --keytab
  args: !SimpleFlagArg
    name: KEYTAB
  optional: true
parent:
subcommands: []
usage: []
help_flag: !Flag
  description: Show this help message and exit.
  synonyms:
  - --help
  - -h
  args: !EmptyFlagArg {}
  optional: true
usage_flag:
version_flag: !Flag
  description: ',                  Print the version of current Spark.'
  synonyms:
  - --version
  args: !EmptyFlagArg {}
  optional: true
help_text: "['/usr/local/bin/..', '/usr/local/lib/python3.6/site-packages/bdgenomics/adam']\n\
  Using SPARK_SHELL=/usr/local/bin/spark-shell\nUsage: ./bin/spark-shell [options]\n\
  \nScala REPL options:\n  -I <file>                   preload <file>, enforcing line-by-line\
  \ interpretation\n\nOptions:\n  --master MASTER_URL         spark://host:port, mesos://host:port,\
  \ yarn,\n                              k8s://https://host:port, or local (Default:\
  \ local[*]).\n  --deploy-mode DEPLOY_MODE   Whether to launch the driver program\
  \ locally (\"client\") or\n                              on one of the worker machines\
  \ inside the cluster (\"cluster\")\n                              (Default: client).\n\
  \  --class CLASS_NAME          Your application's main class (for Java / Scala apps).\n\
  \  --name NAME                 A name of your application.\n  --jars JARS      \
  \           Comma-separated list of jars to include on the driver\n            \
  \                  and executor classpaths.\n  --packages                  Comma-separated\
  \ list of maven coordinates of jars to include\n                              on\
  \ the driver and executor classpaths. Will search the local\n                  \
  \            maven repo, then maven central and any additional remote\n        \
  \                      repositories given by --repositories. The format for the\n\
  \                              coordinates should be groupId:artifactId:version.\n\
  \  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude\
  \ while\n                              resolving the dependencies provided in --packages\
  \ to avoid\n                              dependency conflicts.\n  --repositories\
  \              Comma-separated list of additional remote repositories to\n     \
  \                         search for the maven coordinates given with --packages.\n\
  \  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files\
  \ to place\n                              on the PYTHONPATH for Python apps.\n \
  \ --files FILES               Comma-separated list of files to be placed in the\
  \ working\n                              directory of each executor. File paths\
  \ of these files\n                              in executors can be accessed via\
  \ SparkFiles.get(fileName).\n\n  --conf PROP=VALUE           Arbitrary Spark configuration\
  \ property.\n  --properties-file FILE      Path to a file from which to load extra\
  \ properties. If not\n                              specified, this will look for\
  \ conf/spark-defaults.conf.\n\n  --driver-memory MEM         Memory for driver (e.g.\
  \ 1000M, 2G) (Default: 1024M).\n  --driver-java-options       Extra Java options\
  \ to pass to the driver.\n  --driver-library-path       Extra library path entries\
  \ to pass to the driver.\n  --driver-class-path         Extra class path entries\
  \ to pass to the driver. Note that\n                              jars added with\
  \ --jars are automatically included in the\n                              classpath.\n\
  \n  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n\
  \n  --proxy-user NAME           User to impersonate when submitting the application.\n\
  \                              This argument does not work with --principal / --keytab.\n\
  \n  --help, -h                  Show this help message and exit.\n  --verbose, -v\
  \               Print additional debug output.\n  --version,                  Print\
  \ the version of current Spark.\n\n Cluster deploy mode only:\n  --driver-cores\
  \ NUM          Number of cores used by the driver, only in cluster mode\n      \
  \                        (Default: 1).\n\n Spark standalone or Mesos with cluster\
  \ deploy mode only:\n  --supervise                 If given, restarts the driver\
  \ on failure.\n  --kill SUBMISSION_ID        If given, kills the driver specified.\n\
  \  --status SUBMISSION_ID      If given, requests the status of the driver specified.\n\
  \n Spark standalone and Mesos only:\n  --total-executor-cores NUM  Total cores for\
  \ all executors.\n\n Spark standalone and YARN only:\n  --executor-cores NUM   \
  \     Number of cores per executor. (Default: 1 in YARN mode,\n                \
  \              or all available cores on the worker in standalone mode)\n\n YARN-only:\n\
  \  --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\"\
  ).\n  --num-executors NUM         Number of executors to launch (Default: 2).\n\
  \                              If dynamic allocation is enabled, the initial number\
  \ of\n                              executors will be at least NUM.\n  --archives\
  \ ARCHIVES         Comma separated list of archives to be extracted into the\n \
  \                             working directory of each executor.\n  --principal\
  \ PRINCIPAL       Principal to be used to login to KDC, while running on\n     \
  \                         secure HDFS.\n  --keytab KEYTAB             The full path\
  \ to the file that contains the keytab for the\n                              principal\
  \ specified above. This keytab will be copied to\n                             \
  \ the node running the Application Master via the Secure\n                     \
  \         Distributed Cache, for renewing the login tickets and the\n          \
  \                    delegation tokens periodically.\n      \n"
generated_using:
- --help
