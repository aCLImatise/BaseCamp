!Command
command:
- gcloud
- ml-engine
- predict
positional: []
named:
- !Flag
  description: Name of the model.
  synonyms:
  - --model
  args: !SimpleFlagArg
    name: MODEL
  optional: true
- !Flag
  description: "Path to a local file from which instances are read. Instances are\
    \ in\nJSON format; newline delimited.\nAn example of the JSON instances file:\n\
    {\"images\": [0.0, ..., 0.1], \"key\": 3}\n{\"images\": [0.0, ..., 0.1], \"key\"\
    : 2}\n...\nThis flag accepts \"-\" for stdin."
  synonyms:
  - --json-instances
  args: !SimpleFlagArg
    name: JSON_INSTANCES
  optional: true
- !Flag
  description: "Path to a local file from which instances are read. Instances are\
    \ in\nUTF-8 encoded text format; newline delimited.\nAn example of the text instances\
    \ file:\n107,4.9,2.5,4.5,1.7\n100,5.7,2.8,4.1,1.3\n...\nThis flag accepts \"-\"\
    \ for stdin."
  synonyms:
  - --text-instances
  args: !SimpleFlagArg
    name: TEXT_INSTANCES
  optional: true
parent:
subcommands: []
usage: []
help_flag: !Flag
  description: $ gcloud help for details.
  synonyms:
  - --flatten
  - --format
  - --help
  - --log-http
  - --project
  - --quiet
  - --trace-token
  - --user-output-enabled
  - --verbosity.
  args: !SimpleFlagArg
    name: Run
  optional: true
usage_flag:
version_flag: !Flag
  description: "Model version to be used.\nIf unspecified, the default version of\
    \ the model will be used. To list\nmodel versions run\n$ gcloud ml-engine versions\
    \ list"
  synonyms:
  - --version
  args: !SimpleFlagArg
    name: VERSION
  optional: true
help_text: "NAME\n    gcloud ml-engine predict - run Cloud ML Engine online prediction\n\
  \nSYNOPSIS\n    gcloud ml-engine predict --model=MODEL\n        (--json-instances=JSON_INSTANCES\
  \ | --text-instances=TEXT_INSTANCES)\n        [--version=VERSION] [GCLOUD_WIDE_FLAG\
  \ ...]\n\nDESCRIPTION\n    gcloud ml-engine predict sends a prediction request to\
  \ Cloud ML Engine for\n    the given instances. This command will read up to 100\
  \ instances, though the\n    service itself will accept instances up to the payload\
  \ limit size\n    (currently, 1.5MB). If you are predicting on more instances, you\
  \ should use\n    batch prediction via\n\n        $ gcloud ml-engine jobs submit\
  \ prediction.\n\nREQUIRED FLAGS\n     --model=MODEL\n        Name of the model.\n\
  \n    Exactly one of these must be specified:\n\n     --json-instances=JSON_INSTANCES\n\
  \        Path to a local file from which instances are read. Instances are in\n\
  \        JSON format; newline delimited.\n\n        An example of the JSON instances\
  \ file:\n\n            {\"images\": [0.0, ..., 0.1], \"key\": 3}\n            {\"\
  images\": [0.0, ..., 0.1], \"key\": 2}\n            ...\n\n        This flag accepts\
  \ \"-\" for stdin.\n\n     --text-instances=TEXT_INSTANCES\n        Path to a local\
  \ file from which instances are read. Instances are in\n        UTF-8 encoded text\
  \ format; newline delimited.\n\n        An example of the text instances file:\n\
  \n            107,4.9,2.5,4.5,1.7\n            100,5.7,2.8,4.1,1.3\n           \
  \ ...\n\n        This flag accepts \"-\" for stdin.\n\nOPTIONAL FLAGS\n     --version=VERSION\n\
  \        Model version to be used.\n\n        If unspecified, the default version\
  \ of the model will be used. To list\n        model versions run\n\n           \
  \ $ gcloud ml-engine versions list\n\nGCLOUD WIDE FLAGS\n    These flags are available\
  \ to all commands: --account, --configuration,\n    --flatten, --format, --help,\
  \ --log-http, --project, --quiet, --trace-token,\n    --user-output-enabled, --verbosity.\
  \ Run $ gcloud help for details.\n"
generated_using:
- --help
