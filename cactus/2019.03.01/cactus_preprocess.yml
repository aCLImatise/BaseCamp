!Command
positional:
- !Positional
  description: Directory where the processed sequences will be placed
  position: 0
  name: outputSequenceDir
  optional: false
- !Positional
  description: input FASTA file(s)
  position: 1
  name: inputSequences
  optional: false
- !Positional
  description: 'The location of the job store for the workflow. A job store holds
    persistent information about the jobs and files in a workflow. If the workflow
    is run with a distributed batch system, the job store must be accessible by all
    worker nodes. Depending on the desired job store implementation, the location
    should be formatted according to one of the following schemes: file:<path> where
    <path> points to a directory on the file systen aws:<region>:<prefix> where <region>
    is the name of an AWS region like us- west-2 and <prefix> will be prepended to
    the names of any top-level AWS resources in use by job store, e.g. S3 buckets.
    azure:<account>:<prefix> google:<project_id>:<prefix> TODO: explain For backwards
    compatibility, you may also specify ./foo (equivalent to file:./foo or just file:foo)
    or /bar (equivalent to file:/bar).'
  position: 0
  name: jobStore
  optional: false
named:
- !Flag
  description: Same as --logCritical
  synonyms:
  - --logOff
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: Turn on logging at level CRITICAL and above. (default is INFO)
  synonyms:
  - --logCritical
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: Turn on logging at level ERROR and above. (default is INFO)
  synonyms:
  - --logError
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: Turn on logging at level WARNING and above. (default is INFO)
  synonyms:
  - --logWarning
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: Turn on logging at level INFO and above. (default is INFO)
  synonyms:
  - --logInfo
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: Turn on logging at level DEBUG and above. (default is INFO)
  synonyms:
  - --logDebug
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: Log at given level (may be either OFF (or CRITICAL), ERROR, WARN (or
    WARNING), INFO or DEBUG). (default is INFO)
  synonyms:
  - --logLevel
  args: !SimpleFlagArg
    name: LOGLEVEL
  optional: true
- !Flag
  description: File to log in
  synonyms:
  - --logFile
  args: !SimpleFlagArg
    name: LOGFILE
  optional: true
- !Flag
  description: Turn on rotating logging, which prevents log files getting too big.
  synonyms:
  - --rotatingLogging
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: Absolute path to directory where temporary files generated during the
    Toil run should be placed. Temp files and folders will be placed in a directory
    toil-<workflowID> within workDir (The workflowID is generated by Toil and will
    be reported in the workflow logs. Default is determined by the variables (TMPDIR,
    TEMP, TMP) via mkdtemp. This directory needs to exist on all machines running
    jobs.
  synonyms:
  - --workDir
  args: !SimpleFlagArg
    name: WORKDIR
  optional: true
- !Flag
  description: Records statistics about the toil workflow to be used by 'toil stats'.
  synonyms:
  - --stats
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Determines the deletion of the jobStore upon completion of the program.\
    \ Choices: 'always', 'onError','never', 'onSuccess'. The --stats option requires\
    \ information from the jobStore upon completion so the jobStore will never be\
    \ deleted withthat flag. If you wish to be able to restart the run, choose 'never'\
    \ or 'onSuccess'. Default is 'never' if stats is enabled, and 'onSuccess' otherwise"
  synonyms:
  - --clean
  args: !ChoiceFlagArg
    choices: !!set
      onSuccess:
      never:
      always:
      onError:
  optional: true
- !Flag
  description: "Determines deletion of temporary worker directory upon completion\
    \ of a job. Choices: 'always', 'never', 'onSuccess'. Default = always. WARNING:\
    \ This option should be changed for debugging only. Running a full pipeline with\
    \ this option could fill your disk with intermediate data."
  synonyms:
  - --cleanWorkDir
  args: !ChoiceFlagArg
    choices: !!set
      onError:
      never:
      always:
      onSuccess:
  optional: true
- !Flag
  description: '[CLUSTERSTATS] If enabled, writes out JSON resource usage statistics
    to a file. The default location for this file is the current working directory,
    but an absolute path can also be passed to specify where this file should be written.
    This options only applies when using scalable batch systems.'
  synonyms:
  - --clusterStats
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: If --restart is specified then will attempt to restart existing workflow
    at the location pointed to by the --jobStore option. Will raise an exception if
    the workflow does not exist
  synonyms:
  - --restart
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: The type of batch system to run the job(s) with, currently can be one
    of LSF, Mesos, Slurm, Torque, HTCondor, singleMachine, parasol, gridEngine'. default=singleMachine
  synonyms:
  - --batchSystem
  args: !SimpleFlagArg
    name: BATCHSYSTEM
  optional: true
- !Flag
  description: Should hot-deployment of the user script be deactivated? If True, the
    user script/package should be present at the same location on all workers. default=false
  synonyms:
  - --disableHotDeployment
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: The name or path of the parasol program. Will be looked up on PATH
    unless it starts with a slashdefault=parasol
  synonyms:
  - --parasolCommand
  args: !SimpleFlagArg
    name: PARASOLCOMMAND
  optional: true
- !Flag
  description: Maximum number of job batches the Parasol batch is allowed to create.
    One batch is created for jobs with a a unique set of resource requirements. default=1000
  synonyms:
  - --parasolMaxBatches
  args: !SimpleFlagArg
    name: PARASOLMAXBATCHES
  optional: true
- !Flag
  description: A scaling factor to change the value of all submitted tasks's submitted
    cores. Used in singleMachine batch system. default=1
  synonyms:
  - --scale
  args: !SimpleFlagArg
    name: SCALE
  optional: true
- !Flag
  description: When using Toil's importFile function for staging, input files are
    copied to the job store. Specifying this option saves space by sym-linking imported
    files. As long as caching is enabled Toil will protect the file automatically
    by changing the permissions to read-only.
  synonyms:
  - --linkImports
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: 'The host and port of the Mesos master separated by colon. (default:
    115.146.93.132:5050)'
  synonyms:
  - --mesosMaster
  args: !SimpleFlagArg
    name: MESOSMASTERADDRESS
  optional: true
- !Flag
  description: The provisioner for cluster auto-scaling. The currently supported choices
    are'cgcloud' or 'aws'. The default is None.
  synonyms:
  - --provisioner
  args: !ChoiceFlagArg
    choices: !!set
      aws:
  optional: true
- !Flag
  description: List of node types separated by commas. The syntax for each node type
    depends on the provisioner used. For the cgcloud and AWS provisioners this is
    the name of an EC2 instance type, optionally followed by a colon and the price
    in dollars to bid for a spot instance of that type, for example 'c3.8xlarge:0.42'.If
    no spot bid is specified, nodes of this type will be non- preemptable.It is acceptable
    to specify an instance as both preemptable and non-preemptable, including it twice
    in the list. In that case,preemptable nodes of that type will be preferred when
    creating new nodes once the maximum number of preemptable-nodes has beenreached.
  synonyms:
  - --nodeTypes
  args: !SimpleFlagArg
    name: NODETYPES
  optional: true
- !Flag
  description: Options for provisioning the nodes. The syntax depends on the provisioner
    used. Neither the CGCloud nor the AWS provisioner support any node options.
  synonyms:
  - --nodeOptions
  args: !SimpleFlagArg
    name: NODEOPTIONS
  optional: true
- !Flag
  description: Mininum number of nodes of each type in the cluster, if using auto-scaling.
    This should be provided as a comma-separated list of the same length as the list
    of node types. default=0
  synonyms:
  - --minNodes
  args: !SimpleFlagArg
    name: MINNODES
  optional: true
- !Flag
  description: Maximum number of nodes of each type in the cluster, if using autoscaling,
    provided as a comma-separated list. The first value is used as a default if the
    list length is less than the number of nodeTypes. default=10
  synonyms:
  - --maxNodes
  args: !SimpleFlagArg
    name: MAXNODES
  optional: true
- !Flag
  description: The total number of nodes estimated to be required to compute the issued
    jobs is multiplied by the alpha packing parameter to produce the actual number
    of nodes requested. Values of this coefficient greater than one will tend to over
    provision and values less than one will under provision. default=0.8
  synonyms:
  - --alphaPacking
  args: !SimpleFlagArg
    name: ALPHAPACKING
  optional: true
- !Flag
  description: A smoothing parameter to prevent unnecessary oscillations in the number
    of provisioned nodes. If the number of nodes is within the beta inertia of the
    currently provisioned number of nodes then no change is made to the number of
    requested nodes. default=1.2
  synonyms:
  - --betaInertia
  args: !SimpleFlagArg
    name: BETAINERTIA
  optional: true
- !Flag
  description: The interval (seconds) between assessing if the scale of the cluster
    needs to change. default=30
  synonyms:
  - --scaleInterval
  args: !SimpleFlagArg
    name: SCALEINTERVAL
  optional: true
- !Flag
  description: The preference of the autoscaler to replace preemptable nodes with
    non-preemptable nodes, when preemptable nodes cannot be started for some reason.
    Defaults to 0.0. This value must be between 0.0 and 1.0, inclusive. A value of
    0.0 disables such compensation, a value of 0.5 compensates two missing preemptable
    nodes with a non-preemptable one. A value of 1.0 replaces every missing pre-emptable
    node with a non-preemptable one.
  synonyms:
  - --preemptableCompensation
  args: !SimpleFlagArg
    name: PREEMPTABLECOMPENSATION
  optional: true
- !Flag
  description: Specify the size of the root volume of worker nodes when they are launched
    in gigabytes. You may want to set this if your jobs require a lot of disk space.
    The default value is 50.
  synonyms:
  - --nodeStorage
  args: !SimpleFlagArg
    name: NODESTORAGE
  optional: true
- !Flag
  description: Enable the prometheus/grafana dashboard for monitoring CPU/RAM usage,
    queue size, and issued jobs.
  synonyms:
  - --metrics
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: The maximum number of service jobs that can be run concurrently, excluding
    service jobs running on preemptable nodes. default=9223372036854775807
  synonyms:
  - --maxServiceJobs
  args: !SimpleFlagArg
    name: MAXSERVICEJOBS
  optional: true
- !Flag
  description: The maximum number of service jobs that can run concurrently on preemptable
    nodes. default=9223372036854775807
  synonyms:
  - --maxPreemptableServiceJobs
  args: !SimpleFlagArg
    name: MAXPREEMPTABLESERVICEJOBS
  optional: true
- !Flag
  description: The minimum number of seconds to observe the cluster stuck running
    only the same service jobs before throwing a deadlock exception. default=60
  synonyms:
  - --deadlockWait
  args: !SimpleFlagArg
    name: DEADLOCKWAIT
  optional: true
- !Flag
  description: Time, in seconds, to wait before doing a scheduler query for job state.
    Return cached results if within the waiting period.
  synonyms:
  - --statePollingWait
  args: !SimpleFlagArg
    name: STATEPOLLINGWAIT
  optional: true
- !Flag
  description: The maximum number of CPU cores to request from the batch system at
    any one time. Standard suffixes like K, Ki, M, Mi, G or Gi are supported. Default
    is 8.0 Ei
  synonyms:
  - --maxCores
  args: !SimpleFlagArg
    name: INT
  optional: true
- !Flag
  description: The maximum amount of memory to request from the batch system at any
    one time. Standard suffixes like K, Ki, M, Mi, G or Gi are supported. Default
    is 8.0 Ei
  synonyms:
  - --maxMemory
  args: !SimpleFlagArg
    name: INT
  optional: true
- !Flag
  description: The maximum amount of disk space to request from the batch system at
    any one time. Standard suffixes like K, Ki, M, Mi, G or Gi are supported. Default
    is 8.0 Ei
  synonyms:
  - --maxDisk
  args: !SimpleFlagArg
    name: INT
  optional: true
- !Flag
  description: Number of times to retry a failing job before giving up and labeling
    job failed. default=1
  synonyms:
  - --retryCount
  args: !SimpleFlagArg
    name: RETRYCOUNT
  optional: true
- !Flag
  description: Maximum runtime of a job (in seconds) before we kill it (this is a
    lower bound, and the actual time before killing the job may be longer). default=9223372036854775807
  synonyms:
  - --maxJobDuration
  args: !SimpleFlagArg
    name: MAXJOBDURATION
  optional: true
- !Flag
  description: Period of time to wait (in seconds) between checking for missing/overlong
    jobs, that is jobs which get lost by the batch system. Expert parameter. default=3600
  synonyms:
  - --rescueJobsFrequency
  args: !SimpleFlagArg
    name: RESCUEJOBSFREQUENCY
  optional: true
- !Flag
  description: Disables caching in the file store. This flag must be set to use a
    batch system that does not support caching such as Grid Engine, Parasol, LSF,
    or Slurm
  synonyms:
  - --disableCaching
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: The maximum size of a job log file to keep (in bytes), log files larger
    than this will be truncated to the last X bytes. Setting this option to zero will
    prevent any truncation. Setting this option to a negative value will truncate
    from the beginning.Default=62.5 K
  synonyms:
  - --maxLogFileSize
  args: !SimpleFlagArg
    name: MAXLOGFILESIZE
  optional: true
- !Flag
  description: "[WRITELOGS] Write worker logs received by the leader into their own\
    \ files at the specified path. The current working directory will be used if a\
    \ path is not specified explicitly. Note: By default only the logs of failed jobs\
    \ are returned to leader. Set log level to 'debug' to get logs back from successful\
    \ jobs, and adjust 'maxLogFileSize' to control the truncation limit for worker\
    \ logs."
  synonyms:
  - --writeLogs
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: '[WRITELOGSGZIP] Identical to --writeLogs except the logs files are
    gzipped on the leader.'
  synonyms:
  - --writeLogsGzip
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: Enable real-time logging from workers to masters
  synonyms:
  - --realTimeLogging
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: Path to file containing 32 character key to be used for server-side
    encryption on awsJobStore or googleJobStore. SSE will not be used if this flag
    is not passed.
  synonyms:
  - --sseKey
  args: !SimpleFlagArg
    name: SSEKEY
  optional: true
- !Flag
  description: Path to file containing 256-bit key to be used for client-side encryption
    on azureJobStore. By default, no encryption is used.
  synonyms:
  - --cseKey
  args: !SimpleFlagArg
    name: CSEKEY
  optional: true
- !Flag
  description: =VALUE or NAME, -e NAME=VALUE or NAME Set an environment variable early
    on in the worker. If VALUE is omitted, it will be looked up in the current environment.
    Independently of this option, the worker will try to emulate the leader's environment
    before running a job. Using this option, a variable can be injected into the worker
    process itself before it is started.
  synonyms:
  - --setEnv
  args: !SimpleFlagArg
    name: NAME
  optional: true
- !Flag
  description: Interval of time service jobs wait between polling for the existence
    of the keep-alive flag (defailt=60)
  synonyms:
  - --servicePollingInterval
  args: !SimpleFlagArg
    name: SERVICEPOLLINGINTERVAL
  optional: true
- !Flag
  description: Experimental no forking mode for local debugging. Specifically, workers
    are not forked and stderr/stdout are not redirected to the log.
  synonyms:
  - --debugWorker
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: For testing purposes randomly kill 'badWorker' proportion of jobs using
    SIGKILL, default=0.0
  synonyms:
  - --badWorker
  args: !SimpleFlagArg
    name: BADWORKER
  optional: true
- !Flag
  description: When killing the job pick uniformly within the interval from 0.0 to
    'badWorkerFailInterval' seconds after the worker starts, default=0.01
  synonyms:
  - --badWorkerFailInterval
  args: !SimpleFlagArg
    name: BADWORKERFAILINTERVAL
  optional: true
command:
- cactus_preprocess
parent:
subcommands: []
help_flag: !Flag
  description: ''
  synonyms:
  - -h
  args: !EmptyFlagArg {}
  optional: true
usage_flag:
version_flag:
help_text: "usage: cactus_preprocess [-h] [--logOff] [--logCritical] [--logError]\n\
  \                         [--logWarning] [--logInfo] [--logDebug]\n            \
  \             [--logLevel LOGLEVEL] [--logFile LOGFILE]\n                      \
  \   [--rotatingLogging] [--workDir WORKDIR] [--stats]\n                        \
  \ [--clean {always,onError,never,onSuccess}]\n                         [--cleanWorkDir\
  \ {always,never,onSuccess,onError}]\n                         [--clusterStats [CLUSTERSTATS]]\
  \ [--restart]\n                         [--batchSystem BATCHSYSTEM] [--disableHotDeployment]\n\
  \                         [--parasolCommand PARASOLCOMMAND]\n                  \
  \       [--parasolMaxBatches PARASOLMAXBATCHES]\n                         [--scale\
  \ SCALE] [--linkImports]\n                         [--mesosMaster MESOSMASTERADDRESS]\n\
  \                         [--provisioner {aws}] [--nodeTypes NODETYPES]\n      \
  \                   [--nodeOptions NODEOPTIONS] [--minNodes MINNODES]\n        \
  \                 [--maxNodes MAXNODES] [--alphaPacking ALPHAPACKING]\n        \
  \                 [--betaInertia BETAINERTIA]\n                         [--scaleInterval\
  \ SCALEINTERVAL]\n                         [--preemptableCompensation PREEMPTABLECOMPENSATION]\n\
  \                         [--nodeStorage NODESTORAGE] [--metrics]\n            \
  \             [--maxServiceJobs MAXSERVICEJOBS]\n                         [--maxPreemptableServiceJobs\
  \ MAXPREEMPTABLESERVICEJOBS]\n                         [--deadlockWait DEADLOCKWAIT]\n\
  \                         [--statePollingWait STATEPOLLINGWAIT]\n              \
  \           [--defaultMemory INT] [--defaultCores FLOAT]\n                     \
  \    [--defaultDisk INT] [--defaultPreemptable]\n                         [--maxCores\
  \ INT] [--maxMemory INT] [--maxDisk INT]\n                         [--retryCount\
  \ RETRYCOUNT]\n                         [--maxJobDuration MAXJOBDURATION]\n    \
  \                     [--rescueJobsFrequency RESCUEJOBSFREQUENCY]\n            \
  \             [--disableCaching] [--maxLogFileSize MAXLOGFILESIZE]\n           \
  \              [--writeLogs [WRITELOGS]]\n                         [--writeLogsGzip\
  \ [WRITELOGSGZIP]] [--realTimeLogging]\n                         [--sseKey SSEKEY]\
  \ [--cseKey CSEKEY]\n                         [--setEnv NAME=VALUE or NAME]\n  \
  \                       [--servicePollingInterval SERVICEPOLLINGINTERVAL]\n    \
  \                     [--debugWorker] [--badWorker BADWORKER]\n                \
  \         [--badWorkerFailInterval BADWORKERFAILINTERVAL]\n                    \
  \     [--configFile CONFIGFILE]\n                         jobStore outputSequenceDir\
  \ inputSequences\n                         [inputSequences ...]\n\npositional arguments:\n\
  \  outputSequenceDir     Directory where the processed sequences will be placed\n\
  \  inputSequences        input FASTA file(s)\n\noptional arguments:\n  -h, --help\
  \            show this help message and exit\n  --configFile CONFIGFILE\n\nLogging\
  \ Options:\n  Options that control logging\n\n  --logOff              Same as --logCritical\n\
  \  --logCritical         Turn on logging at level CRITICAL and above. (default\n\
  \                        is INFO)\n  --logError            Turn on logging at level\
  \ ERROR and above. (default is\n                        INFO)\n  --logWarning  \
  \        Turn on logging at level WARNING and above. (default\n                \
  \        is INFO)\n  --logInfo             Turn on logging at level INFO and above.\
  \ (default is\n                        INFO)\n  --logDebug            Turn on logging\
  \ at level DEBUG and above. (default is\n                        INFO)\n  --logLevel\
  \ LOGLEVEL   Log at given level (may be either OFF (or CRITICAL),\n            \
  \            ERROR, WARN (or WARNING), INFO or DEBUG). (default is\n           \
  \             INFO)\n  --logFile LOGFILE     File to log in\n  --rotatingLogging\
  \     Turn on rotating logging, which prevents log files\n                     \
  \   getting too big.\n\ntoil core options:\n  Options to specify the location of\
  \ the Toil workflow and turn on stats\n  collation about the performance of jobs.\n\
  \n  jobStore              The location of the job store for the workflow. A job\n\
  \                        store holds persistent information about the jobs and\n\
  \                        files in a workflow. If the workflow is run with a\n  \
  \                      distributed batch system, the job store must be\n       \
  \                 accessible by all worker nodes. Depending on the\n           \
  \             desired job store implementation, the location should\n          \
  \              be formatted according to one of the following\n                \
  \        schemes: file:<path> where <path> points to a\n                       \
  \ directory on the file systen aws:<region>:<prefix>\n                        where\
  \ <region> is the name of an AWS region like us-\n                        west-2\
  \ and <prefix> will be prepended to the names of\n                        any top-level\
  \ AWS resources in use by job store, e.g.\n                        S3 buckets. azure:<account>:<prefix>\n\
  \                        google:<project_id>:<prefix> TODO: explain For\n      \
  \                  backwards compatibility, you may also specify ./foo\n       \
  \                 (equivalent to file:./foo or just file:foo) or /bar\n        \
  \                (equivalent to file:/bar).\n  --workDir WORKDIR     Absolute path\
  \ to directory where temporary files\n                        generated during the\
  \ Toil run should be placed. Temp\n                        files and folders will\
  \ be placed in a directory\n                        toil-<workflowID> within workDir\
  \ (The workflowID is\n                        generated by Toil and will be reported\
  \ in the workflow\n                        logs. Default is determined by the variables\
  \ (TMPDIR,\n                        TEMP, TMP) via mkdtemp. This directory needs\
  \ to exist\n                        on all machines running jobs.\n  --stats   \
  \            Records statistics about the toil workflow to be used\n           \
  \             by 'toil stats'.\n  --clean {always,onError,never,onSuccess}\n   \
  \                     Determines the deletion of the jobStore upon\n           \
  \             completion of the program. Choices: 'always',\n                  \
  \      'onError','never', 'onSuccess'. The --stats option\n                    \
  \    requires information from the jobStore upon completion\n                  \
  \      so the jobStore will never be deleted withthat flag.\n                  \
  \      If you wish to be able to restart the run, choose\n                     \
  \   'never' or 'onSuccess'. Default is 'never' if stats is\n                   \
  \     enabled, and 'onSuccess' otherwise\n  --cleanWorkDir {always,never,onSuccess,onError}\n\
  \                        Determines deletion of temporary worker directory upon\n\
  \                        completion of a job. Choices: 'always', 'never',\n    \
  \                    'onSuccess'. Default = always. WARNING: This option\n     \
  \                   should be changed for debugging only. Running a full\n     \
  \                   pipeline with this option could fill your disk with\n      \
  \                  intermediate data.\n  --clusterStats [CLUSTERSTATS]\n       \
  \                 If enabled, writes out JSON resource usage statistics\n      \
  \                  to a file. The default location for this file is the\n      \
  \                  current working directory, but an absolute path can\n       \
  \                 also be passed to specify where this file should be\n        \
  \                written. This options only applies when using scalable\n      \
  \                  batch systems.\n\ntoil options for restarting an existing workflow:\n\
  \  Allows the restart of an existing workflow\n\n  --restart             If --restart\
  \ is specified then will attempt to restart\n                        existing workflow\
  \ at the location pointed to by the\n                        --jobStore option.\
  \ Will raise an exception if the\n                        workflow does not exist\n\
  \ntoil options for specifying the batch system:\n  Allows the specification of the\
  \ batch system, and arguments to the batch\n  system/big batch system (see below).\n\
  \n  --batchSystem BATCHSYSTEM\n                        The type of batch system\
  \ to run the job(s) with,\n                        currently can be one of LSF,\
  \ Mesos, Slurm, Torque,\n                        HTCondor, singleMachine, parasol,\
  \ gridEngine'.\n                        default=singleMachine\n  --disableHotDeployment\n\
  \                        Should hot-deployment of the user script be\n         \
  \               deactivated? If True, the user script/package should\n         \
  \               be present at the same location on all workers.\n              \
  \          default=false\n  --parasolCommand PARASOLCOMMAND\n                  \
  \      The name or path of the parasol program. Will be\n                      \
  \  looked up on PATH unless it starts with a\n                        slashdefault=parasol\n\
  \  --parasolMaxBatches PARASOLMAXBATCHES\n                        Maximum number\
  \ of job batches the Parasol batch is\n                        allowed to create.\
  \ One batch is created for jobs with\n                        a a unique set of\
  \ resource requirements. default=1000\n  --scale SCALE         A scaling factor\
  \ to change the value of all submitted\n                        tasks's submitted\
  \ cores. Used in singleMachine batch\n                        system. default=1\n\
  \  --linkImports         When using Toil's importFile function for staging,\n  \
  \                      input files are copied to the job store. Specifying\n   \
  \                     this option saves space by sym-linking imported files.\n \
  \                       As long as caching is enabled Toil will protect the\n  \
  \                      file automatically by changing the permissions to\n     \
  \                   read-only.\n  --mesosMaster MESOSMASTERADDRESS\n           \
  \             The host and port of the Mesos master separated by\n             \
  \           colon. (default: 115.146.93.132:5050)\n\ntoil options for autoscaling\
  \ the cluster of worker nodes:\n  Allows the specification of the minimum and maximum\
  \ number of nodes in an\n  autoscaled cluster, as well as parameters to control\
  \ the level of\n  provisioning.\n\n  --provisioner {aws}   The provisioner for cluster\
  \ auto-scaling. The\n                        currently supported choices are'cgcloud'\
  \ or 'aws'. The\n                        default is None.\n  --nodeTypes NODETYPES\n\
  \                        List of node types separated by commas. The syntax for\n\
  \                        each node type depends on the provisioner used. For\n \
  \                       the cgcloud and AWS provisioners this is the name of\n \
  \                       an EC2 instance type, optionally followed by a colon\n \
  \                       and the price in dollars to bid for a spot instance of\n\
  \                        that type, for example 'c3.8xlarge:0.42'.If no spot\n \
  \                       bid is specified, nodes of this type will be non-\n    \
  \                    preemptable.It is acceptable to specify an instance as\n  \
  \                      both preemptable and non-preemptable, including it\n    \
  \                    twice in the list. In that case,preemptable nodes of\n    \
  \                    that type will be preferred when creating new nodes\n     \
  \                   once the maximum number of preemptable-nodes has\n         \
  \               beenreached.\n  --nodeOptions NODEOPTIONS\n                    \
  \    Options for provisioning the nodes. The syntax depends\n                  \
  \      on the provisioner used. Neither the CGCloud nor the\n                  \
  \      AWS provisioner support any node options.\n  --minNodes MINNODES   Mininum\
  \ number of nodes of each type in the cluster,\n                        if using\
  \ auto-scaling. This should be provided as a\n                        comma-separated\
  \ list of the same length as the list of\n                        node types. default=0\n\
  \  --maxNodes MAXNODES   Maximum number of nodes of each type in the cluster,\n\
  \                        if using autoscaling, provided as a comma-separated\n \
  \                       list. The first value is used as a default if the list\n\
  \                        length is less than the number of nodeTypes.\n        \
  \                default=10\n  --alphaPacking ALPHAPACKING\n                   \
  \     The total number of nodes estimated to be required to\n                  \
  \      compute the issued jobs is multiplied by the alpha\n                    \
  \    packing parameter to produce the actual number of\n                       \
  \ nodes requested. Values of this coefficient greater\n                        than\
  \ one will tend to over provision and values less\n                        than\
  \ one will under provision. default=0.8\n  --betaInertia BETAINERTIA\n         \
  \               A smoothing parameter to prevent unnecessary\n                 \
  \       oscillations in the number of provisioned nodes. If\n                  \
  \      the number of nodes is within the beta inertia of the\n                 \
  \       currently provisioned number of nodes then no change\n                 \
  \       is made to the number of requested nodes. default=1.2\n  --scaleInterval\
  \ SCALEINTERVAL\n                        The interval (seconds) between assessing\
  \ if the scale\n                        of the cluster needs to change. default=30\n\
  \  --preemptableCompensation PREEMPTABLECOMPENSATION\n                        The\
  \ preference of the autoscaler to replace\n                        preemptable nodes\
  \ with non-preemptable nodes, when\n                        preemptable nodes cannot\
  \ be started for some reason.\n                        Defaults to 0.0. This value\
  \ must be between 0.0 and\n                        1.0, inclusive. A value of 0.0\
  \ disables such\n                        compensation, a value of 0.5 compensates\
  \ two missing\n                        preemptable nodes with a non-preemptable\
  \ one. A value\n                        of 1.0 replaces every missing pre-emptable\
  \ node with a\n                        non-preemptable one.\n  --nodeStorage NODESTORAGE\n\
  \                        Specify the size of the root volume of worker nodes\n \
  \                       when they are launched in gigabytes. You may want to\n \
  \                       set this if your jobs require a lot of disk space. The\n\
  \                        default value is 50.\n  --metrics             Enable the\
  \ prometheus/grafana dashboard for monitoring\n                        CPU/RAM usage,\
  \ queue size, and issued jobs.\n\ntoil options for limiting the number of service\
  \ jobs and detecting service deadlocks:\n  Allows the specification of the maximum\
  \ number of service jobs in a\n  cluster. By keeping this limited we can avoid all\
  \ the nodes being occupied\n  with services, so causing a deadlock\n\n  --maxServiceJobs\
  \ MAXSERVICEJOBS\n                        The maximum number of service jobs that\
  \ can be run\n                        concurrently, excluding service jobs running\
  \ on\n                        preemptable nodes. default=9223372036854775807\n \
  \ --maxPreemptableServiceJobs MAXPREEMPTABLESERVICEJOBS\n                      \
  \  The maximum number of service jobs that can run\n                        concurrently\
  \ on preemptable nodes.\n                        default=9223372036854775807\n \
  \ --deadlockWait DEADLOCKWAIT\n                        The minimum number of seconds\
  \ to observe the cluster\n                        stuck running only the same service\
  \ jobs before\n                        throwing a deadlock exception. default=60\n\
  \  --statePollingWait STATEPOLLINGWAIT\n                        Time, in seconds,\
  \ to wait before doing a scheduler\n                        query for job state.\
  \ Return cached results if within\n                        the waiting period.\n\
  \ntoil options for cores/memory requirements:\n  The options to specify default\
  \ cores/memory requirements (if not specified\n  by the jobs themselves), and to\
  \ limit the total amount of memory/cores\n  requested from the batch system.\n\n\
  \  --defaultMemory INT   The default amount of memory to request for a job.\n  \
  \                      Only applicable to jobs that do not specify an\n        \
  \                explicit value for this requirement. Standard suffixes\n      \
  \                  like K, Ki, M, Mi, G or Gi are supported. Default is\n      \
  \                  2.0 Gi\n  --defaultCores FLOAT  The default number of CPU cores\
  \ to dedicate a job.\n                        Only applicable to jobs that do not\
  \ specify an\n                        explicit value for this requirement. Fractions\
  \ of a\n                        core (for example 0.1) are supported on some batch\n\
  \                        systems, namely Mesos and singleMachine. Default is\n \
  \                       1.0\n  --defaultDisk INT     The default amount of disk\
  \ space to dedicate a job.\n                        Only applicable to jobs that\
  \ do not specify an\n                        explicit value for this requirement.\
  \ Standard suffixes\n                        like K, Ki, M, Mi, G or Gi are supported.\
  \ Default is\n                        2.0 Gi\n  --defaultPreemptable\n  --maxCores\
  \ INT        The maximum number of CPU cores to request from the\n             \
  \           batch system at any one time. Standard suffixes like\n             \
  \           K, Ki, M, Mi, G or Gi are supported. Default is 8.0 Ei\n  --maxMemory\
  \ INT       The maximum amount of memory to request from the batch\n           \
  \             system at any one time. Standard suffixes like K, Ki,\n          \
  \              M, Mi, G or Gi are supported. Default is 8.0 Ei\n  --maxDisk INT\
  \         The maximum amount of disk space to request from the\n               \
  \         batch system at any one time. Standard suffixes like\n               \
  \         K, Ki, M, Mi, G or Gi are supported. Default is 8.0 Ei\n\ntoil options\
  \ for rescuing/killing/restarting jobs:\n  The options for jobs that either run\
  \ too long/fail or get lost (some batch\n  systems have issues!)\n\n  --retryCount\
  \ RETRYCOUNT\n                        Number of times to retry a failing job before\
  \ giving\n                        up and labeling job failed. default=1\n  --maxJobDuration\
  \ MAXJOBDURATION\n                        Maximum runtime of a job (in seconds)\
  \ before we kill\n                        it (this is a lower bound, and the actual\
  \ time before\n                        killing the job may be longer).\n       \
  \                 default=9223372036854775807\n  --rescueJobsFrequency RESCUEJOBSFREQUENCY\n\
  \                        Period of time to wait (in seconds) between checking\n\
  \                        for missing/overlong jobs, that is jobs which get lost\n\
  \                        by the batch system. Expert parameter. default=3600\n\n\
  toil miscellaneous options:\n  Miscellaneous options\n\n  --disableCaching     \
  \ Disables caching in the file store. This flag must be\n                      \
  \  set to use a batch system that does not support\n                        caching\
  \ such as Grid Engine, Parasol, LSF, or Slurm\n  --maxLogFileSize MAXLOGFILESIZE\n\
  \                        The maximum size of a job log file to keep (in bytes),\n\
  \                        log files larger than this will be truncated to the\n \
  \                       last X bytes. Setting this option to zero will prevent\n\
  \                        any truncation. Setting this option to a negative\n   \
  \                     value will truncate from the beginning.Default=62.5 K\n  --writeLogs\
  \ [WRITELOGS]\n                        Write worker logs received by the leader\
  \ into their\n                        own files at the specified path. The current\
  \ working\n                        directory will be used if a path is not specified\n\
  \                        explicitly. Note: By default only the logs of failed\n\
  \                        jobs are returned to leader. Set log level to 'debug'\n\
  \                        to get logs back from successful jobs, and adjust\n   \
  \                     'maxLogFileSize' to control the truncation limit for\n   \
  \                     worker logs.\n  --writeLogsGzip [WRITELOGSGZIP]\n        \
  \                Identical to --writeLogs except the logs files are\n          \
  \              gzipped on the leader.\n  --realTimeLogging     Enable real-time\
  \ logging from workers to masters\n  --sseKey SSEKEY       Path to file containing\
  \ 32 character key to be used\n                        for server-side encryption\
  \ on awsJobStore or\n                        googleJobStore. SSE will not be used\
  \ if this flag is\n                        not passed.\n  --cseKey CSEKEY      \
  \ Path to file containing 256-bit key to be used for\n                        client-side\
  \ encryption on azureJobStore. By default,\n                        no encryption\
  \ is used.\n  --setEnv NAME=VALUE or NAME, -e NAME=VALUE or NAME\n             \
  \           Set an environment variable early on in the worker. If\n           \
  \             VALUE is omitted, it will be looked up in the current\n          \
  \              environment. Independently of this option, the worker\n         \
  \               will try to emulate the leader's environment before\n          \
  \              running a job. Using this option, a variable can be\n           \
  \             injected into the worker process itself before it is\n           \
  \             started.\n  --servicePollingInterval SERVICEPOLLINGINTERVAL\n    \
  \                    Interval of time service jobs wait between polling for\n  \
  \                      the existence of the keep-alive flag (defailt=60)\n\ntoil\
  \ debug options:\n  Debug options\n\n  --debugWorker         Experimental no forking\
  \ mode for local debugging.\n                        Specifically, workers are not\
  \ forked and stderr/stdout\n                        are not redirected to the log.\n\
  \  --badWorker BADWORKER\n                        For testing purposes randomly\
  \ kill 'badWorker'\n                        proportion of jobs using SIGKILL, default=0.0\n\
  \  --badWorkerFailInterval BADWORKERFAILINTERVAL\n                        When killing\
  \ the job pick uniformly within the\n                        interval from 0.0 to\
  \ 'badWorkerFailInterval' seconds\n                        after the worker starts,\
  \ default=0.01\n"
generated_using:
- --help
