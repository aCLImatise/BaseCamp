!Command
positional: []
named:
- !Flag
  description: preload <file>, enforcing line-by-line interpretation
  synonyms:
  - -I
  args: !SimpleFlagArg
    name: file
  optional: true
- !Flag
  description: 'spark://host:port, mesos://host:port, yarn, k8s://https://host:port,
    or local (Default: local[*]).'
  synonyms:
  - --master
  args: !SimpleFlagArg
    name: MASTER_URL
  optional: true
- !Flag
  description: 'Whether to launch the driver program locally ("client") or on one
    of the worker machines inside the cluster ("cluster") (Default: client).'
  synonyms:
  - --deploy-mode
  args: !SimpleFlagArg
    name: DEPLOY_MODE
  optional: true
- !Flag
  description: Your application's main class (for Java / Scala apps).
  synonyms:
  - --class
  args: !SimpleFlagArg
    name: CLASS_NAME
  optional: true
- !Flag
  description: A name of your application.
  synonyms:
  - --name
  args: !SimpleFlagArg
    name: NAME
  optional: true
- !Flag
  description: Comma-separated list of jars to include on the driver and executor
    classpaths.
  synonyms:
  - --jars
  args: !SimpleFlagArg
    name: JARS
  optional: true
- !Flag
  description: Comma-separated list of maven coordinates of jars to include on the
    driver and executor classpaths. Will search the local maven repo, then maven central
    and any additional remote repositories given by --repositories. The format for
    the coordinates should be groupId:artifactId:version.
  synonyms:
  - --packages
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: Comma-separated list of groupId:artifactId, to exclude while resolving
    the dependencies provided in --packages to avoid dependency conflicts.
  synonyms:
  - --exclude-packages
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: Comma-separated list of additional remote repositories to search for
    the maven coordinates given with --packages.
  synonyms:
  - --repositories
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH
    for Python apps.
  synonyms:
  - --py-files
  args: !SimpleFlagArg
    name: PY_FILES
  optional: true
- !Flag
  description: Comma-separated list of files to be placed in the working directory
    of each executor. File paths of these files in executors can be accessed via SparkFiles.get(fileName).
  synonyms:
  - --files
  args: !SimpleFlagArg
    name: FILES
  optional: true
- !Flag
  description: =VALUE       Arbitrary Spark configuration property.
  synonyms:
  - --conf
  - -c
  args: !SimpleFlagArg
    name: PROP
  optional: true
- !Flag
  description: Path to a file from which to load extra properties. If not specified,
    this will look for conf/spark-defaults.conf.
  synonyms:
  - --properties-file
  args: !SimpleFlagArg
    name: FILE
  optional: true
- !Flag
  description: 'Memory for driver (e.g. 1000M, 2G) (Default: 1024M).'
  synonyms:
  - --driver-memory
  args: !SimpleFlagArg
    name: MEM
  optional: true
- !Flag
  description: Extra Java options to pass to the driver.
  synonyms:
  - --driver-java-options
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: Extra library path entries to pass to the driver.
  synonyms:
  - --driver-library-path
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: Extra class path entries to pass to the driver. Note that jars added
    with --jars are automatically included in the classpath.
  synonyms:
  - --driver-class-path
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: 'Memory per executor (e.g. 1000M, 2G) (Default: 1G).'
  synonyms:
  - --executor-memory
  args: !SimpleFlagArg
    name: MEM
  optional: true
- !Flag
  description: User to impersonate when submitting the application. This argument
    does not work with --principal / --keytab.
  synonyms:
  - --proxy-user
  args: !SimpleFlagArg
    name: NAME
  optional: true
- !Flag
  description: Print additional debug output.
  synonyms:
  - --verbose
  - -v
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: 'Number of cores used by the driver, only in cluster mode (Default:
    1).'
  synonyms:
  - --driver-cores
  args: !SimpleFlagArg
    name: NUM
  optional: true
- !Flag
  description: If given, restarts the driver on failure.
  synonyms:
  - --supervise
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: If given, kills the driver specified.
  synonyms:
  - --kill
  args: !SimpleFlagArg
    name: SUBMISSION_ID
  optional: true
- !Flag
  description: If given, requests the status of the driver specified.
  synonyms:
  - --status
  args: !SimpleFlagArg
    name: SUBMISSION_ID
  optional: true
- !Flag
  description: Total cores for all executors.
  synonyms:
  - --total-executor-cores
  args: !SimpleFlagArg
    name: NUM
  optional: true
- !Flag
  description: 'Number of cores used by each executor. (Default: 1 in YARN and K8S
    modes, or all available cores on the worker in standalone mode).'
  synonyms:
  - --executor-cores
  args: !SimpleFlagArg
    name: NUM
  optional: true
- !Flag
  description: 'Number of executors to launch (Default: 2). If dynamic allocation
    is enabled, the initial number of executors will be at least NUM.'
  synonyms:
  - --num-executors
  args: !SimpleFlagArg
    name: NUM
  optional: true
- !Flag
  description: Principal to be used to login to KDC.
  synonyms:
  - --principal
  args: !SimpleFlagArg
    name: PRINCIPAL
  optional: true
- !Flag
  description: The full path to the file that contains the keytab for the principal
    specified above.
  synonyms:
  - --keytab
  args: !SimpleFlagArg
    name: KEYTAB
  optional: true
- !Flag
  description: 'The YARN queue to submit to (Default: "default").'
  synonyms:
  - --queue
  args: !SimpleFlagArg
    name: QUEUE_NAME
  optional: true
- !Flag
  description: Comma separated list of archives to be extracted into the working directory
    of each executor.
  synonyms:
  - --archives
  args: !SimpleFlagArg
    name: ARCHIVES
  optional: true
command:
- adam-shell
parent:
subcommands: []
help_flag: !Flag
  description: Show this help message and exit.
  synonyms:
  - --help
  - -h
  args: !EmptyFlagArg {}
  optional: true
usage_flag:
version_flag: !Flag
  description: ',                  Print the version of current Spark.'
  synonyms:
  - --version
  args: !EmptyFlagArg {}
  optional: true
help_text: "Using SPARK_SHELL=/tmp/tmp_7qcra1x/bin/spark-shell\nUsage: ./bin/spark-shell\
  \ [options]\n\nScala REPL options:\n  -I <file>                   preload <file>,\
  \ enforcing line-by-line interpretation\n\nOptions:\n  --master MASTER_URL     \
  \    spark://host:port, mesos://host:port, yarn,\n                             \
  \ k8s://https://host:port, or local (Default: local[*]).\n  --deploy-mode DEPLOY_MODE\
  \   Whether to launch the driver program locally (\"client\") or\n             \
  \                 on one of the worker machines inside the cluster (\"cluster\"\
  )\n                              (Default: client).\n  --class CLASS_NAME      \
  \    Your application's main class (for Java / Scala apps).\n  --name NAME     \
  \            A name of your application.\n  --jars JARS                 Comma-separated\
  \ list of jars to include on the driver\n                              and executor\
  \ classpaths.\n  --packages                  Comma-separated list of maven coordinates\
  \ of jars to include\n                              on the driver and executor classpaths.\
  \ Will search the local\n                              maven repo, then maven central\
  \ and any additional remote\n                              repositories given by\
  \ --repositories. The format for the\n                              coordinates\
  \ should be groupId:artifactId:version.\n  --exclude-packages          Comma-separated\
  \ list of groupId:artifactId, to exclude while\n                              resolving\
  \ the dependencies provided in --packages to avoid\n                           \
  \   dependency conflicts.\n  --repositories              Comma-separated list of\
  \ additional remote repositories to\n                              search for the\
  \ maven coordinates given with --packages.\n  --py-files PY_FILES         Comma-separated\
  \ list of .zip, .egg, or .py files to place\n                              on the\
  \ PYTHONPATH for Python apps.\n  --files FILES               Comma-separated list\
  \ of files to be placed in the working\n                              directory\
  \ of each executor. File paths of these files\n                              in\
  \ executors can be accessed via SparkFiles.get(fileName).\n\n  --conf, -c PROP=VALUE\
  \       Arbitrary Spark configuration property.\n  --properties-file FILE      Path\
  \ to a file from which to load extra properties. If not\n                      \
  \        specified, this will look for conf/spark-defaults.conf.\n\n  --driver-memory\
  \ MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n  --driver-java-options\
  \       Extra Java options to pass to the driver.\n  --driver-library-path     \
  \  Extra library path entries to pass to the driver.\n  --driver-class-path    \
  \     Extra class path entries to pass to the driver. Note that\n              \
  \                jars added with --jars are automatically included in the\n    \
  \                          classpath.\n\n  --executor-memory MEM       Memory per\
  \ executor (e.g. 1000M, 2G) (Default: 1G).\n\n  --proxy-user NAME           User\
  \ to impersonate when submitting the application.\n                            \
  \  This argument does not work with --principal / --keytab.\n\n  --help, -h    \
  \              Show this help message and exit.\n  --verbose, -v               Print\
  \ additional debug output.\n  --version,                  Print the version of current\
  \ Spark.\n\n Cluster deploy mode only:\n  --driver-cores NUM          Number of\
  \ cores used by the driver, only in cluster mode\n                             \
  \ (Default: 1).\n\n Spark standalone or Mesos with cluster deploy mode only:\n \
  \ --supervise                 If given, restarts the driver on failure.\n\n Spark\
  \ standalone, Mesos or K8s with cluster deploy mode only:\n  --kill SUBMISSION_ID\
  \        If given, kills the driver specified.\n  --status SUBMISSION_ID      If\
  \ given, requests the status of the driver specified.\n\n Spark standalone, Mesos\
  \ and Kubernetes only:\n  --total-executor-cores NUM  Total cores for all executors.\n\
  \n Spark standalone, YARN and Kubernetes only:\n  --executor-cores NUM        Number\
  \ of cores used by each executor. (Default: 1 in\n                             \
  \ YARN and K8S modes, or all available cores on the worker\n                   \
  \           in standalone mode).\n\n Spark on YARN and Kubernetes only:\n  --num-executors\
  \ NUM         Number of executors to launch (Default: 2).\n                    \
  \          If dynamic allocation is enabled, the initial number of\n           \
  \                   executors will be at least NUM.\n  --principal PRINCIPAL   \
  \    Principal to be used to login to KDC.\n  --keytab KEYTAB             The full\
  \ path to the file that contains the keytab for the\n                          \
  \    principal specified above.\n\n Spark on YARN only:\n  --queue QUEUE_NAME  \
  \        The YARN queue to submit to (Default: \"default\").\n  --archives ARCHIVES\
  \         Comma separated list of archives to be extracted into the\n          \
  \                    working directory of each executor.\n      \n"
generated_using:
- --help
