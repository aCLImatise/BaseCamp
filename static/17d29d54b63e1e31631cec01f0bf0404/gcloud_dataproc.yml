!Command
command:
- gcloud
- dataproc
positional:
- !Positional
  optional: false
  position: 0
  name: clusters
  description: Create and manage Google Cloud Dataproc clusters.
- !Positional
  optional: false
  position: 1
  name: jobs
  description: Submit and manage Google Cloud Dataproc jobs.
- !Positional
  optional: false
  position: 2
  name: operations
  description: View and manage Google Cloud Dataproc operations.
named:
- !Flag
  optional: true
  synonyms:
  - --region
  description: "Specifies the Cloud Dataproc region to use. Each Cloud Dataproc region\n\
    constitutes an independent resource namespace constrained to deploying\ninstances\
    \ into Google Compute Engine zones inside the region. The\ndefault value of \"\
    global\" is a special multi-region namespace which is\ncapable of deploying instances\
    \ into all Google Compute Engine zones\nglobally, and is disjoint from other Cloud\
    \ Dataproc regions. Overrides\nthe default dataproc/region property value for\
    \ this command invocation."
  args: !SimpleFlagArg
    name: REGION
parent:
subcommands: []
usage: []
help_flag: !Flag
  optional: true
  synonyms:
  - --flatten
  - --format
  - --help
  - --log-http
  - --project
  - --quiet
  - --trace-token
  - --user-output-enabled
  - --verbosity.
  description: $ gcloud help for details.
  args: !SimpleFlagArg
    name: Run
usage_flag:
version_flag:
help_text: "NAME\n    gcloud dataproc - create and manage Google Cloud Dataproc clusters\
  \ and jobs\n\nSYNOPSIS\n    gcloud dataproc GROUP [--region=REGION] [GCLOUD_WIDE_FLAG\
  \ ...]\n\nDESCRIPTION\n    The gcloud dataproc command group lets you create and\
  \ manage Google Cloud\n    Dataproc clusters and jobs.\n\n    Cloud Dataproc is\
  \ an Apache Hadoop, Apache Spark, Apache Pig, and Apache\n    Hive service. It easily\
  \ processes big datasets at low cost, creating\n    managed clusters of any size\
  \ that scale down once processing is complete.\n\n    More information on Cloud\
  \ Dataproc can be found here:\n    https://cloud.google.com/dataproc and detailed\
  \ documentation can be found\n    here: https://cloud.google.com/dataproc/docs/\n\
  \nEXAMPLES\n    To see how to create and manage clusters, run:\n\n        $ gcloud\
  \ dataproc clusters\n\n    To see how to submit and manage jobs, run:\n\n      \
  \  $ gcloud dataproc jobs\n\nFLAGS\n     --region=REGION\n        Specifies the\
  \ Cloud Dataproc region to use. Each Cloud Dataproc region\n        constitutes\
  \ an independent resource namespace constrained to deploying\n        instances\
  \ into Google Compute Engine zones inside the region. The\n        default value\
  \ of \"global\" is a special multi-region namespace which is\n        capable of\
  \ deploying instances into all Google Compute Engine zones\n        globally, and\
  \ is disjoint from other Cloud Dataproc regions. Overrides\n        the default\
  \ dataproc/region property value for this command invocation.\n\nGCLOUD WIDE FLAGS\n\
  \    These flags are available to all commands: --account, --configuration,\n  \
  \  --flatten, --format, --help, --log-http, --project, --quiet, --trace-token,\n\
  \    --user-output-enabled, --verbosity. Run $ gcloud help for details.\n\nGROUPS\n\
  \    GROUP is one of the following:\n\n     clusters\n        Create and manage\
  \ Google Cloud Dataproc clusters.\n\n     jobs\n        Submit and manage Google\
  \ Cloud Dataproc jobs.\n\n     operations\n        View and manage Google Cloud\
  \ Dataproc operations.\n"
generated_using:
- --help
docker_image:
