!Command
command:
- caper
- run
positional:
- !Positional
  optional: false
  position: 0
  name: server.
  description: --singularity-cachedir SINGULARITY_CACHEDIR
named:
- !Flag
  optional: true
  synonyms:
  - -c
  - --conf
  description: Specify config file
  args: !SimpleFlagArg
    name: CONF
- !Flag
  optional: true
  synonyms:
  - -D
  - --debug
  description: Prints all logs >= DEBUG level
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --gcp-service-account-key-json
  description: "Secret key JSON file for Google Cloud Platform service\naccount. This\
    \ service account should have enough\npermission to Storage for client functions\
    \ and\nStorage/Compute Engine/Genomics API/Life Sciences API\nfor server/runner\
    \ functions."
  args: !SimpleFlagArg
    name: GCP_SERVICE_ACCOUNT_KEY_JSON
- !Flag
  optional: true
  synonyms:
  - -i
  - --inputs
  description: Workflow inputs JSON file
  args: !SimpleFlagArg
    name: INPUTS
- !Flag
  optional: true
  synonyms:
  - -o
  - --options
  description: Workflow options JSON file
  args: !SimpleFlagArg
    name: OPTIONS
- !Flag
  optional: true
  synonyms:
  - -l
  - --labels
  description: Workflow labels JSON file
  args: !SimpleFlagArg
    name: LABELS
- !Flag
  optional: true
  synonyms:
  - -p
  - --imports
  description: Zip file of imported subworkflows
  args: !SimpleFlagArg
    name: IMPORTS
- !Flag
  optional: true
  synonyms:
  - -s
  - --str-label
  description: "Caper's special label for a workflow This label will\nbe added to\
    \ a workflow labels JSON file as a value for\na key \"caper-label\". DO NOT USE\
    \ IRREGULAR CHARACTERS.\nUSE LETTERS, NUMBERS, DASHES AND UNDERSCORES ONLY\n(^[A-Za-z0-9\\\
    -\\_]+$) since this label is used to\ncompose a path for workflow's temporary/cache\n\
    directory (.caper_tmp/label/timestamp/)"
  args: !SimpleFlagArg
    name: STR_LABEL
- !Flag
  optional: true
  synonyms:
  - --hold
  description: Put a hold on a workflow when submitted to a Cromwell
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --use-gsutil-for-s3
  description: "Use gsutil CLI for direct trasnfer between S3 and GCS\nbuckets. Otherwise,\
    \ such file transfer will stream\nthrough local machine. Make sure that gsutil\
    \ is\ninstalled on your system and it has access to\ncredentials for AWS (e.g.\
    \ ~/.boto or\n~/.aws/credentials)."
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --no-deepcopy
  description: "(IMPORTANT) --deepcopy has been deprecated.\nDeepcopying is now activated\
    \ by default. This flag\ndisables deepcopy for JSON (.json), TSV (.tsv) and CSV\n\
    (.csv) files specified in an input JSON file\n(--inputs). Find all path/URI string\
    \ values in an\ninput JSON file and make copies of files on a\nlocal/remote storage\
    \ for a target backend. Make sure\nthat you have installed gsutil for GCS and\
    \ aws for S3."
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --ignore-womtool
  description: Ignore warnings from womtool.jar.
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --womtool
  description: Path or URL for Cromwell's womtool JAR file
  args: !SimpleFlagArg
    name: WOMTOOL
- !Flag
  optional: true
  synonyms:
  - --java-heap-womtool
  description: Java heap size for Womtool (java -Xmx)
  args: !SimpleFlagArg
    name: JAVA_HEAP_WOMTOOL
- !Flag
  optional: true
  synonyms:
  - --max-retries
  description: "Number of retries for failing tasks. equivalent to\n\"maxRetries\"\
    \ in workflow options JSON file."
  args: !SimpleFlagArg
    name: MAX_RETRIES
- !Flag
  optional: true
  synonyms:
  - -m
  - --metadata-output
  description: "An optional directory path to output metadata JSON\nfile"
  args: !SimpleFlagArg
    name: METADATA_OUTPUT
- !Flag
  optional: true
  synonyms:
  - --java-heap-run
  description: Cromwell Java heap size for "run" mode (java -Xmx)
  args: !SimpleFlagArg
    name: JAVA_HEAP_RUN
- !Flag
  optional: true
  synonyms:
  - --cromwell-stdout
  description: "Local file to write STDOUT of Cromwell Java process\nto. This is for\
    \ Cromwell (not for Caper's logging\nsystem). Note that STDERR is redirected to\
    \ STDOUT."
  args: !SimpleFlagArg
    name: CROMWELL_STDOUT
- !Flag
  optional: true
  synonyms:
  - -b
  - --backend
  description: Backend to run a workflow
  args: !SimpleFlagArg
    name: BACKEND
- !Flag
  optional: true
  synonyms:
  - --dry-run
  description: "Caper localizes remote files and validates WDL but\ndoes not run/submit\
    \ a pipeline."
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --local-loc-dir
  - --tmp-dir
  description: "Temporary directory to store Cromwell's intermediate\nbackend files.\
    \ These files include backend.conf,\nworkflow_opts.json, imports.zip. and localized\
    \ input\nJSON files due to deepcopying (recursive\nlocalization). Cromwell's MySQL/PostgreSQL\
    \ DB password\ncan be exposed on backend.conf on this directory.\nTherefore, DO\
    \ NOT USE /tmp HERE. This directory is\nalso used for storing cached files for\n\
    local/slurm/sge/pbs backends."
  args: !SimpleFlagArg
    name: LOCAL_LOC_DIR
- !Flag
  optional: true
  synonyms:
  - --gcp-loc-dir
  - --tmp-gcs-bucket
  description: "Temporary directory to store cached files for gcp\nbackend. e.g. gs://my-bucket/caper-cache-dir."
  args: !SimpleFlagArg
    name: GCP_LOC_DIR
- !Flag
  optional: true
  synonyms:
  - --aws-loc-dir
  - --tmp-s3-bucket
  description: "Temporary directory to store cached files for aws\nbackend. e.g. s3://my-bucket/caper-cache-dir."
  args: !SimpleFlagArg
    name: AWS_LOC_DIR
- !Flag
  optional: true
  synonyms:
  - --docker
  description: "[DOCKER]     URI for Docker image (e.g. ubuntu:latest). This can\n\
    also be used as a flag to use Docker image address\ndefined in your WDL file as\
    \ a comment (\"#CAPER\ndocker\") or as \"workflow.meta.caper_docker\" in WDL."
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --singularity
  description: "[SINGULARITY]\nURI or path for Singularity image (e.g.\n~/.singularity/ubuntu-latest.simg,\n\
    docker://ubuntu:latest, shub://vsoch/hello-world).\nThis can also be used as a\
    \ flag to use Docker image\naddress defined in your WDL file as a comment (\"\
    #CAPER\nsingularity\") or as \"workflow.meta.caper_singularity\"\nin WDL."
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --no-build-singularity
  description: "Do not build singularity image before running a\nworkflow."
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --slurm-partition
  description: SLURM partition
  args: !SimpleFlagArg
    name: SLURM_PARTITION
- !Flag
  optional: true
  synonyms:
  - --slurm-account
  description: SLURM account
  args: !SimpleFlagArg
    name: SLURM_ACCOUNT
- !Flag
  optional: true
  synonyms:
  - --slurm-extra-param
  description: SLURM extra parameters. Must be double-quoted
  args: !SimpleFlagArg
    name: SLURM_EXTRA_PARAM
- !Flag
  optional: true
  synonyms:
  - --sge-pe
  description: SGE parallel environment. Check with "qconf -spl"
  args: !SimpleFlagArg
    name: SGE_PE
- !Flag
  optional: true
  synonyms:
  - --sge-queue
  description: SGE queue. Check with "qconf -sql"
  args: !SimpleFlagArg
    name: SGE_QUEUE
- !Flag
  optional: true
  synonyms:
  - --sge-extra-param
  description: SGE extra parameters. Must be double-quoted
  args: !SimpleFlagArg
    name: SGE_EXTRA_PARAM
- !Flag
  optional: true
  synonyms:
  - --pbs-queue
  description: PBS queue
  args: !SimpleFlagArg
    name: PBS_QUEUE
- !Flag
  optional: true
  synonyms:
  - --pbs-extra-param
  description: PBS extra parameters. Must be double-quoted
  args: !SimpleFlagArg
    name: PBS_EXTRA_PARAM
- !Flag
  optional: true
  synonyms:
  - --db
  description: Cromwell metadata database type
  args: !SimpleFlagArg
    name: DB
- !Flag
  optional: true
  synonyms:
  - --db-timeout
  description: Milliseconds to wait for DB connection.
  args: !SimpleFlagArg
    name: DB_TIMEOUT
- !Flag
  optional: true
  synonyms:
  - --file-db
  - -d
  description: "Default DB file for Cromwell's built-in HyperSQL\ndatabase."
  args: !SimpleFlagArg
    name: FILE_DB
- !Flag
  optional: true
  synonyms:
  - --mysql-db-ip
  description: MySQL Database IP address (e.g. localhost)
  args: !SimpleFlagArg
    name: MYSQL_DB_IP
- !Flag
  optional: true
  synonyms:
  - --mysql-db-port
  description: MySQL Database TCP/IP port (e.g. 3306)
  args: !SimpleFlagArg
    name: MYSQL_DB_PORT
- !Flag
  optional: true
  synonyms:
  - --mysql-db-user
  description: MySQL DB username
  args: !SimpleFlagArg
    name: MYSQL_DB_USER
- !Flag
  optional: true
  synonyms:
  - --mysql-db-password
  description: MySQL DB password
  args: !SimpleFlagArg
    name: MYSQL_DB_PASSWORD
- !Flag
  optional: true
  synonyms:
  - --mysql-db-name
  description: MySQL DB name for Cromwell
  args: !SimpleFlagArg
    name: MYSQL_DB_NAME
- !Flag
  optional: true
  synonyms:
  - --postgresql-db-ip
  description: PostgreSQL DB IP address (e.g. localhost)
  args: !SimpleFlagArg
    name: POSTGRESQL_DB_IP
- !Flag
  optional: true
  synonyms:
  - --postgresql-db-port
  description: PostgreSQL DB TCP/IP port (e.g. 5432)
  args: !SimpleFlagArg
    name: POSTGRESQL_DB_PORT
- !Flag
  optional: true
  synonyms:
  - --postgresql-db-user
  description: PostgreSQL DB username
  args: !SimpleFlagArg
    name: POSTGRESQL_DB_USER
- !Flag
  optional: true
  synonyms:
  - --postgresql-db-password
  description: PostgreSQL DB password
  args: !SimpleFlagArg
    name: POSTGRESQL_DB_PASSWORD
- !Flag
  optional: true
  synonyms:
  - --postgresql-db-name
  description: PostgreSQL DB name for Cromwell
  args: !SimpleFlagArg
    name: POSTGRESQL_DB_NAME
- !Flag
  optional: true
  synonyms:
  - --cromwell
  description: Path or URL for Cromwell JAR file
  args: !SimpleFlagArg
    name: CROMWELL
- !Flag
  optional: true
  synonyms:
  - --max-concurrent-tasks
  description: "Number of concurrent tasks. \"config.concurrent-job-\nlimit\" in Cromwell\
    \ backend configuration for each\nbackend"
  args: !SimpleFlagArg
    name: MAX_CONCURRENT_TASKS
- !Flag
  optional: true
  synonyms:
  - --max-concurrent-workflows
  description: "Number of concurrent workflows. \"system.max-\nconcurrent-workflows\"\
    \ in backend configuration"
  args: !SimpleFlagArg
    name: MAX_CONCURRENT_WORKFLOWS
- !Flag
  optional: true
  synonyms:
  - --disable-call-caching
  description: "Disable Cromwell's call caching, which re-uses outputs\nfrom previous\
    \ workflows"
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --backend-file
  description: "Custom Cromwell backend configuration file to override\nall"
  args: !SimpleFlagArg
    name: BACKEND_FILE
- !Flag
  optional: true
  synonyms:
  - --soft-glob-output
  description: "Use soft-linking when globbing outputs for a\nfilesystem that does\
    \ not allow hard-linking. e.g.\nbeeGFS. This flag does not work with backends\
    \ based on\na Docker container. i.e. gcp and aws. Also, it does\nnot work with\
    \ local backends (local/slurm/sge/pbs)\nwith --. However, it works fine with --singularity."
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --local-hash-strat
  description: "{file,path,path+modtime}\nFile hashing strategy for call caching.\
    \ For local\nbackends (local/slurm/sge/pbs) only. file: use md5sum\nhash (slow),\
    \ path: use path only, path+modtime\n(default): use path + mtime."
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --local-out-dir
  - --out-dir
  description: "Output directory path for local backend. Cloud\nbackends (gcp, aws)\
    \ use different output directories.\nFor gcp, define --gcp-out-dir. For aws, define\
    \ --aws-\nout-dir."
  args: !SimpleFlagArg
    name: LOCAL_OUT_DIR
- !Flag
  optional: true
  synonyms:
  - --gcp-prj
  description: GC project
  args: !SimpleFlagArg
    name: GCP_PRJ
- !Flag
  optional: true
  synonyms:
  - --gcp-memory-retry-error-keys
  description: "If an error caught by these comma-separated keys\noccurs, then increase\
    \ memory by --gcp-memory-retry-\nmultiplier for retrials controlled by --max-retries.\n\
    See https://cromwell.readthedocs.io/en/stable/backends\n/Google/ for details."
  args: !SimpleFlagArg
    name: GCP_MEMORY_RETRY_ERROR_KEYS
- !Flag
  optional: true
  synonyms:
  - --gcp-memory-retry-multiplier
  description: "If an error caught by --gcp-memory-retry-error-keys\noccurs, then\
    \ increase memory by this for retrials\ncontrolled by --max-retries. See https://cromwell.read\n\
    thedocs.io/en/stable/backends/Google/ for details."
  args: !SimpleFlagArg
    name: GCP_MEMORY_RETRY_MULTIPLIER
- !Flag
  optional: true
  synonyms:
  - --gcp-region
  description: "GCP region for Google Cloud Life Sciences API. This is\nused only\
    \ when --use-google-cloud-life-sciences is\ndefined."
  args: !SimpleFlagArg
    name: GCP_REGION
- !Flag
  optional: true
  synonyms:
  - --gcp-call-caching-dup-strat
  description: "Duplication strategy for call-cached outputs for GCP\nbackend: copy:\
    \ make a copy, reference: refer to old\noutput in metadata.json."
  args: !ChoiceFlagArg
    choices: !!set
      reference:
- !Flag
  optional: true
  synonyms:
  - --gcp-out-dir
  - --out-gcs-bucket
  description: "Output directory path for GCP backend. e.g. gs://my-\nbucket/my-output."
  args: !SimpleFlagArg
    name: GCP_OUT_DIR
- !Flag
  optional: true
  synonyms:
  - --aws-batch-arn
  description: ARN for AWS Batch
  args: !SimpleFlagArg
    name: AWS_BATCH_ARN
- !Flag
  optional: true
  synonyms:
  - --aws-region
  description: AWS region (e.g. us-west-1)
  args: !SimpleFlagArg
    name: AWS_REGION
- !Flag
  optional: true
  synonyms:
  - --aws-out-dir
  - --out-s3-bucket
  description: "Output path on S3 bucket for AWS backend. e.g.\ns3://my-bucket/my-output."
  args: !SimpleFlagArg
    name: AWS_OUT_DIR
- !Flag
  optional: true
  synonyms:
  - --use-google-cloud-life-sciences
  description: "Use Google Cloud Life Sciences API (v2beta) instead of\ndeprecated\
    \ Genomics API (v2alpha1).Life Sciences API\nrequires only one region specified\
    \ withgcp-region.\ngcp-zones will be ignored since it is for Genomics\nAPI.See\
    \ https://cloud.google.com/life-\nsciences/docs/concepts/locations for supported\n\
    regions."
  args: !EmptyFlagArg {}
- !Flag
  optional: true
  synonyms:
  - --gcp-zones
  description: "Comma-separated GCP zones used for Genomics API. (e.g.\nus-west1-b,us-central1-b).\
    \ If you use --use-google-\ncloud-life-sciences then define --gcp-region instead.\n"
  args: !SimpleFlagArg
    name: GCP_ZONES
parent:
subcommands: []
usage: []
help_flag: !Flag
  optional: true
  synonyms:
  - -h
  - --help
  description: show this help message and exit
  args: !EmptyFlagArg {}
usage_flag:
version_flag:
help_text: "usage: caper run [-h] [-c CONF] [-D]\n                 [--gcp-service-account-key-json\
  \ GCP_SERVICE_ACCOUNT_KEY_JSON]\n                 [--local-loc-dir LOCAL_LOC_DIR]\
  \ [--gcp-loc-dir GCP_LOC_DIR]\n                 [--aws-loc-dir AWS_LOC_DIR] [-i\
  \ INPUTS] [-o OPTIONS]\n                 [-l LABELS] [-p IMPORTS] [-s STR_LABEL]\
  \ [--hold]\n                 [--singularity-cachedir SINGULARITY_CACHEDIR]\n   \
  \              [--use-gsutil-for-s3] [--no-deepcopy] [--ignore-womtool]\n      \
  \           [--womtool WOMTOOL] [--java-heap-womtool JAVA_HEAP_WOMTOOL]\n      \
  \           [--max-retries MAX_RETRIES] [--docker [DOCKER]]\n                 [--singularity\
  \ [SINGULARITY]] [--no-build-singularity]\n                 [--slurm-partition SLURM_PARTITION]\n\
  \                 [--slurm-account SLURM_ACCOUNT]\n                 [--slurm-extra-param\
  \ SLURM_EXTRA_PARAM] [--sge-pe SGE_PE]\n                 [--sge-queue SGE_QUEUE]\
  \ [--sge-extra-param SGE_EXTRA_PARAM]\n                 [--pbs-queue PBS_QUEUE]\
  \ [--pbs-extra-param PBS_EXTRA_PARAM]\n                 [-m METADATA_OUTPUT] [--java-heap-run\
  \ JAVA_HEAP_RUN]\n                 [--cromwell-stdout CROMWELL_STDOUT] [--db DB]\n\
  \                 [--db-timeout DB_TIMEOUT] [--file-db FILE_DB]\n              \
  \   [--mysql-db-ip MYSQL_DB_IP] [--mysql-db-port MYSQL_DB_PORT]\n              \
  \   [--mysql-db-user MYSQL_DB_USER]\n                 [--mysql-db-password MYSQL_DB_PASSWORD]\n\
  \                 [--mysql-db-name MYSQL_DB_NAME]\n                 [--postgresql-db-ip\
  \ POSTGRESQL_DB_IP]\n                 [--postgresql-db-port POSTGRESQL_DB_PORT]\n\
  \                 [--postgresql-db-user POSTGRESQL_DB_USER]\n                 [--postgresql-db-password\
  \ POSTGRESQL_DB_PASSWORD]\n                 [--postgresql-db-name POSTGRESQL_DB_NAME]\n\
  \                 [--cromwell CROMWELL]\n                 [--max-concurrent-tasks\
  \ MAX_CONCURRENT_TASKS]\n                 [--max-concurrent-workflows MAX_CONCURRENT_WORKFLOWS]\n\
  \                 [--disable-call-caching] [--backend-file BACKEND_FILE]\n     \
  \            [--soft-glob-output]\n                 [--local-hash-strat {file,path,path+modtime}]\n\
  \                 [--local-out-dir LOCAL_OUT_DIR] [--gcp-prj GCP_PRJ]\n        \
  \         [--gcp-memory-retry-error-keys GCP_MEMORY_RETRY_ERROR_KEYS]\n        \
  \         [--gcp-memory-retry-multiplier GCP_MEMORY_RETRY_MULTIPLIER]\n        \
  \         [--gcp-region GCP_REGION]\n                 [--gcp-call-caching-dup-strat\
  \ {reference,reference}]\n                 [--gcp-out-dir GCP_OUT_DIR] [--aws-batch-arn\
  \ AWS_BATCH_ARN]\n                 [--aws-region AWS_REGION] [--aws-out-dir AWS_OUT_DIR]\n\
  \                 [-b BACKEND] [--dry-run] [--use-google-cloud-life-sciences]\n\
  \                 [--gcp-zones GCP_ZONES]\n                 wdl\n\npositional arguments:\n\
  \  wdl                   Path, URL or URI for WDL script Example:\n            \
  \            /scratch/my.wdl, gs://some/where/our.wdl,\n                       \
  \ http://hello.com/world/your.wdl\n\noptional arguments:\n  -h, --help         \
  \   show this help message and exit\n  -c CONF, --conf CONF  Specify config file\n\
  \  -D, --debug           Prints all logs >= DEBUG level\n  --gcp-service-account-key-json\
  \ GCP_SERVICE_ACCOUNT_KEY_JSON\n                        Secret key JSON file for\
  \ Google Cloud Platform service\n                        account. This service account\
  \ should have enough\n                        permission to Storage for client functions\
  \ and\n                        Storage/Compute Engine/Genomics API/Life Sciences\
  \ API\n                        for server/runner functions.\n  -i INPUTS, --inputs\
  \ INPUTS\n                        Workflow inputs JSON file\n  -o OPTIONS, --options\
  \ OPTIONS\n                        Workflow options JSON file\n  -l LABELS, --labels\
  \ LABELS\n                        Workflow labels JSON file\n  -p IMPORTS, --imports\
  \ IMPORTS\n                        Zip file of imported subworkflows\n  -s STR_LABEL,\
  \ --str-label STR_LABEL\n                        Caper's special label for a workflow\
  \ This label will\n                        be added to a workflow labels JSON file\
  \ as a value for\n                        a key \"caper-label\". DO NOT USE IRREGULAR\
  \ CHARACTERS.\n                        USE LETTERS, NUMBERS, DASHES AND UNDERSCORES\
  \ ONLY\n                        (^[A-Za-z0-9\\-\\_]+$) since this label is used\
  \ to\n                        compose a path for workflow's temporary/cache\n  \
  \                      directory (.caper_tmp/label/timestamp/)\n  --hold       \
  \         Put a hold on a workflow when submitted to a Cromwell\n              \
  \          server.\n  --singularity-cachedir SINGULARITY_CACHEDIR\n            \
  \            Singularity cache directory. Equivalent to exporting\n            \
  \            an environment variable SINGULARITY_CACHEDIR. Define\n            \
  \            it to prevent repeatedly building a singularity image\n           \
  \             for every pipeline task\n  --use-gsutil-for-s3   Use gsutil CLI for\
  \ direct trasnfer between S3 and GCS\n                        buckets. Otherwise,\
  \ such file transfer will stream\n                        through local machine.\
  \ Make sure that gsutil is\n                        installed on your system and\
  \ it has access to\n                        credentials for AWS (e.g. ~/.boto or\n\
  \                        ~/.aws/credentials).\n  --no-deepcopy         (IMPORTANT)\
  \ --deepcopy has been deprecated.\n                        Deepcopying is now activated\
  \ by default. This flag\n                        disables deepcopy for JSON (.json),\
  \ TSV (.tsv) and CSV\n                        (.csv) files specified in an input\
  \ JSON file\n                        (--inputs). Find all path/URI string values\
  \ in an\n                        input JSON file and make copies of files on a\n\
  \                        local/remote storage for a target backend. Make sure\n\
  \                        that you have installed gsutil for GCS and aws for S3.\n\
  \  --ignore-womtool      Ignore warnings from womtool.jar.\n  --womtool WOMTOOL\
  \     Path or URL for Cromwell's womtool JAR file\n  --java-heap-womtool JAVA_HEAP_WOMTOOL\n\
  \                        Java heap size for Womtool (java -Xmx)\n  --max-retries\
  \ MAX_RETRIES\n                        Number of retries for failing tasks. equivalent\
  \ to\n                        \"maxRetries\" in workflow options JSON file.\n  -m\
  \ METADATA_OUTPUT, --metadata-output METADATA_OUTPUT\n                        An\
  \ optional directory path to output metadata JSON\n                        file\n\
  \  --java-heap-run JAVA_HEAP_RUN\n                        Cromwell Java heap size\
  \ for \"run\" mode (java -Xmx)\n  --cromwell-stdout CROMWELL_STDOUT\n          \
  \              Local file to write STDOUT of Cromwell Java process\n           \
  \             to. This is for Cromwell (not for Caper's logging\n              \
  \          system). Note that STDERR is redirected to STDOUT.\n  -b BACKEND, --backend\
  \ BACKEND\n                        Backend to run a workflow\n  --dry-run      \
  \       Caper localizes remote files and validates WDL but\n                   \
  \     does not run/submit a pipeline.\n\ncache directories for localization:\n \
  \ --local-loc-dir LOCAL_LOC_DIR, --tmp-dir LOCAL_LOC_DIR\n                     \
  \   Temporary directory to store Cromwell's intermediate\n                     \
  \   backend files. These files include backend.conf,\n                        workflow_opts.json,\
  \ imports.zip. and localized input\n                        JSON files due to deepcopying\
  \ (recursive\n                        localization). Cromwell's MySQL/PostgreSQL\
  \ DB password\n                        can be exposed on backend.conf on this directory.\n\
  \                        Therefore, DO NOT USE /tmp HERE. This directory is\n  \
  \                      also used for storing cached files for\n                \
  \        local/slurm/sge/pbs backends.\n  --gcp-loc-dir GCP_LOC_DIR, --tmp-gcs-bucket\
  \ GCP_LOC_DIR\n                        Temporary directory to store cached files\
  \ for gcp\n                        backend. e.g. gs://my-bucket/caper-cache-dir.\n\
  \  --aws-loc-dir AWS_LOC_DIR, --tmp-s3-bucket AWS_LOC_DIR\n                    \
  \    Temporary directory to store cached files for aws\n                       \
  \ backend. e.g. s3://my-bucket/caper-cache-dir.\n\ndependency resolver for all backends:\n\
  \  Cloud-based backends (gc and aws) will only use Docker so that \"--docker\n \
  \ URI_FOR_DOCKER_IMG\" must be specified in the command line argument or as a\n\
  \  comment \"#CAPER docker URI_FOR_DOCKER_IMG\" or value for\n  \"workflow.meta.caper_docker\"\
  in a WDL file\n\n  --docker [DOCKER]     URI for Docker image (e.g. ubuntu:latest).\
  \ This can\n                        also be used as a flag to use Docker image address\n\
  \                        defined in your WDL file as a comment (\"#CAPER\n     \
  \                   docker\") or as \"workflow.meta.caper_docker\" in WDL.\n\ndependency\
  \ resolver for local backend:\n  Singularity is for local backend only. Other backends\
  \ (gcp and aws) will\n  use Docker only. Local backend defaults to not use any container-based\n\
  \  methods. Use \"--singularity\" or \"--docker\" to use containers\n\n  --singularity\
  \ [SINGULARITY]\n                        URI or path for Singularity image (e.g.\n\
  \                        ~/.singularity/ubuntu-latest.simg,\n                  \
  \      docker://ubuntu:latest, shub://vsoch/hello-world).\n                    \
  \    This can also be used as a flag to use Docker image\n                     \
  \   address defined in your WDL file as a comment (\"#CAPER\n                  \
  \      singularity\") or as \"workflow.meta.caper_singularity\"\n              \
  \          in WDL.\n  --no-build-singularity\n                        Do not build\
  \ singularity image before running a\n                        workflow.\n\nSLURM\
  \ arguments:\n  --slurm-partition SLURM_PARTITION\n                        SLURM\
  \ partition\n  --slurm-account SLURM_ACCOUNT\n                        SLURM account\n\
  \  --slurm-extra-param SLURM_EXTRA_PARAM\n                        SLURM extra parameters.\
  \ Must be double-quoted\n\nSGE arguments:\n  --sge-pe SGE_PE       SGE parallel\
  \ environment. Check with \"qconf -spl\"\n  --sge-queue SGE_QUEUE\n            \
  \            SGE queue. Check with \"qconf -sql\"\n  --sge-extra-param SGE_EXTRA_PARAM\n\
  \                        SGE extra parameters. Must be double-quoted\n\nPBS arguments:\n\
  \  --pbs-queue PBS_QUEUE\n                        PBS queue\n  --pbs-extra-param\
  \ PBS_EXTRA_PARAM\n                        PBS extra parameters. Must be double-quoted\n\
  \nGeneral DB settings (for both file DB and MySQL DB):\n  --db DB              \
  \ Cromwell metadata database type\n  --db-timeout DB_TIMEOUT\n                 \
  \       Milliseconds to wait for DB connection.\n\nHyperSQL file DB arguments (unstable,\
  \ not recommended):\n  --file-db FILE_DB, -d FILE_DB\n                        Default\
  \ DB file for Cromwell's built-in HyperSQL\n                        database.\n\n\
  MySQL DB arguments:\n  --mysql-db-ip MYSQL_DB_IP\n                        MySQL\
  \ Database IP address (e.g. localhost)\n  --mysql-db-port MYSQL_DB_PORT\n      \
  \                  MySQL Database TCP/IP port (e.g. 3306)\n  --mysql-db-user MYSQL_DB_USER\n\
  \                        MySQL DB username\n  --mysql-db-password MYSQL_DB_PASSWORD\n\
  \                        MySQL DB password\n  --mysql-db-name MYSQL_DB_NAME\n  \
  \                      MySQL DB name for Cromwell\n\nPostgreSQL DB arguments:\n\
  \  --postgresql-db-ip POSTGRESQL_DB_IP\n                        PostgreSQL DB IP\
  \ address (e.g. localhost)\n  --postgresql-db-port POSTGRESQL_DB_PORT\n        \
  \                PostgreSQL DB TCP/IP port (e.g. 5432)\n  --postgresql-db-user POSTGRESQL_DB_USER\n\
  \                        PostgreSQL DB username\n  --postgresql-db-password POSTGRESQL_DB_PASSWORD\n\
  \                        PostgreSQL DB password\n  --postgresql-db-name POSTGRESQL_DB_NAME\n\
  \                        PostgreSQL DB name for Cromwell\n\nCromwell settings:\n\
  \  --cromwell CROMWELL   Path or URL for Cromwell JAR file\n  --max-concurrent-tasks\
  \ MAX_CONCURRENT_TASKS\n                        Number of concurrent tasks. \"config.concurrent-job-\n\
  \                        limit\" in Cromwell backend configuration for each\n  \
  \                      backend\n  --max-concurrent-workflows MAX_CONCURRENT_WORKFLOWS\n\
  \                        Number of concurrent workflows. \"system.max-\n       \
  \                 concurrent-workflows\" in backend configuration\n  --disable-call-caching\n\
  \                        Disable Cromwell's call caching, which re-uses outputs\n\
  \                        from previous workflows\n  --backend-file BACKEND_FILE\n\
  \                        Custom Cromwell backend configuration file to override\n\
  \                        all\n  --soft-glob-output    Use soft-linking when globbing\
  \ outputs for a\n                        filesystem that does not allow hard-linking.\
  \ e.g.\n                        beeGFS. This flag does not work with backends based\
  \ on\n                        a Docker container. i.e. gcp and aws. Also, it does\n\
  \                        not work with local backends (local/slurm/sge/pbs)\n  \
  \                      with --. However, it works fine with --singularity.\n  --local-hash-strat\
  \ {file,path,path+modtime}\n                        File hashing strategy for call\
  \ caching. For local\n                        backends (local/slurm/sge/pbs) only.\
  \ file: use md5sum\n                        hash (slow), path: use path only, path+modtime\n\
  \                        (default): use path + mtime.\n\nlocal backend arguments:\n\
  \  --local-out-dir LOCAL_OUT_DIR, --out-dir LOCAL_OUT_DIR\n                    \
  \    Output directory path for local backend. Cloud\n                        backends\
  \ (gcp, aws) use different output directories.\n                        For gcp,\
  \ define --gcp-out-dir. For aws, define --aws-\n                        out-dir.\n\
  \nGCP backend arguments for server/runner:\n  --gcp-prj GCP_PRJ     GC project\n\
  \  --gcp-memory-retry-error-keys GCP_MEMORY_RETRY_ERROR_KEYS\n                 \
  \       If an error caught by these comma-separated keys\n                     \
  \   occurs, then increase memory by --gcp-memory-retry-\n                      \
  \  multiplier for retrials controlled by --max-retries.\n                      \
  \  See https://cromwell.readthedocs.io/en/stable/backends\n                    \
  \    /Google/ for details.\n  --gcp-memory-retry-multiplier GCP_MEMORY_RETRY_MULTIPLIER\n\
  \                        If an error caught by --gcp-memory-retry-error-keys\n \
  \                       occurs, then increase memory by this for retrials\n    \
  \                    controlled by --max-retries. See https://cromwell.read\n  \
  \                      thedocs.io/en/stable/backends/Google/ for details.\n  --gcp-region\
  \ GCP_REGION\n                        GCP region for Google Cloud Life Sciences\
  \ API. This is\n                        used only when --use-google-cloud-life-sciences\
  \ is\n                        defined.\n  --gcp-call-caching-dup-strat {reference,reference}\n\
  \                        Duplication strategy for call-cached outputs for GCP\n\
  \                        backend: copy: make a copy, reference: refer to old\n \
  \                       output in metadata.json.\n  --gcp-out-dir GCP_OUT_DIR, --out-gcs-bucket\
  \ GCP_OUT_DIR\n                        Output directory path for GCP backend. e.g.\
  \ gs://my-\n                        bucket/my-output.\n\nAWS backend arguments:\n\
  \  --aws-batch-arn AWS_BATCH_ARN\n                        ARN for AWS Batch\n  --aws-region\
  \ AWS_REGION\n                        AWS region (e.g. us-west-1)\n  --aws-out-dir\
  \ AWS_OUT_DIR, --out-s3-bucket AWS_OUT_DIR\n                        Output path\
  \ on S3 bucket for AWS backend. e.g.\n                        s3://my-bucket/my-output.\n\
  \nGCP backend arguments for server/runner/client:\n  --use-google-cloud-life-sciences\n\
  \                        Use Google Cloud Life Sciences API (v2beta) instead of\n\
  \                        deprecated Genomics API (v2alpha1).Life Sciences API\n\
  \                        requires only one region specified withgcp-region.\n  \
  \                      gcp-zones will be ignored since it is for Genomics\n    \
  \                    API.See https://cloud.google.com/life-\n                  \
  \      sciences/docs/concepts/locations for supported\n                        regions.\n\
  \  --gcp-zones GCP_ZONES\n                        Comma-separated GCP zones used\
  \ for Genomics API. (e.g.\n                        us-west1-b,us-central1-b). If\
  \ you use --use-google-\n                        cloud-life-sciences then define\
  \ --gcp-region instead.\n"
generated_using:
- --help
docker_image: quay.io/biocontainers/caper:1.1.0--py_0
