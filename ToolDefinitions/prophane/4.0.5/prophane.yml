!Command
command: &id001
- prophane
positional: []
named: []
parent:
subcommands:
- !Command
  command: &id002
  - prophane
  - target
  positional: []
  named:
  - !Flag
    optional: true
    synonyms:
    - --list-dbs
    description: "print list of configured databases\ndatabases are looked up in 'db_base_dir'\
      \ configured in:\n/usr/local/opt/prophane/general_config.yaml"
    args: !EmptyFlagArg {}
  - !Flag
    optional: true
    synonyms:
    - --list-styles
    description: "print list of available input file styles\nstyles are looked up\
      \ in the following folder:\n/usr/local/opt/prophane/styles"
    args: !EmptyFlagArg {}
  - !Flag
    optional: true
    synonyms:
    - --db-maintenance
    description: "trigger database maintenance scripts\nwill migrate database structure\
      \ from deprecated structure to most recent"
    args: !EmptyFlagArg {}
  - !Flag
    optional: true
    synonyms:
    - -j
    - --cores
    description: number of cores
    args: !EmptyFlagArg {}
  - !Flag
    optional: true
    synonyms:
    - -k
    - --keep-going
    description: go on with independent jobs if a job fails
    args: !EmptyFlagArg {}
  - !Flag
    optional: true
    synonyms:
    - -n
    - --dryrun
    description: do not execute anything
    args: !EmptyFlagArg {}
  - !Flag
    optional: true
    synonyms:
    - -p
    - --printshellcmds
    description: print out the shell commands that will be executed
    args: !EmptyFlagArg {}
  - !Flag
    optional: true
    synonyms:
    - -t
    - --timestamp
    description: add a timestamp to all logging output
    args: !EmptyFlagArg {}
  parent: &id009 !Command
    command: *id001
    positional: []
    named: []
    parent:
    subcommands:
    - !Command
      command: *id002
      positional: []
      named:
      - !Flag
        optional: true
        synonyms:
        - --list-dbs
        description: "print list of configured databases\ndatabases are looked up\
          \ in 'db_base_dir' configured in:\n/usr/local/opt/prophane/general_config.yaml"
        args: !EmptyFlagArg {}
      - !Flag
        optional: true
        synonyms:
        - --list-styles
        description: "print list of available input file styles\nstyles are looked\
          \ up in the following folder:\n/usr/local/opt/prophane/styles"
        args: !EmptyFlagArg {}
      - !Flag
        optional: true
        synonyms:
        - --db-maintenance
        description: "trigger database maintenance scripts\nwill migrate database\
          \ structure from deprecated structure to most recent"
        args: !EmptyFlagArg {}
      - !Flag
        optional: true
        synonyms:
        - -j
        - --cores
        description: number of cores
        args: !EmptyFlagArg {}
      - !Flag
        optional: true
        synonyms:
        - -k
        - --keep-going
        description: go on with independent jobs if a job fails
        args: !EmptyFlagArg {}
      - !Flag
        optional: true
        synonyms:
        - -n
        - --dryrun
        description: do not execute anything
        args: !EmptyFlagArg {}
      - !Flag
        optional: true
        synonyms:
        - -p
        - --printshellcmds
        description: print out the shell commands that will be executed
        args: !EmptyFlagArg {}
      - !Flag
        optional: true
        synonyms:
        - -t
        - --timestamp
        description: add a timestamp to all logging output
        args: !EmptyFlagArg {}
      parent: &id008 !Command
        command: *id001
        positional: []
        named: []
        parent:
        subcommands:
        - !Command
          command: *id002
          positional: []
          named:
          - !Flag
            optional: true
            synonyms:
            - --list-dbs
            description: "print list of configured databases\ndatabases are looked\
              \ up in 'db_base_dir' configured in:\n/usr/local/opt/prophane/general_config.yaml"
            args: !EmptyFlagArg {}
          - !Flag
            optional: true
            synonyms:
            - --list-styles
            description: "print list of available input file styles\nstyles are looked\
              \ up in the following folder:\n/usr/local/opt/prophane/styles"
            args: !EmptyFlagArg {}
          - !Flag
            optional: true
            synonyms:
            - --db-maintenance
            description: "trigger database maintenance scripts\nwill migrate database\
              \ structure from deprecated structure to most recent"
            args: !EmptyFlagArg {}
          - !Flag
            optional: true
            synonyms:
            - -j
            - --cores
            description: number of cores
            args: !EmptyFlagArg {}
          - !Flag
            optional: true
            synonyms:
            - -k
            - --keep-going
            description: go on with independent jobs if a job fails
            args: !EmptyFlagArg {}
          - !Flag
            optional: true
            synonyms:
            - -n
            - --dryrun
            description: do not execute anything
            args: !EmptyFlagArg {}
          - !Flag
            optional: true
            synonyms:
            - -p
            - --printshellcmds
            description: print out the shell commands that will be executed
            args: !EmptyFlagArg {}
          - !Flag
            optional: true
            synonyms:
            - -t
            - --timestamp
            description: add a timestamp to all logging output
            args: !EmptyFlagArg {}
          parent: &id007 !Command
            command: *id001
            positional: []
            named: []
            parent:
            subcommands:
            - !Command
              command: *id002
              positional: []
              named:
              - !Flag
                optional: true
                synonyms:
                - --list-dbs
                description: "print list of configured databases\ndatabases are looked\
                  \ up in 'db_base_dir' configured in:\n/usr/local/opt/prophane/general_config.yaml"
                args: !EmptyFlagArg {}
              - !Flag
                optional: true
                synonyms:
                - --list-styles
                description: "print list of available input file styles\nstyles are\
                  \ looked up in the following folder:\n/usr/local/opt/prophane/styles"
                args: !EmptyFlagArg {}
              - !Flag
                optional: true
                synonyms:
                - --db-maintenance
                description: "trigger database maintenance scripts\nwill migrate database\
                  \ structure from deprecated structure to most recent"
                args: !EmptyFlagArg {}
              - !Flag
                optional: true
                synonyms:
                - -j
                - --cores
                description: number of cores
                args: !EmptyFlagArg {}
              - !Flag
                optional: true
                synonyms:
                - -k
                - --keep-going
                description: go on with independent jobs if a job fails
                args: !EmptyFlagArg {}
              - !Flag
                optional: true
                synonyms:
                - -n
                - --dryrun
                description: do not execute anything
                args: !EmptyFlagArg {}
              - !Flag
                optional: true
                synonyms:
                - -p
                - --printshellcmds
                description: print out the shell commands that will be executed
                args: !EmptyFlagArg {}
              - !Flag
                optional: true
                synonyms:
                - -t
                - --timestamp
                description: add a timestamp to all logging output
                args: !EmptyFlagArg {}
              parent: &id006 !Command
                command: *id001
                positional: []
                named: []
                parent:
                subcommands:
                - !Command
                  command: *id002
                  positional: []
                  named:
                  - !Flag
                    optional: true
                    synonyms:
                    - --list-dbs
                    description: "print list of configured databases\ndatabases are\
                      \ looked up in 'db_base_dir' configured in:\n/usr/local/opt/prophane/general_config.yaml"
                    args: !EmptyFlagArg {}
                  - !Flag
                    optional: true
                    synonyms:
                    - --list-styles
                    description: "print list of available input file styles\nstyles\
                      \ are looked up in the following folder:\n/usr/local/opt/prophane/styles"
                    args: !EmptyFlagArg {}
                  - !Flag
                    optional: true
                    synonyms:
                    - --db-maintenance
                    description: "trigger database maintenance scripts\nwill migrate\
                      \ database structure from deprecated structure to most recent"
                    args: !EmptyFlagArg {}
                  - !Flag
                    optional: true
                    synonyms:
                    - -j
                    - --cores
                    description: number of cores
                    args: !EmptyFlagArg {}
                  - !Flag
                    optional: true
                    synonyms:
                    - -k
                    - --keep-going
                    description: go on with independent jobs if a job fails
                    args: !EmptyFlagArg {}
                  - !Flag
                    optional: true
                    synonyms:
                    - -n
                    - --dryrun
                    description: do not execute anything
                    args: !EmptyFlagArg {}
                  - !Flag
                    optional: true
                    synonyms:
                    - -p
                    - --printshellcmds
                    description: print out the shell commands that will be executed
                    args: !EmptyFlagArg {}
                  - !Flag
                    optional: true
                    synonyms:
                    - -t
                    - --timestamp
                    description: add a timestamp to all logging output
                    args: !EmptyFlagArg {}
                  parent: &id003 !Command
                    command: *id001
                    positional: []
                    named:
                    - !Flag
                      optional: true
                      synonyms:
                      - --dry-run
                      - --dryrun
                      - -n
                      description: "Do not execute anything, and display what would\
                        \ be\ndone. If you have a very large workflow, use --dry-run\n\
                        --quiet to just print a summary of the DAG of jobs."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --profile
                      description: "Name of profile to use for configuring Snakemake.\n\
                        Snakemake will search for a corresponding folder in\n/etc/xdg/snakemake\
                        \ and /root/.config/snakemake.\nAlternatively, this can be\
                        \ an absolute or relative\npath. The profile folder has to\
                        \ contain a file\n'config.yaml'. This file can be used to\
                        \ set default\nvalues for command line options in YAML format.\
                        \ For\nexample, '--cluster qsub' becomes 'cluster: qsub' in\n\
                        the YAML file. Profiles can be obtained from\nhttps://github.com/snakemake-profiles."
                      args: !SimpleFlagArg
                        name: PROFILE
                    - !Flag
                      optional: true
                      synonyms:
                      - --cache
                      description: "Store output files of given rules in a central\
                        \ cache\ngiven by the environment variable\n$SNAKEMAKE_OUTPUT_CACHE.\
                        \ Likewise, retrieve output\nfiles of the given rules from\
                        \ this cache if they have\nbeen created before (by anybody\
                        \ writing to the same\ncache), instead of actually executing\
                        \ the rules.\nOutput files are identified by hashing all steps,\n\
                        parameters and software stack (conda envs or\ncontainers)\
                        \ needed to create them."
                      args: !RepeatFlagArg
                        name: RULE
                    - !Flag
                      optional: true
                      synonyms:
                      - --snakefile
                      - -s
                      description: "The workflow definition in form of a\nsnakefile.Usually,\
                        \ you should not need to specify\nthis. By default, Snakemake\
                        \ will search for\n'Snakefile', 'snakefile', 'workflow/Snakefile',\n\
                        'workflow/snakefile' beneath the current working\ndirectory,\
                        \ in this order. Only if you definitely want\na different\
                        \ layout, you need to use this parameter."
                      args: !SimpleFlagArg
                        name: FILE
                    - !Flag
                      optional: true
                      synonyms:
                      - --cores
                      description: "[N], --jobs [N], -j [N]\nUse at most N CPU cores/jobs\
                        \ in parallel. If N is\nomitted or 'all', the limit is set\
                        \ to the number of\navailable CPU cores."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --local-cores
                      description: "In cluster mode, use at most N cores of the host\n\
                        machine in parallel (default: number of CPU cores of\nthe\
                        \ host). The cores are used to execute local rules.\nThis\
                        \ option is ignored when not in cluster mode."
                      args: !SimpleFlagArg
                        name: N
                    - !Flag
                      optional: true
                      synonyms:
                      - --resources
                      description: "[NAME=INT [NAME=INT ...]], --res [NAME=INT [NAME=INT\
                        \ ...]]\nDefine additional resources that shall constrain\
                        \ the\nscheduling analogously to threads (see above). A\n\
                        resource is defined as a name and an integer value.\nE.g.\
                        \ --resources gpu=1. Rules can use resources by\ndefining\
                        \ the resource keyword, e.g. resources: gpu=1.\nIf now two\
                        \ rules require 1 of the resource 'gpu' they\nwon't be run\
                        \ in parallel by the scheduler."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --set-threads
                      description: "[RULE=THREADS [RULE=THREADS ...]]\nOverwrite thread\
                        \ usage of rules. This allows to fine-\ntune workflow parallelization.\
                        \ In particular, this is\nhelpful to target certain cluster\
                        \ nodes by e.g.\nshifting a rule to use more, or less threads\
                        \ than\ndefined in the workflow. Thereby, THREADS has to be\
                        \ a\npositive integer, and RULE has to be the name of the\n\
                        rule."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --default-resources
                      description: "[NAME=INT [NAME=INT ...]], --default-res [NAME=INT\
                        \ [NAME=INT ...]]\nDefine default values of resources for\
                        \ rules that do\nnot define their own values. In addition\
                        \ to plain\nintegers, python expressions over inputsize are\n\
                        allowed (e.g. '2*input.size_mb').When specifying this\nwithout\
                        \ any arguments (--default-resources), it\ndefines 'mem_mb=max(2*input.size_mb,\
                        \ 1000)'\n'disk_mb=max(2*input.size_mb, 1000)', i.e., default\n\
                        disk and mem usage is twice the input file size but at\nleast\
                        \ 1GB."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --config
                      description: "[KEY=VALUE [KEY=VALUE ...]], -C [KEY=VALUE [KEY=VALUE\
                        \ ...]]\nSet or overwrite values in the workflow config object.\n\
                        The workflow config object is accessible as variable\nconfig\
                        \ inside the workflow. Default values can be set\nby providing\
                        \ a JSON file (see Documentation)."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --configfile
                      - --configfiles
                      description: "Specify or overwrite the config file of the workflow\n\
                        (see the docs). Values specified in JSON or YAML\nformat are\
                        \ available in the global config dictionary\ninside the workflow.\
                        \ Multiple files overwrite each\nother in the given order."
                      args: !RepeatFlagArg
                        name: FILE
                    - !Flag
                      optional: true
                      synonyms:
                      - --directory
                      - -d
                      description: "Specify working directory (relative paths in the\n\
                        snakefile will use this as their origin)."
                      args: !SimpleFlagArg
                        name: DIR
                    - !Flag
                      optional: true
                      synonyms:
                      - --touch
                      - -t
                      description: "Touch output files (mark them up to date without\n\
                        really changing them) instead of running their\ncommands.\
                        \ This is used to pretend that the rules were\nexecuted, in\
                        \ order to fool future invocations of\nsnakemake. Fails if\
                        \ a file does not yet exist. Note\nthat this will only touch\
                        \ files that would otherwise\nbe recreated by Snakemake (e.g.\
                        \ because their input\nfiles are newer). For enforcing a touch,\
                        \ combine this\nwith --force, --forceall, or --forcerun. Note\
                        \ however\nthat you loose the provenance information when\
                        \ the\nfiles have been created in realitiy. Hence, this\n\
                        should be used only as a last resort."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --keep-going
                      - -k
                      description: Go on with independent jobs if a job fails.
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --force
                      - -f
                      description: "Force the execution of the selected target or\
                        \ the\nfirst rule regardless of already created output."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --forceall
                      - -F
                      description: "Force the execution of the selected (or the first)\n\
                        rule and all rules it is dependent on regardless of\nalready\
                        \ created output."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --forcerun
                      description: "[TARGET [TARGET ...]], -R [TARGET [TARGET ...]]\n\
                        Force the re-execution or creation of the given rules\nor\
                        \ files. Use this option if you changed a rule and\nwant to\
                        \ have all its output in your workflow updated."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --prioritize
                      - -P
                      description: "Tell the scheduler to assign creation of given\
                        \ targets\n(and all their dependencies) highest priority.\n\
                        (EXPERIMENTAL)"
                      args: !RepeatFlagArg
                        name: TARGET
                    - !Flag
                      optional: true
                      synonyms:
                      - --batch
                      description: "=BATCH/BATCHES\nOnly create the given BATCH of\
                        \ the input files of the\ngiven RULE. This can be used to\
                        \ iteratively run parts\nof very large workflows. Only the\
                        \ execution plan of\nthe relevant part of the workflow has\
                        \ to be\ncalculated, thereby speeding up DAG computation.\
                        \ It is\nrecommended to provide the most suitable rule for\n\
                        batching when documenting a workflow. It should be\nsome aggregating\
                        \ rule that would be executed only\nonce, and has a large\
                        \ number of input files. For\nexample, it can be a rule that\
                        \ aggregates over\nsamples."
                      args: !SimpleFlagArg
                        name: RULE
                    - !Flag
                      optional: true
                      synonyms:
                      - --until
                      - -U
                      description: "Runs the pipeline until it reaches the specified\
                        \ rules\nor files. Only runs jobs that are dependencies of\
                        \ the\nspecified rule or files, does not run sibling DAGs."
                      args: !RepeatFlagArg
                        name: TARGET
                    - !Flag
                      optional: true
                      synonyms:
                      - --omit-from
                      - -O
                      description: "Prevent the execution or creation of the given\
                        \ rules\nor files as well as any rules or files that are\n\
                        downstream of these targets in the DAG. Also runs jobs\nin\
                        \ sibling DAGs that are independent of the rules or\nfiles\
                        \ specified here."
                      args: !RepeatFlagArg
                        name: TARGET
                    - !Flag
                      optional: true
                      synonyms:
                      - --rerun-incomplete
                      - --ri
                      description: "Re-run all jobs the output of which is recognized\
                        \ as\nincomplete."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --shadow-prefix
                      description: "Specify a directory in which the 'shadow' directory\
                        \ is\ncreated. If not supplied, the value is set to the\n\
                        '.snakemake' directory relative to the working\ndirectory."
                      args: !SimpleFlagArg
                        name: DIR
                    - !Flag
                      optional: true
                      synonyms:
                      - --report
                      description: "[HTMLFILE]   Create an HTML report with results\
                        \ and statistics. If\nno filename is given, report.html is\
                        \ the default."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --lint
                      description: "[{text,json}]  Perform linting on the given workflow.\
                        \ This will print\nsnakemake specific suggestions to improve\
                        \ code quality\n(work in progress, more lints to be added\
                        \ in the\nfuture). If no argument is provided, plain text\
                        \ output\nis used."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --export-cwl
                      description: Compile workflow to CWL and store it in given FILE.
                      args: !SimpleFlagArg
                        name: FILE
                    - !Flag
                      optional: true
                      synonyms:
                      - --list
                      - -l
                      description: Show available rules in given Snakefile.
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --list-target-rules
                      - --lt
                      description: Show available target rules in given Snakefile.
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --dag
                      description: "Do not execute anything and print the directed\
                        \ acyclic\ngraph of jobs in the dot language. Recommended\
                        \ use on\nUnix systems: snakemake --dag | dot | display"
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --rulegraph
                      description: "Do not execute anything and print the dependency\
                        \ graph\nof rules in the dot language. This will be less\n\
                        crowded than above DAG of jobs, but also show less\ninformation.\
                        \ Note that each rule is displayed once,\nhence the displayed\
                        \ graph will be cyclic if a rule\nappears in several steps\
                        \ of the workflow. Use this if\nabove option leads to a DAG\
                        \ that is too large.\nRecommended use on Unix systems: snakemake\
                        \ --rulegraph\n| dot | display"
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --filegraph
                      description: "Do not execute anything and print the dependency\
                        \ graph\nof rules with their input and output files in the\
                        \ dot\nlanguage. This is an intermediate solution between\n\
                        above DAG of jobs and the rule graph. Note that each\nrule\
                        \ is displayed once, hence the displayed graph will\nbe cyclic\
                        \ if a rule appears in several steps of the\nworkflow. Use\
                        \ this if above option leads to a DAG that\nis too large.\
                        \ Recommended use on Unix systems:\nsnakemake --filegraph\
                        \ | dot | display"
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --d3dag
                      description: Print the DAG in D3.js compatible JSON format.
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --summary
                      - -S
                      description: "Print a summary of all files created by the workflow.\n\
                        The has the following columns: filename, modification\ntime,\
                        \ rule version, status, plan. Thereby rule version\ncontains\
                        \ the versionthe file was created with (see the\nversion keyword\
                        \ of rules), and status denotes whether\nthe file is missing,\
                        \ its input files are newer or if\nversion or implementation\
                        \ of the rule changed since\nfile creation. Finally the last\
                        \ column denotes whether\nthe file will be updated or created\
                        \ during the next\nworkflow execution."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --detailed-summary
                      - -D
                      description: "Print a summary of all files created by the workflow.\n\
                        The has the following columns: filename, modification\ntime,\
                        \ rule version, input file(s), shell command,\nstatus, plan.\
                        \ Thereby rule version contains the\nversion the file was\
                        \ created with (see the version\nkeyword of rules), and status\
                        \ denotes whether the file\nis missing, its input files are\
                        \ newer or if version or\nimplementation of the rule changed\
                        \ since file\ncreation. The input file and shell command columns\
                        \ are\nself explanatory. Finally the last column denotes\n\
                        whether the file will be updated or created during the\nnext\
                        \ workflow execution."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --archive
                      description: "Archive the workflow into the given tar archive\
                        \ FILE.\nThe archive will be created such that the workflow\
                        \ can\nbe re-executed on a vanilla system. The function needs\n\
                        conda and git to be installed. It will archive every\nfile\
                        \ that is under git version control. Note that it\nis best\
                        \ practice to have the Snakefile, config files,\nand scripts\
                        \ under version control. Hence, they will be\nincluded in\
                        \ the archive. Further, it will add input\nfiles that are\
                        \ not generated by by the workflow itself\nand conda environments.\
                        \ Note that symlinks are\ndereferenced. Supported formats\
                        \ are .tar, .tar.gz,\n.tar.bz2 and .tar.xz."
                      args: !SimpleFlagArg
                        name: FILE
                    - !Flag
                      optional: true
                      synonyms:
                      - --cleanup-metadata
                      - --cm
                      description: "Cleanup the metadata of given files. That means\
                        \ that\nsnakemake removes any tracked version info, and any\n\
                        marks that files are incomplete."
                      args: !RepeatFlagArg
                        name: FILE
                    - !Flag
                      optional: true
                      synonyms:
                      - --cleanup-shadow
                      description: "Cleanup old shadow directories which have not\
                        \ been\ndeleted due to failures or power loss."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --skip-script-cleanup
                      description: Don't delete wrapper scripts used for execution
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --unlock
                      description: Remove a lock on the working directory.
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --list-version-changes
                      - --lv
                      description: "List all output files that have been created with\
                        \ a\ndifferent version (as determined by the version\nkeyword)."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --list-code-changes
                      - --lc
                      description: "List all output files for which the rule body\
                        \ (run or\nshell) have changed in the Snakefile."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --list-input-changes
                      - --li
                      description: "List all output files for which the defined input\n\
                        files have changed in the Snakefile (e.g. new input\nfiles\
                        \ were added in the rule definition or files were\nrenamed).\
                        \ For listing input file modification in the\nfilesystem,\
                        \ use --summary."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --list-params-changes
                      - --lp
                      description: "List all output files for which the defined params\n\
                        have changed in the Snakefile."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --list-untracked
                      - --lu
                      description: "List all files in the working directory that are\
                        \ not\nused in the workflow. This can be used e.g. for\nidentifying\
                        \ leftover files. Hidden files and\ndirectories are ignored."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --delete-all-output
                      description: "Remove all files generated by the workflow. Use\n\
                        together with --dry-run to list files without actually\ndeleting\
                        \ anything. Note that this will not recurse\ninto subworkflows.\
                        \ Write-protected files are not\nremoved. Nevertheless, use\
                        \ with care!"
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --delete-temp-output
                      description: "Remove all temporary files generated by the workflow.\n\
                        Use together with --dry-run to list files without\nactually\
                        \ deleting anything. Note that this will not\nrecurse into\
                        \ subworkflows."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --bash-completion
                      description: "Output code to register bash completion for snakemake.\n\
                        Put the following in your .bashrc (including the\naccents):\
                        \ `snakemake --bash-completion` or issue it in\nan open terminal\
                        \ session."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --keep-incomplete
                      description: Do not remove incomplete output files by failed
                        jobs.
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --reason
                      - -r
                      description: Print the reason for each executed rule.
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --gui
                      description: "[PORT]          Serve an HTML based user interface\
                        \ to the given\nnetwork and port e.g. 168.129.10.15:8000.\
                        \ By default\nSnakemake is only available in the local network\n\
                        (default port: 8000). To make Snakemake listen to all\nip\
                        \ addresses add the special host address 0.0.0.0 to\nthe url\
                        \ (0.0.0.0:8000). This is important if Snakemake\nis used\
                        \ in a virtualised environment like Docker. If\npossible,\
                        \ a browser window is opened."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --printshellcmds
                      - -p
                      description: Print out the shell commands that will be executed.
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --debug-dag
                      description: "Print candidate and selected jobs (including their\n\
                        wildcards) while inferring DAG. This can help to debug\nunexpected\
                        \ DAG topology or errors."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --stats
                      description: "Write stats about Snakefile execution in JSON\
                        \ format\nto the given file."
                      args: !SimpleFlagArg
                        name: FILE
                    - !Flag
                      optional: true
                      synonyms:
                      - --nocolor
                      description: Do not use a colored output.
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --quiet
                      - -q
                      description: Do not output any progress or rule information.
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --print-compilation
                      description: Print the python representation of the workflow.
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --verbose
                      description: Print debugging output.
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --force-use-threads
                      description: "Force threads rather than processes. Helpful if\
                        \ shared\nmemory (/dev/shm) is full or unavailable."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --allow-ambiguity
                      - -a
                      description: "Don't check for ambiguous rules and simply use\
                        \ the\nfirst if several can produce the same file. This\n\
                        allows the user to prioritize rules by their order in\nthe\
                        \ snakefile."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --nolock
                      description: Do not lock the working directory
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --ignore-incomplete
                      - --ii
                      description: Do not check for incomplete output files.
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --latency-wait
                      - --output-wait
                      - -w
                      description: "Wait given seconds if an output file of a job\
                        \ is not\npresent after the job finished. This helps if your\n\
                        filesystem suffers from latency (default 5)."
                      args: !SimpleFlagArg
                        name: SECONDS
                    - !Flag
                      optional: true
                      synonyms:
                      - --wait-for-files
                      description: "[FILE [FILE ...]]\nWait --latency-wait seconds\
                        \ for these files to be\npresent before executing the workflow.\
                        \ This option is\nused internally to handle filesystem latency\
                        \ in\ncluster environments."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --notemp
                      - --nt
                      description: "Ignore temp() declarations. This is useful when\n\
                        running only a part of the workflow, since temp()\nwould lead\
                        \ to deletion of probably needed files by\nother parts of\
                        \ the workflow."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --keep-remote
                      description: Keep local copies of remote input files.
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --keep-target-files
                      description: "Do not adjust the paths of given target files\
                        \ relative\nto the working directory."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --allowed-rules
                      description: "Only consider given rules. If omitted, all rules\
                        \ in\nSnakefile are used. Note that this is intended\nprimarily\
                        \ for internal use and may lead to unexpected\nresults otherwise."
                      args: !RepeatFlagArg
                        name: ALLOWED_RULES
                    - !Flag
                      optional: true
                      synonyms:
                      - --max-jobs-per-second
                      description: "Maximal number of cluster/drmaa jobs per second,\n\
                        default is 10, fractions allowed."
                      args: !SimpleFlagArg
                        name: MAX_JOBS_PER_SECOND
                    - !Flag
                      optional: true
                      synonyms:
                      - --max-status-checks-per-second
                      description: "Maximal number of job status checks per second,\n\
                        default is 10, fractions allowed."
                      args: !SimpleFlagArg
                        name: MAX_STATUS_CHECKS_PER_SECOND
                    - !Flag
                      optional: true
                      synonyms:
                      - --restart-times
                      description: "Number of times to restart failing jobs (defaults\
                        \ to\n0)."
                      args: !SimpleFlagArg
                        name: RESTART_TIMES
                    - !Flag
                      optional: true
                      synonyms:
                      - --attempt
                      description: "Internal use only: define the initial value of\
                        \ the\nattempt parameter (default: 1)."
                      args: !SimpleFlagArg
                        name: ATTEMPT
                    - !Flag
                      optional: true
                      synonyms:
                      - --wrapper-prefix
                      description: "Prefix for URL created from wrapper directive\n\
                        (default: https://github.com/snakemake/snakemake-\nwrappers/raw/).\
                        \ Set this to a different URL to use\nyour fork or a local\
                        \ clone of the repository, e.g.,\nuse a git URL like\n'git+file://path/to/your/local/clone@'."
                      args: !SimpleFlagArg
                        name: WRAPPER_PREFIX
                    - !Flag
                      optional: true
                      synonyms:
                      - --default-remote-provider
                      description: "Specify default remote provider to be used for\
                        \ all\ninput and output files that don't yet specify one."
                      args: !ChoiceFlagArg
                        choices: !!set
                          ? gridftp
                          ? FTP
                          ? GS
                          ? iRODS
                          ? SFTP
                          ? S3
                          ? gfal
                          ? S3Mocked
                    - !Flag
                      optional: true
                      synonyms:
                      - --default-remote-prefix
                      description: "Specify prefix for default remote provider. E.g.\
                        \ a\nbucket name."
                      args: !SimpleFlagArg
                        name: DEFAULT_REMOTE_PREFIX
                    - !Flag
                      optional: true
                      synonyms:
                      - --no-shared-fs
                      description: "Do not assume that jobs share a common file system.\n\
                        When this flag is activated, Snakemake will assume\nthat the\
                        \ filesystem on a cluster node is not shared\nwith other nodes.\
                        \ For example, this will lead to\ndownloading remote files\
                        \ on each cluster node\nseparately. Further, it won't take\
                        \ special measures to\ndeal with filesystem latency issues.\
                        \ This option will\nin most cases only make sense in combination\
                        \ with\n--default-remote-provider. Further, when using\n--cluster\
                        \ you will have to also provide --cluster-\nstatus. Only activate\
                        \ this if you know what you are\ndoing."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --greediness
                      description: "Set the greediness of scheduling. This value between\
                        \ 0\nand 1 determines how careful jobs are selected for\n\
                        execution. The default value (1.0) provides the best\nspeed\
                        \ and still acceptable scheduling quality."
                      args: !SimpleFlagArg
                        name: GREEDINESS
                    - !Flag
                      optional: true
                      synonyms:
                      - --no-hooks
                      description: "Do not invoke onstart, onsuccess or onerror hooks\n\
                        after execution."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --overwrite-shellcmd
                      description: "Provide a shell command that shall be executed\
                        \ instead\nof those given in the workflow. This is for debugging\n\
                        purposes only."
                      args: !SimpleFlagArg
                        name: OVERWRITE_SHELLCMD
                    - !Flag
                      optional: true
                      synonyms:
                      - --debug
                      description: "Allow to debug rules with e.g. PDB. This flag\
                        \ allows\nto set breakpoints in run blocks."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --runtime-profile
                      description: "Profile Snakemake and write the output to FILE.\
                        \ This\nrequires yappi to be installed."
                      args: !SimpleFlagArg
                        name: FILE
                    - !Flag
                      optional: true
                      synonyms:
                      - --mode
                      description: Set execution mode of Snakemake (internal use only).
                      args: !ChoiceFlagArg
                        choices: !!set
                          ? "1"
                          ? "2"
                          ? "0"
                    - !Flag
                      optional: true
                      synonyms:
                      - --show-failed-logs
                      description: Automatically display logs of failed jobs.
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --log-handler-script
                      description: "Provide a custom script containing a function\
                        \ 'def\nlog_handler(msg):'. Snakemake will call this function\n\
                        for every logging output (given as a dictionary\nmsg)allowing\
                        \ to e.g. send notifications in the form of\ne.g. slack messages\
                        \ or emails."
                      args: !SimpleFlagArg
                        name: FILE
                    - !Flag
                      optional: true
                      synonyms:
                      - --log-service
                      description: "Set a specific messaging service for logging\n\
                        output.Snakemake will notify the service on errors and\ncompleted\
                        \ execution.Currently only slack is supported."
                      args: !ChoiceFlagArg
                        choices: !!set
                          ? none
                          ? slack
                    - !Flag
                      optional: true
                      synonyms:
                      - --cluster
                      - -c
                      description: "Execute snakemake rules with the given submit\
                        \ command,\ne.g. qsub. Snakemake compiles jobs into scripts\
                        \ that\nare submitted to the cluster with the given command,\n\
                        once all input files for a particular job are present.\nThe\
                        \ submit command can be decorated to make it aware\nof certain\
                        \ job properties (name, rulename, input,\noutput, params,\
                        \ wildcards, log, threads and\ndependencies (see the argument\
                        \ below)), e.g.: $\nsnakemake --cluster 'qsub -pe threaded\
                        \ {threads}'."
                      args: !SimpleFlagArg
                        name: CMD
                    - !Flag
                      optional: true
                      synonyms:
                      - --cluster-sync
                      description: "cluster submission command will block, returning\
                        \ the\nremote exitstatus upon remote termination (for\nexample,\
                        \ this should be usedif the cluster command is\n'qsub -sync\
                        \ y' (SGE)"
                      args: !SimpleFlagArg
                        name: CMD
                    - !Flag
                      optional: true
                      synonyms:
                      - --drmaa
                      description: "[ARGS]        Execute snakemake on a cluster accessed\
                        \ via DRMAA,\nSnakemake compiles jobs into scripts that are\n\
                        submitted to the cluster with the given command, once\nall\
                        \ input files for a particular job are present. ARGS\ncan\
                        \ be used to specify options of the underlying\ncluster system,\
                        \ thereby using the job properties name,\nrulename, input,\
                        \ output, params, wildcards, log,\nthreads and dependencies,\
                        \ e.g.: --drmaa ' -pe threaded\n{threads}'. Note that ARGS\
                        \ must be given in quotes and\nwith a leading whitespace."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --cluster-config
                      - -u
                      description: "A JSON or YAML file that defines the wildcards\
                        \ used in\n'cluster'for specific rules, instead of having\
                        \ them\nspecified in the Snakefile. For example, for rule\n\
                        'job' you may define: { 'job' : { 'time' : '24:00:00'\n} }\
                        \ to specify the time for rule 'job'. You can\nspecify more\
                        \ than one file. The configuration files\nare merged with\
                        \ later values overriding earlier ones.\nThis option is deprecated\
                        \ in favor of using --profile,\nsee docs."
                      args: !SimpleFlagArg
                        name: FILE
                    - !Flag
                      optional: true
                      synonyms:
                      - --immediate-submit
                      - --is
                      description: "Immediately submit all jobs to the cluster instead\
                        \ of\nwaiting for present input files. This will fail,\nunless\
                        \ you make the cluster aware of job dependencies,\ne.g. via:\
                        \ $ snakemake --cluster 'sbatch --dependency\n{dependencies}.\
                        \ Assuming that your submit script (here\nsbatch) outputs\
                        \ the generated job id to the first\nstdout line, {dependencies}\
                        \ will be filled with space\nseparated job ids this job depends\
                        \ on."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --jobscript
                      - --js
                      description: "Provide a custom job script for submission to\
                        \ the\ncluster. The default script resides as 'jobscript.sh'\n\
                        in the installation directory."
                      args: !SimpleFlagArg
                        name: SCRIPT
                    - !Flag
                      optional: true
                      synonyms:
                      - --jobname
                      - --jn
                      description: "Provide a custom name for the jobscript that is\n\
                        submitted to the cluster (see --cluster). NAME is\n\"snakejob.{name}.{jobid}.sh\"\
                        \ per default. The wildcard\n{jobid} has to be present in\
                        \ the name."
                      args: !SimpleFlagArg
                        name: NAME
                    - !Flag
                      optional: true
                      synonyms:
                      - --cluster-status
                      description: "Status command for cluster execution. This is\
                        \ only\nconsidered in combination with the --cluster flag.\
                        \ If\nprovided, Snakemake will use the status command to\n\
                        determine if a job has finished successfully or\nfailed. For\
                        \ this it is necessary that the submit\ncommand provided to\
                        \ --cluster returns the cluster job\nid. Then, the status\
                        \ command will be invoked with the\njob id. Snakemake expects\
                        \ it to return 'success' if\nthe job was successfull, 'failed'\
                        \ if the job failed\nand 'running' if the job still runs."
                      args: !SimpleFlagArg
                        name: CLUSTER_STATUS
                    - !Flag
                      optional: true
                      synonyms:
                      - --drmaa-log-dir
                      description: "Specify a directory in which stdout and stderr\
                        \ files\nof DRMAA jobs will be written. The value may be given\n\
                        as a relative path, in which case Snakemake will use\nthe\
                        \ current invocation directory as the origin. If\ngiven, this\
                        \ will override any given '-o' and/or '-e'\nnative specification.\
                        \ If not given, all DRMAA stdout\nand stderr files are written\
                        \ to the current working\ndirectory."
                      args: !SimpleFlagArg
                        name: DIR
                    - !Flag
                      optional: true
                      synonyms:
                      - --kubernetes
                      description: "[NAMESPACE]\nExecute workflow in a kubernetes\
                        \ cluster (in the\ncloud). NAMESPACE is the namespace you\
                        \ want to use for\nyour job (if nothing specified: 'default').\
                        \ Usually,\nthis requires --default-remote-provider and --default-\n\
                        remote-prefix to be set to a S3 or GS bucket where\nyour .\
                        \ data shall be stored. It is further advisable\nto activate\
                        \ conda integration via --use-conda."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --container-image
                      description: "Docker image to use, e.g., when submitting jobs\
                        \ to\nkubernetes. By default, this is\n'https://hub.docker.com/r/snakemake/snakemake',\
                        \ tagged\nwith the same version as the currently running\n\
                        Snakemake instance. Note that overwriting this value\nis up\
                        \ to your responsibility. Any used image has to\ncontain a\
                        \ working snakemake installation that is\ncompatible with\
                        \ (or ideally the same as) the currently\nrunning version."
                      args: !SimpleFlagArg
                        name: IMAGE
                    - !Flag
                      optional: true
                      synonyms:
                      - --tibanna
                      description: "Execute workflow on AWS cloud using Tibanna. This\n\
                        requires --default-remote-prefix to be set to S3\nbucket name\
                        \ and prefix (e.g.\n'bucketname/subdirectory') where input\
                        \ is already\nstored and output will be sent to. Using --tibanna\n\
                        implies --default-resources is set as default.\nOptionally,\
                        \ use --precommand to specify any\npreparation command to\
                        \ run before snakemake command on\nthe cloud (inside snakemake\
                        \ container on Tibanna VM).\nAlso, --use-conda, --use-singularity,\
                        \ --config,\n--configfile are supported and will be carried\
                        \ over."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --tibanna-sfn
                      description: "Name of Tibanna Unicorn step function (e.g.\n\
                        tibanna_unicorn_monty).This works as serverless\nscheduler/resource\
                        \ allocator and must be deployed\nfirst using tibanna cli.\
                        \ (e.g. tibanna deploy_unicorn\n--usergroup=monty --buckets=bucketname)"
                      args: !SimpleFlagArg
                        name: TIBANNA_SFN
                    - !Flag
                      optional: true
                      synonyms:
                      - --precommand
                      description: "Any command to execute before snakemake command\
                        \ on AWS\ncloud such as wget, git clone, unzip, etc. This\
                        \ is\nused with --tibanna.Do not include input/output\ndownload/upload\
                        \ commands - file transfer between S3\nbucket and the run\
                        \ environment (container) is\nautomatically handled by Tibanna."
                      args: !SimpleFlagArg
                        name: PRECOMMAND
                    - !Flag
                      optional: true
                      synonyms:
                      - --tibanna-config
                      description: "Additional tibanan config e.g. --tibanna-config\n\
                        spot_instance=true subnet=<subnet_id> security\ngroup=<security_group_id>"
                      args: !RepeatFlagArg
                        name: TIBANNA_CONFIG
                    - !Flag
                      optional: true
                      synonyms:
                      - --use-conda
                      description: "If defined in the rule, run job in a conda\nenvironment.\
                        \ If this flag is not set, the conda\ndirective is ignored."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --list-conda-envs
                      description: List all conda environments and their location
                        on
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --conda-prefix
                      description: "Specify a directory in which the 'conda' and 'conda-\n\
                        archive' directories are created. These are used to\nstore\
                        \ conda environments and their archives,\nrespectively. If\
                        \ not supplied, the value is set to the\n'.snakemake' directory\
                        \ relative to the invocation\ndirectory. If supplied, the\
                        \ `--use-conda` flag must\nalso be set. The value may be given\
                        \ as a relative\npath, which will be extrapolated to the invocation\n\
                        directory, or as an absolute path."
                      args: !SimpleFlagArg
                        name: DIR
                    - !Flag
                      optional: true
                      synonyms:
                      - --create-envs-only
                      description: "If specified, only creates the job-specific conda\n\
                        environments then exits. The `--use-conda` flag must\nalso\
                        \ be set."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --use-singularity
                      description: "If defined in the rule, run job within a singularity\n\
                        container. If this flag is not set, the singularity\ndirective\
                        \ is ignored."
                      args: !EmptyFlagArg {}
                    - !Flag
                      optional: true
                      synonyms:
                      - --singularity-prefix
                      description: "Specify a directory in which singularity images\
                        \ will\nbe stored.If not supplied, the value is set to the\n\
                        '.snakemake' directory relative to the invocation\ndirectory.\
                        \ If supplied, the `--use-singularity` flag\nmust also be\
                        \ set. The value may be given as a relative\npath, which will\
                        \ be extrapolated to the invocation\ndirectory, or as an absolute\
                        \ path."
                      args: !SimpleFlagArg
                        name: DIR
                    - !Flag
                      optional: true
                      synonyms:
                      - --singularity-args
                      description: Pass additional args to singularity.
                      args: !SimpleFlagArg
                        name: ARGS
                    - !Flag
                      optional: true
                      synonyms:
                      - --use-envmodules
                      description: "If defined in the rule, run job within the given\n\
                        environment modules, loaded in the given order. This\ncan\
                        \ be combined with --use-conda and --use-\nsingularity, which\
                        \ will then be only used as a\nfallback for rules which don't\
                        \ define environment\nmodules.\n"
                      args: !EmptyFlagArg {}
                    parent:
                    subcommands:
                    - !Command
                      command: *id002
                      positional: []
                      named:
                      - !Flag
                        optional: true
                        synonyms:
                        - --list-dbs
                        description: "print list of configured databases\ndatabases\
                          \ are looked up in 'db_base_dir' configured in:\n/usr/local/opt/prophane/general_config.yaml"
                        args: !EmptyFlagArg {}
                      - !Flag
                        optional: true
                        synonyms:
                        - --list-styles
                        description: "print list of available input file styles\n\
                          styles are looked up in the following folder:\n/usr/local/opt/prophane/styles"
                        args: !EmptyFlagArg {}
                      - !Flag
                        optional: true
                        synonyms:
                        - --db-maintenance
                        description: "trigger database maintenance scripts\nwill migrate\
                          \ database structure from deprecated structure to most recent"
                        args: !EmptyFlagArg {}
                      - !Flag
                        optional: true
                        synonyms:
                        - -j
                        - --cores
                        description: number of cores
                        args: !EmptyFlagArg {}
                      - !Flag
                        optional: true
                        synonyms:
                        - -k
                        - --keep-going
                        description: go on with independent jobs if a job fails
                        args: !EmptyFlagArg {}
                      - !Flag
                        optional: true
                        synonyms:
                        - -n
                        - --dryrun
                        description: do not execute anything
                        args: !EmptyFlagArg {}
                      - !Flag
                        optional: true
                        synonyms:
                        - -p
                        - --printshellcmds
                        description: print out the shell commands that will be executed
                        args: !EmptyFlagArg {}
                      - !Flag
                        optional: true
                        synonyms:
                        - -t
                        - --timestamp
                        description: add a timestamp to all logging output
                        args: !EmptyFlagArg {}
                      parent: *id003
                      subcommands: []
                      usage: []
                      help_flag: !Flag
                        optional: true
                        synonyms:
                        - --help
                        description: show Snakemake help (or snakemake -h)
                        args: !EmptyFlagArg {}
                      usage_flag:
                      version_flag:
                      help_text: "ERROR\ntarget: No such file or directory\nProphane\
                        \ Pipeline (powered by Snakemake)\n\nUsage: /usr/local/bin/prophane\
                        \ CONFIG_FILE [Snakemake options]\n\n Full list of parameters:\n\
                        \   --help                 show Snakemake help (or snakemake\
                        \ -h)\n   --list-dbs             print list of configured\
                        \ databases\n                          databases are looked\
                        \ up in 'db_base_dir' configured in:\n                   \
                        \           /usr/local/opt/prophane/general_config.yaml\n\
                        \   --list-styles          print list of available input file\
                        \ styles\n                          styles are looked up in\
                        \ the following folder:\n                              /usr/local/opt/prophane/styles\n\
                        \   --db-maintenance       trigger database maintenance scripts\n\
                        \                          will migrate database structure\
                        \ from deprecated structure to most recent\n\n Useful Snakemake\
                        \ parameters:\n   -j, --cores            number of cores\n\
                        \   -k, --keep-going       go on with independent jobs if\
                        \ a job fails\n   -n, --dryrun           do not execute anything\n\
                        \   -p, --printshellcmds   print out the shell commands that\
                        \ will be executed\n   -t, --timestamp  \t\tadd a timestamp\
                        \ to all logging output\n\n"
                      generated_using: &id004
                      - --help
                      docker_image:
                    - !Command
                      command: &id005
                      - prophane
                      - disk.
                      positional: []
                      named:
                      - !Flag
                        optional: true
                        synonyms:
                        - --list-dbs
                        description: "print list of configured databases\ndatabases\
                          \ are looked up in 'db_base_dir' configured in:\n/usr/local/opt/prophane/general_config.yaml"
                        args: !EmptyFlagArg {}
                      - !Flag
                        optional: true
                        synonyms:
                        - --list-styles
                        description: "print list of available input file styles\n\
                          styles are looked up in the following folder:\n/usr/local/opt/prophane/styles"
                        args: !EmptyFlagArg {}
                      - !Flag
                        optional: true
                        synonyms:
                        - --db-maintenance
                        description: "trigger database maintenance scripts\nwill migrate\
                          \ database structure from deprecated structure to most recent"
                        args: !EmptyFlagArg {}
                      - !Flag
                        optional: true
                        synonyms:
                        - -j
                        - --cores
                        description: number of cores
                        args: !EmptyFlagArg {}
                      - !Flag
                        optional: true
                        synonyms:
                        - -k
                        - --keep-going
                        description: go on with independent jobs if a job fails
                        args: !EmptyFlagArg {}
                      - !Flag
                        optional: true
                        synonyms:
                        - -n
                        - --dryrun
                        description: do not execute anything
                        args: !EmptyFlagArg {}
                      - !Flag
                        optional: true
                        synonyms:
                        - -p
                        - --printshellcmds
                        description: print out the shell commands that will be executed
                        args: !EmptyFlagArg {}
                      - !Flag
                        optional: true
                        synonyms:
                        - -t
                        - --timestamp
                        description: add a timestamp to all logging output
                        args: !EmptyFlagArg {}
                      parent: *id003
                      subcommands: []
                      usage: []
                      help_flag: !Flag
                        optional: true
                        synonyms:
                        - --help
                        description: show Snakemake help (or snakemake -h)
                        args: !EmptyFlagArg {}
                      usage_flag:
                      version_flag:
                      help_text: "ERROR\ndisk.: No such file or directory\nProphane\
                        \ Pipeline (powered by Snakemake)\n\nUsage: /usr/local/bin/prophane\
                        \ CONFIG_FILE [Snakemake options]\n\n Full list of parameters:\n\
                        \   --help                 show Snakemake help (or snakemake\
                        \ -h)\n   --list-dbs             print list of configured\
                        \ databases\n                          databases are looked\
                        \ up in 'db_base_dir' configured in:\n                   \
                        \           /usr/local/opt/prophane/general_config.yaml\n\
                        \   --list-styles          print list of available input file\
                        \ styles\n                          styles are looked up in\
                        \ the following folder:\n                              /usr/local/opt/prophane/styles\n\
                        \   --db-maintenance       trigger database maintenance scripts\n\
                        \                          will migrate database structure\
                        \ from deprecated structure to most recent\n\n Useful Snakemake\
                        \ parameters:\n   -j, --cores            number of cores\n\
                        \   -k, --keep-going       go on with independent jobs if\
                        \ a job fails\n   -n, --dryrun           do not execute anything\n\
                        \   -p, --printshellcmds   print out the shell commands that\
                        \ will be executed\n   -t, --timestamp  \t\tadd a timestamp\
                        \ to all logging output\n\n"
                      generated_using: *id004
                      docker_image:
                    usage: []
                    help_flag: !Flag
                      optional: true
                      synonyms:
                      - -h
                      - --help
                      description: show this help message and exit
                      args: !EmptyFlagArg {}
                    usage_flag:
                    version_flag: !Flag
                      optional: true
                      synonyms:
                      - --version
                      - -v
                      description: show program's version number and exit
                      args: !EmptyFlagArg {}
                    help_text: "usage: snakemake [-h] [--dry-run] [--profile PROFILE]\n\
                      \                 [--cache RULE [RULE ...]] [--snakefile FILE]\
                      \ [--cores [N]]\n                 [--local-cores N] [--resources\
                      \ [NAME=INT [NAME=INT ...]]]\n                 [--set-threads\
                      \ [RULE=THREADS [RULE=THREADS ...]]]\n                 [--default-resources\
                      \ [NAME=INT [NAME=INT ...]]]\n                 [--config [KEY=VALUE\
                      \ [KEY=VALUE ...]]]\n                 [--configfile FILE [FILE\
                      \ ...]] [--directory DIR] [--touch]\n                 [--keep-going]\
                      \ [--force] [--forceall]\n                 [--forcerun [TARGET\
                      \ [TARGET ...]]]\n                 [--prioritize TARGET [TARGET\
                      \ ...]]\n                 [--batch RULE=BATCH/BATCHES] [--until\
                      \ TARGET [TARGET ...]]\n                 [--omit-from TARGET\
                      \ [TARGET ...]] [--rerun-incomplete]\n                 [--shadow-prefix\
                      \ DIR] [--report [HTMLFILE]]\n                 [--lint [{text,json}]]\
                      \ [--export-cwl FILE] [--list]\n                 [--list-target-rules]\
                      \ [--dag] [--rulegraph] [--filegraph]\n                 [--d3dag]\
                      \ [--summary] [--detailed-summary] [--archive FILE]\n      \
                      \           [--cleanup-metadata FILE [FILE ...]] [--cleanup-shadow]\n\
                      \                 [--skip-script-cleanup] [--unlock] [--list-version-changes]\n\
                      \                 [--list-code-changes] [--list-input-changes]\n\
                      \                 [--list-params-changes] [--list-untracked]\n\
                      \                 [--delete-all-output] [--delete-temp-output]\n\
                      \                 [--bash-completion] [--keep-incomplete] [--version]\n\
                      \                 [--reason] [--gui [PORT]] [--printshellcmds]\
                      \ [--debug-dag]\n                 [--stats FILE] [--nocolor]\
                      \ [--quiet] [--print-compilation]\n                 [--verbose]\
                      \ [--force-use-threads] [--allow-ambiguity]\n              \
                      \   [--nolock] [--ignore-incomplete] [--latency-wait SECONDS]\n\
                      \                 [--wait-for-files [FILE [FILE ...]]] [--notemp]\n\
                      \                 [--keep-remote] [--keep-target-files]\n  \
                      \               [--allowed-rules ALLOWED_RULES [ALLOWED_RULES\
                      \ ...]]\n                 [--max-jobs-per-second MAX_JOBS_PER_SECOND]\n\
                      \                 [--max-status-checks-per-second MAX_STATUS_CHECKS_PER_SECOND]\n\
                      \                 [--restart-times RESTART_TIMES] [--attempt\
                      \ ATTEMPT]\n                 [--wrapper-prefix WRAPPER_PREFIX]\n\
                      \                 [--default-remote-provider {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp,iRODS}]\n\
                      \                 [--default-remote-prefix DEFAULT_REMOTE_PREFIX]\n\
                      \                 [--no-shared-fs] [--greediness GREEDINESS]\
                      \ [--no-hooks]\n                 [--overwrite-shellcmd OVERWRITE_SHELLCMD]\
                      \ [--debug]\n                 [--runtime-profile FILE] [--mode\
                      \ {0,1,2}]\n                 [--show-failed-logs] [--log-handler-script\
                      \ FILE]\n                 [--log-service {none,slack}]\n   \
                      \              [--cluster CMD | --cluster-sync CMD | --drmaa\
                      \ [ARGS]]\n                 [--cluster-config FILE] [--immediate-submit]\n\
                      \                 [--jobscript SCRIPT] [--jobname NAME]\n  \
                      \               [--cluster-status CLUSTER_STATUS] [--drmaa-log-dir\
                      \ DIR]\n                 [--kubernetes [NAMESPACE]] [--container-image\
                      \ IMAGE]\n                 [--tibanna] [--tibanna-sfn TIBANNA_SFN]\n\
                      \                 [--precommand PRECOMMAND]\n              \
                      \   [--tibanna-config TIBANNA_CONFIG [TIBANNA_CONFIG ...]]\n\
                      \                 [--use-conda] [--list-conda-envs] [--cleanup-conda]\n\
                      \                 [--conda-prefix DIR] [--create-envs-only]\
                      \ [--use-singularity]\n                 [--singularity-prefix\
                      \ DIR] [--singularity-args ARGS]\n                 [--use-envmodules]\n\
                      \                 [target [target ...]]\n\nSnakemake is a Python\
                      \ based language and execution environment for GNU Make-\nlike\
                      \ workflows.\n\noptional arguments:\n  -h, --help          \
                      \  show this help message and exit\n\nEXECUTION:\n  target \
                      \               Targets to build. May be rules or files.\n \
                      \ --dry-run, --dryrun, -n\n                        Do not execute\
                      \ anything, and display what would be\n                    \
                      \    done. If you have a very large workflow, use --dry-run\n\
                      \                        --quiet to just print a summary of\
                      \ the DAG of jobs.\n  --profile PROFILE     Name of profile\
                      \ to use for configuring Snakemake.\n                      \
                      \  Snakemake will search for a corresponding folder in\n   \
                      \                     /etc/xdg/snakemake and /root/.config/snakemake.\n\
                      \                        Alternatively, this can be an absolute\
                      \ or relative\n                        path. The profile folder\
                      \ has to contain a file\n                        'config.yaml'.\
                      \ This file can be used to set default\n                   \
                      \     values for command line options in YAML format. For\n\
                      \                        example, '--cluster qsub' becomes 'cluster:\
                      \ qsub' in\n                        the YAML file. Profiles\
                      \ can be obtained from\n                        https://github.com/snakemake-profiles.\n\
                      \  --cache RULE [RULE ...]\n                        Store output\
                      \ files of given rules in a central cache\n                \
                      \        given by the environment variable\n               \
                      \         $SNAKEMAKE_OUTPUT_CACHE. Likewise, retrieve output\n\
                      \                        files of the given rules from this\
                      \ cache if they have\n                        been created before\
                      \ (by anybody writing to the same\n                        cache),\
                      \ instead of actually executing the rules.\n               \
                      \         Output files are identified by hashing all steps,\n\
                      \                        parameters and software stack (conda\
                      \ envs or\n                        containers) needed to create\
                      \ them.\n  --snakefile FILE, -s FILE\n                     \
                      \   The workflow definition in form of a\n                 \
                      \       snakefile.Usually, you should not need to specify\n\
                      \                        this. By default, Snakemake will search\
                      \ for\n                        'Snakefile', 'snakefile', 'workflow/Snakefile',\n\
                      \                        'workflow/snakefile' beneath the current\
                      \ working\n                        directory, in this order.\
                      \ Only if you definitely want\n                        a different\
                      \ layout, you need to use this parameter.\n  --cores [N], --jobs\
                      \ [N], -j [N]\n                        Use at most N CPU cores/jobs\
                      \ in parallel. If N is\n                        omitted or 'all',\
                      \ the limit is set to the number of\n                      \
                      \  available CPU cores.\n  --local-cores N       In cluster\
                      \ mode, use at most N cores of the host\n                  \
                      \      machine in parallel (default: number of CPU cores of\n\
                      \                        the host). The cores are used to execute\
                      \ local rules.\n                        This option is ignored\
                      \ when not in cluster mode.\n  --resources [NAME=INT [NAME=INT\
                      \ ...]], --res [NAME=INT [NAME=INT ...]]\n                 \
                      \       Define additional resources that shall constrain the\n\
                      \                        scheduling analogously to threads (see\
                      \ above). A\n                        resource is defined as\
                      \ a name and an integer value.\n                        E.g.\
                      \ --resources gpu=1. Rules can use resources by\n          \
                      \              defining the resource keyword, e.g. resources:\
                      \ gpu=1.\n                        If now two rules require 1\
                      \ of the resource 'gpu' they\n                        won't\
                      \ be run in parallel by the scheduler.\n  --set-threads [RULE=THREADS\
                      \ [RULE=THREADS ...]]\n                        Overwrite thread\
                      \ usage of rules. This allows to fine-\n                   \
                      \     tune workflow parallelization. In particular, this is\n\
                      \                        helpful to target certain cluster nodes\
                      \ by e.g.\n                        shifting a rule to use more,\
                      \ or less threads than\n                        defined in the\
                      \ workflow. Thereby, THREADS has to be a\n                 \
                      \       positive integer, and RULE has to be the name of the\n\
                      \                        rule.\n  --default-resources [NAME=INT\
                      \ [NAME=INT ...]], --default-res [NAME=INT [NAME=INT ...]]\n\
                      \                        Define default values of resources\
                      \ for rules that do\n                        not define their\
                      \ own values. In addition to plain\n                       \
                      \ integers, python expressions over inputsize are\n        \
                      \                allowed (e.g. '2*input.size_mb').When specifying\
                      \ this\n                        without any arguments (--default-resources),\
                      \ it\n                        defines 'mem_mb=max(2*input.size_mb,\
                      \ 1000)'\n                        'disk_mb=max(2*input.size_mb,\
                      \ 1000)', i.e., default\n                        disk and mem\
                      \ usage is twice the input file size but at\n              \
                      \          least 1GB.\n  --config [KEY=VALUE [KEY=VALUE ...]],\
                      \ -C [KEY=VALUE [KEY=VALUE ...]]\n                        Set\
                      \ or overwrite values in the workflow config object.\n     \
                      \                   The workflow config object is accessible\
                      \ as variable\n                        config inside the workflow.\
                      \ Default values can be set\n                        by providing\
                      \ a JSON file (see Documentation).\n  --configfile FILE [FILE\
                      \ ...], --configfiles FILE [FILE ...]\n                    \
                      \    Specify or overwrite the config file of the workflow\n\
                      \                        (see the docs). Values specified in\
                      \ JSON or YAML\n                        format are available\
                      \ in the global config dictionary\n                        inside\
                      \ the workflow. Multiple files overwrite each\n            \
                      \            other in the given order.\n  --directory DIR, -d\
                      \ DIR\n                        Specify working directory (relative\
                      \ paths in the\n                        snakefile will use this\
                      \ as their origin).\n  --touch, -t           Touch output files\
                      \ (mark them up to date without\n                        really\
                      \ changing them) instead of running their\n                \
                      \        commands. This is used to pretend that the rules were\n\
                      \                        executed, in order to fool future invocations\
                      \ of\n                        snakemake. Fails if a file does\
                      \ not yet exist. Note\n                        that this will\
                      \ only touch files that would otherwise\n                  \
                      \      be recreated by Snakemake (e.g. because their input\n\
                      \                        files are newer). For enforcing a touch,\
                      \ combine this\n                        with --force, --forceall,\
                      \ or --forcerun. Note however\n                        that\
                      \ you loose the provenance information when the\n          \
                      \              files have been created in realitiy. Hence, this\n\
                      \                        should be used only as a last resort.\n\
                      \  --keep-going, -k      Go on with independent jobs if a job\
                      \ fails.\n  --force, -f           Force the execution of the\
                      \ selected target or the\n                        first rule\
                      \ regardless of already created output.\n  --forceall, -F  \
                      \      Force the execution of the selected (or the first)\n\
                      \                        rule and all rules it is dependent\
                      \ on regardless of\n                        already created\
                      \ output.\n  --forcerun [TARGET [TARGET ...]], -R [TARGET [TARGET\
                      \ ...]]\n                        Force the re-execution or creation\
                      \ of the given rules\n                        or files. Use\
                      \ this option if you changed a rule and\n                  \
                      \      want to have all its output in your workflow updated.\n\
                      \  --prioritize TARGET [TARGET ...], -P TARGET [TARGET ...]\n\
                      \                        Tell the scheduler to assign creation\
                      \ of given targets\n                        (and all their dependencies)\
                      \ highest priority.\n                        (EXPERIMENTAL)\n\
                      \  --batch RULE=BATCH/BATCHES\n                        Only\
                      \ create the given BATCH of the input files of the\n       \
                      \                 given RULE. This can be used to iteratively\
                      \ run parts\n                        of very large workflows.\
                      \ Only the execution plan of\n                        the relevant\
                      \ part of the workflow has to be\n                        calculated,\
                      \ thereby speeding up DAG computation. It is\n             \
                      \           recommended to provide the most suitable rule for\n\
                      \                        batching when documenting a workflow.\
                      \ It should be\n                        some aggregating rule\
                      \ that would be executed only\n                        once,\
                      \ and has a large number of input files. For\n             \
                      \           example, it can be a rule that aggregates over\n\
                      \                        samples.\n  --until TARGET [TARGET\
                      \ ...], -U TARGET [TARGET ...]\n                        Runs\
                      \ the pipeline until it reaches the specified rules\n      \
                      \                  or files. Only runs jobs that are dependencies\
                      \ of the\n                        specified rule or files, does\
                      \ not run sibling DAGs.\n  --omit-from TARGET [TARGET ...],\
                      \ -O TARGET [TARGET ...]\n                        Prevent the\
                      \ execution or creation of the given rules\n               \
                      \         or files as well as any rules or files that are\n\
                      \                        downstream of these targets in the\
                      \ DAG. Also runs jobs\n                        in sibling DAGs\
                      \ that are independent of the rules or\n                   \
                      \     files specified here.\n  --rerun-incomplete, --ri\n  \
                      \                      Re-run all jobs the output of which is\
                      \ recognized as\n                        incomplete.\n  --shadow-prefix\
                      \ DIR   Specify a directory in which the 'shadow' directory\
                      \ is\n                        created. If not supplied, the\
                      \ value is set to the\n                        '.snakemake'\
                      \ directory relative to the working\n                      \
                      \  directory.\n\nUTILITIES:\n  --report [HTMLFILE]   Create\
                      \ an HTML report with results and statistics. If\n         \
                      \               no filename is given, report.html is the default.\n\
                      \  --lint [{text,json}]  Perform linting on the given workflow.\
                      \ This will print\n                        snakemake specific\
                      \ suggestions to improve code quality\n                    \
                      \    (work in progress, more lints to be added in the\n    \
                      \                    future). If no argument is provided, plain\
                      \ text output\n                        is used.\n  --export-cwl\
                      \ FILE     Compile workflow to CWL and store it in given FILE.\n\
                      \  --list, -l            Show available rules in given Snakefile.\n\
                      \  --list-target-rules, --lt\n                        Show available\
                      \ target rules in given Snakefile.\n  --dag                \
                      \ Do not execute anything and print the directed acyclic\n \
                      \                       graph of jobs in the dot language. Recommended\
                      \ use on\n                        Unix systems: snakemake --dag\
                      \ | dot | display\n  --rulegraph           Do not execute anything\
                      \ and print the dependency graph\n                        of\
                      \ rules in the dot language. This will be less\n           \
                      \             crowded than above DAG of jobs, but also show\
                      \ less\n                        information. Note that each\
                      \ rule is displayed once,\n                        hence the\
                      \ displayed graph will be cyclic if a rule\n               \
                      \         appears in several steps of the workflow. Use this\
                      \ if\n                        above option leads to a DAG that\
                      \ is too large.\n                        Recommended use on\
                      \ Unix systems: snakemake --rulegraph\n                    \
                      \    | dot | display\n  --filegraph           Do not execute\
                      \ anything and print the dependency graph\n                \
                      \        of rules with their input and output files in the dot\n\
                      \                        language. This is an intermediate solution\
                      \ between\n                        above DAG of jobs and the\
                      \ rule graph. Note that each\n                        rule is\
                      \ displayed once, hence the displayed graph will\n         \
                      \               be cyclic if a rule appears in several steps\
                      \ of the\n                        workflow. Use this if above\
                      \ option leads to a DAG that\n                        is too\
                      \ large. Recommended use on Unix systems:\n                \
                      \        snakemake --filegraph | dot | display\n  --d3dag  \
                      \             Print the DAG in D3.js compatible JSON format.\n\
                      \  --summary, -S         Print a summary of all files created\
                      \ by the workflow.\n                        The has the following\
                      \ columns: filename, modification\n                        time,\
                      \ rule version, status, plan. Thereby rule version\n       \
                      \                 contains the versionthe file was created with\
                      \ (see the\n                        version keyword of rules),\
                      \ and status denotes whether\n                        the file\
                      \ is missing, its input files are newer or if\n            \
                      \            version or implementation of the rule changed since\n\
                      \                        file creation. Finally the last column\
                      \ denotes whether\n                        the file will be\
                      \ updated or created during the next\n                     \
                      \   workflow execution.\n  --detailed-summary, -D\n        \
                      \                Print a summary of all files created by the\
                      \ workflow.\n                        The has the following columns:\
                      \ filename, modification\n                        time, rule\
                      \ version, input file(s), shell command,\n                 \
                      \       status, plan. Thereby rule version contains the\n  \
                      \                      version the file was created with (see\
                      \ the version\n                        keyword of rules), and\
                      \ status denotes whether the file\n                        is\
                      \ missing, its input files are newer or if version or\n    \
                      \                    implementation of the rule changed since\
                      \ file\n                        creation. The input file and\
                      \ shell command columns are\n                        self explanatory.\
                      \ Finally the last column denotes\n                        whether\
                      \ the file will be updated or created during the\n         \
                      \               next workflow execution.\n  --archive FILE \
                      \       Archive the workflow into the given tar archive FILE.\n\
                      \                        The archive will be created such that\
                      \ the workflow can\n                        be re-executed on\
                      \ a vanilla system. The function needs\n                   \
                      \     conda and git to be installed. It will archive every\n\
                      \                        file that is under git version control.\
                      \ Note that it\n                        is best practice to\
                      \ have the Snakefile, config files,\n                      \
                      \  and scripts under version control. Hence, they will be\n\
                      \                        included in the archive. Further, it\
                      \ will add input\n                        files that are not\
                      \ generated by by the workflow itself\n                    \
                      \    and conda environments. Note that symlinks are\n      \
                      \                  dereferenced. Supported formats are .tar,\
                      \ .tar.gz,\n                        .tar.bz2 and .tar.xz.\n\
                      \  --cleanup-metadata FILE [FILE ...], --cm FILE [FILE ...]\n\
                      \                        Cleanup the metadata of given files.\
                      \ That means that\n                        snakemake removes\
                      \ any tracked version info, and any\n                      \
                      \  marks that files are incomplete.\n  --cleanup-shadow    \
                      \  Cleanup old shadow directories which have not been\n    \
                      \                    deleted due to failures or power loss.\n\
                      \  --skip-script-cleanup\n                        Don't delete\
                      \ wrapper scripts used for execution\n  --unlock           \
                      \   Remove a lock on the working directory.\n  --list-version-changes,\
                      \ --lv\n                        List all output files that have\
                      \ been created with a\n                        different version\
                      \ (as determined by the version\n                        keyword).\n\
                      \  --list-code-changes, --lc\n                        List all\
                      \ output files for which the rule body (run or\n           \
                      \             shell) have changed in the Snakefile.\n  --list-input-changes,\
                      \ --li\n                        List all output files for which\
                      \ the defined input\n                        files have changed\
                      \ in the Snakefile (e.g. new input\n                       \
                      \ files were added in the rule definition or files were\n  \
                      \                      renamed). For listing input file modification\
                      \ in the\n                        filesystem, use --summary.\n\
                      \  --list-params-changes, --lp\n                        List\
                      \ all output files for which the defined params\n          \
                      \              have changed in the Snakefile.\n  --list-untracked,\
                      \ --lu\n                        List all files in the working\
                      \ directory that are not\n                        used in the\
                      \ workflow. This can be used e.g. for\n                    \
                      \    identifying leftover files. Hidden files and\n        \
                      \                directories are ignored.\n  --delete-all-output\
                      \   Remove all files generated by the workflow. Use\n      \
                      \                  together with --dry-run to list files without\
                      \ actually\n                        deleting anything. Note\
                      \ that this will not recurse\n                        into subworkflows.\
                      \ Write-protected files are not\n                        removed.\
                      \ Nevertheless, use with care!\n  --delete-temp-output  Remove\
                      \ all temporary files generated by the workflow.\n         \
                      \               Use together with --dry-run to list files without\n\
                      \                        actually deleting anything. Note that\
                      \ this will not\n                        recurse into subworkflows.\n\
                      \  --bash-completion     Output code to register bash completion\
                      \ for snakemake.\n                        Put the following\
                      \ in your .bashrc (including the\n                        accents):\
                      \ `snakemake --bash-completion` or issue it in\n           \
                      \             an open terminal session.\n  --keep-incomplete\
                      \     Do not remove incomplete output files by failed jobs.\n\
                      \  --version, -v         show program's version number and exit\n\
                      \nOUTPUT:\n  --reason, -r          Print the reason for each\
                      \ executed rule.\n  --gui [PORT]          Serve an HTML based\
                      \ user interface to the given\n                        network\
                      \ and port e.g. 168.129.10.15:8000. By default\n           \
                      \             Snakemake is only available in the local network\n\
                      \                        (default port: 8000). To make Snakemake\
                      \ listen to all\n                        ip addresses add the\
                      \ special host address 0.0.0.0 to\n                        the\
                      \ url (0.0.0.0:8000). This is important if Snakemake\n     \
                      \                   is used in a virtualised environment like\
                      \ Docker. If\n                        possible, a browser window\
                      \ is opened.\n  --printshellcmds, -p  Print out the shell commands\
                      \ that will be executed.\n  --debug-dag           Print candidate\
                      \ and selected jobs (including their\n                     \
                      \   wildcards) while inferring DAG. This can help to debug\n\
                      \                        unexpected DAG topology or errors.\n\
                      \  --stats FILE          Write stats about Snakefile execution\
                      \ in JSON format\n                        to the given file.\n\
                      \  --nocolor             Do not use a colored output.\n  --quiet,\
                      \ -q           Do not output any progress or rule information.\n\
                      \  --print-compilation   Print the python representation of\
                      \ the workflow.\n  --verbose             Print debugging output.\n\
                      \nBEHAVIOR:\n  --force-use-threads   Force threads rather than\
                      \ processes. Helpful if shared\n                        memory\
                      \ (/dev/shm) is full or unavailable.\n  --allow-ambiguity, -a\n\
                      \                        Don't check for ambiguous rules and\
                      \ simply use the\n                        first if several can\
                      \ produce the same file. This\n                        allows\
                      \ the user to prioritize rules by their order in\n         \
                      \               the snakefile.\n  --nolock              Do not\
                      \ lock the working directory\n  --ignore-incomplete, --ii\n\
                      \                        Do not check for incomplete output\
                      \ files.\n  --latency-wait SECONDS, --output-wait SECONDS, -w\
                      \ SECONDS\n                        Wait given seconds if an\
                      \ output file of a job is not\n                        present\
                      \ after the job finished. This helps if your\n             \
                      \           filesystem suffers from latency (default 5).\n \
                      \ --wait-for-files [FILE [FILE ...]]\n                     \
                      \   Wait --latency-wait seconds for these files to be\n    \
                      \                    present before executing the workflow.\
                      \ This option is\n                        used internally to\
                      \ handle filesystem latency in\n                        cluster\
                      \ environments.\n  --notemp, --nt        Ignore temp() declarations.\
                      \ This is useful when\n                        running only\
                      \ a part of the workflow, since temp()\n                   \
                      \     would lead to deletion of probably needed files by\n \
                      \                       other parts of the workflow.\n  --keep-remote\
                      \         Keep local copies of remote input files.\n  --keep-target-files\
                      \   Do not adjust the paths of given target files relative\n\
                      \                        to the working directory.\n  --allowed-rules\
                      \ ALLOWED_RULES [ALLOWED_RULES ...]\n                      \
                      \  Only consider given rules. If omitted, all rules in\n   \
                      \                     Snakefile are used. Note that this is\
                      \ intended\n                        primarily for internal use\
                      \ and may lead to unexpected\n                        results\
                      \ otherwise.\n  --max-jobs-per-second MAX_JOBS_PER_SECOND\n\
                      \                        Maximal number of cluster/drmaa jobs\
                      \ per second,\n                        default is 10, fractions\
                      \ allowed.\n  --max-status-checks-per-second MAX_STATUS_CHECKS_PER_SECOND\n\
                      \                        Maximal number of job status checks\
                      \ per second,\n                        default is 10, fractions\
                      \ allowed.\n  --restart-times RESTART_TIMES\n              \
                      \          Number of times to restart failing jobs (defaults\
                      \ to\n                        0).\n  --attempt ATTEMPT     Internal\
                      \ use only: define the initial value of the\n              \
                      \          attempt parameter (default: 1).\n  --wrapper-prefix\
                      \ WRAPPER_PREFIX\n                        Prefix for URL created\
                      \ from wrapper directive\n                        (default:\
                      \ https://github.com/snakemake/snakemake-\n                \
                      \        wrappers/raw/). Set this to a different URL to use\n\
                      \                        your fork or a local clone of the repository,\
                      \ e.g.,\n                        use a git URL like\n      \
                      \                  'git+file://path/to/your/local/clone@'.\n\
                      \  --default-remote-provider {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp,iRODS}\n\
                      \                        Specify default remote provider to\
                      \ be used for all\n                        input and output\
                      \ files that don't yet specify one.\n  --default-remote-prefix\
                      \ DEFAULT_REMOTE_PREFIX\n                        Specify prefix\
                      \ for default remote provider. E.g. a\n                    \
                      \    bucket name.\n  --no-shared-fs        Do not assume that\
                      \ jobs share a common file system.\n                       \
                      \ When this flag is activated, Snakemake will assume\n     \
                      \                   that the filesystem on a cluster node is\
                      \ not shared\n                        with other nodes. For\
                      \ example, this will lead to\n                        downloading\
                      \ remote files on each cluster node\n                      \
                      \  separately. Further, it won't take special measures to\n\
                      \                        deal with filesystem latency issues.\
                      \ This option will\n                        in most cases only\
                      \ make sense in combination with\n                        --default-remote-provider.\
                      \ Further, when using\n                        --cluster you\
                      \ will have to also provide --cluster-\n                   \
                      \     status. Only activate this if you know what you are\n\
                      \                        doing.\n  --greediness GREEDINESS\n\
                      \                        Set the greediness of scheduling. This\
                      \ value between 0\n                        and 1 determines\
                      \ how careful jobs are selected for\n                      \
                      \  execution. The default value (1.0) provides the best\n  \
                      \                      speed and still acceptable scheduling\
                      \ quality.\n  --no-hooks            Do not invoke onstart, onsuccess\
                      \ or onerror hooks\n                        after execution.\n\
                      \  --overwrite-shellcmd OVERWRITE_SHELLCMD\n               \
                      \         Provide a shell command that shall be executed instead\n\
                      \                        of those given in the workflow. This\
                      \ is for debugging\n                        purposes only.\n\
                      \  --debug               Allow to debug rules with e.g. PDB.\
                      \ This flag allows\n                        to set breakpoints\
                      \ in run blocks.\n  --runtime-profile FILE\n               \
                      \         Profile Snakemake and write the output to FILE. This\n\
                      \                        requires yappi to be installed.\n \
                      \ --mode {0,1,2}        Set execution mode of Snakemake (internal\
                      \ use only).\n  --show-failed-logs    Automatically display\
                      \ logs of failed jobs.\n  --log-handler-script FILE\n      \
                      \                  Provide a custom script containing a function\
                      \ 'def\n                        log_handler(msg):'. Snakemake\
                      \ will call this function\n                        for every\
                      \ logging output (given as a dictionary\n                  \
                      \      msg)allowing to e.g. send notifications in the form of\n\
                      \                        e.g. slack messages or emails.\n  --log-service\
                      \ {none,slack}\n                        Set a specific messaging\
                      \ service for logging\n                        output.Snakemake\
                      \ will notify the service on errors and\n                  \
                      \      completed execution.Currently only slack is supported.\n\
                      \nCLUSTER:\n  --cluster CMD, -c CMD\n                      \
                      \  Execute snakemake rules with the given submit command,\n\
                      \                        e.g. qsub. Snakemake compiles jobs\
                      \ into scripts that\n                        are submitted to\
                      \ the cluster with the given command,\n                    \
                      \    once all input files for a particular job are present.\n\
                      \                        The submit command can be decorated\
                      \ to make it aware\n                        of certain job properties\
                      \ (name, rulename, input,\n                        output, params,\
                      \ wildcards, log, threads and\n                        dependencies\
                      \ (see the argument below)), e.g.: $\n                     \
                      \   snakemake --cluster 'qsub -pe threaded {threads}'.\n  --cluster-sync\
                      \ CMD    cluster submission command will block, returning the\n\
                      \                        remote exitstatus upon remote termination\
                      \ (for\n                        example, this should be usedif\
                      \ the cluster command is\n                        'qsub -sync\
                      \ y' (SGE)\n  --drmaa [ARGS]        Execute snakemake on a cluster\
                      \ accessed via DRMAA,\n                        Snakemake compiles\
                      \ jobs into scripts that are\n                        submitted\
                      \ to the cluster with the given command, once\n            \
                      \            all input files for a particular job are present.\
                      \ ARGS\n                        can be used to specify options\
                      \ of the underlying\n                        cluster system,\
                      \ thereby using the job properties name,\n                 \
                      \       rulename, input, output, params, wildcards, log,\n \
                      \                       threads and dependencies, e.g.: --drmaa\
                      \ ' -pe threaded\n                        {threads}'. Note that\
                      \ ARGS must be given in quotes and\n                       \
                      \ with a leading whitespace.\n  --cluster-config FILE, -u FILE\n\
                      \                        A JSON or YAML file that defines the\
                      \ wildcards used in\n                        'cluster'for specific\
                      \ rules, instead of having them\n                        specified\
                      \ in the Snakefile. For example, for rule\n                \
                      \        'job' you may define: { 'job' : { 'time' : '24:00:00'\n\
                      \                        } } to specify the time for rule 'job'.\
                      \ You can\n                        specify more than one file.\
                      \ The configuration files\n                        are merged\
                      \ with later values overriding earlier ones.\n             \
                      \           This option is deprecated in favor of using --profile,\n\
                      \                        see docs.\n  --immediate-submit, --is\n\
                      \                        Immediately submit all jobs to the\
                      \ cluster instead of\n                        waiting for present\
                      \ input files. This will fail,\n                        unless\
                      \ you make the cluster aware of job dependencies,\n        \
                      \                e.g. via: $ snakemake --cluster 'sbatch --dependency\n\
                      \                        {dependencies}. Assuming that your\
                      \ submit script (here\n                        sbatch) outputs\
                      \ the generated job id to the first\n                      \
                      \  stdout line, {dependencies} will be filled with space\n \
                      \                       separated job ids this job depends on.\n\
                      \  --jobscript SCRIPT, --js SCRIPT\n                       \
                      \ Provide a custom job script for submission to the\n      \
                      \                  cluster. The default script resides as 'jobscript.sh'\n\
                      \                        in the installation directory.\n  --jobname\
                      \ NAME, --jn NAME\n                        Provide a custom\
                      \ name for the jobscript that is\n                        submitted\
                      \ to the cluster (see --cluster). NAME is\n                \
                      \        \"snakejob.{name}.{jobid}.sh\" per default. The wildcard\n\
                      \                        {jobid} has to be present in the name.\n\
                      \  --cluster-status CLUSTER_STATUS\n                       \
                      \ Status command for cluster execution. This is only\n     \
                      \                   considered in combination with the --cluster\
                      \ flag. If\n                        provided, Snakemake will\
                      \ use the status command to\n                        determine\
                      \ if a job has finished successfully or\n                  \
                      \      failed. For this it is necessary that the submit\n  \
                      \                      command provided to --cluster returns\
                      \ the cluster job\n                        id. Then, the status\
                      \ command will be invoked with the\n                       \
                      \ job id. Snakemake expects it to return 'success' if\n    \
                      \                    the job was successfull, 'failed' if the\
                      \ job failed\n                        and 'running' if the job\
                      \ still runs.\n  --drmaa-log-dir DIR   Specify a directory in\
                      \ which stdout and stderr files\n                        of\
                      \ DRMAA jobs will be written. The value may be given\n     \
                      \                   as a relative path, in which case Snakemake\
                      \ will use\n                        the current invocation directory\
                      \ as the origin. If\n                        given, this will\
                      \ override any given '-o' and/or '-e'\n                    \
                      \    native specification. If not given, all DRMAA stdout\n\
                      \                        and stderr files are written to the\
                      \ current working\n                        directory.\n\nKUBERNETES:\n\
                      \  --kubernetes [NAMESPACE]\n                        Execute\
                      \ workflow in a kubernetes cluster (in the\n               \
                      \         cloud). NAMESPACE is the namespace you want to use\
                      \ for\n                        your job (if nothing specified:\
                      \ 'default'). Usually,\n                        this requires\
                      \ --default-remote-provider and --default-\n               \
                      \         remote-prefix to be set to a S3 or GS bucket where\n\
                      \                        your . data shall be stored. It is\
                      \ further advisable\n                        to activate conda\
                      \ integration via --use-conda.\n  --container-image IMAGE\n\
                      \                        Docker image to use, e.g., when submitting\
                      \ jobs to\n                        kubernetes. By default, this\
                      \ is\n                        'https://hub.docker.com/r/snakemake/snakemake',\
                      \ tagged\n                        with the same version as the\
                      \ currently running\n                        Snakemake instance.\
                      \ Note that overwriting this value\n                       \
                      \ is up to your responsibility. Any used image has to\n    \
                      \                    contain a working snakemake installation\
                      \ that is\n                        compatible with (or ideally\
                      \ the same as) the currently\n                        running\
                      \ version.\n\nTIBANNA:\n  --tibanna             Execute workflow\
                      \ on AWS cloud using Tibanna. This\n                       \
                      \ requires --default-remote-prefix to be set to S3\n       \
                      \                 bucket name and prefix (e.g.\n           \
                      \             'bucketname/subdirectory') where input is already\n\
                      \                        stored and output will be sent to.\
                      \ Using --tibanna\n                        implies --default-resources\
                      \ is set as default.\n                        Optionally, use\
                      \ --precommand to specify any\n                        preparation\
                      \ command to run before snakemake command on\n             \
                      \           the cloud (inside snakemake container on Tibanna\
                      \ VM).\n                        Also, --use-conda, --use-singularity,\
                      \ --config,\n                        --configfile are supported\
                      \ and will be carried over.\n  --tibanna-sfn TIBANNA_SFN\n \
                      \                       Name of Tibanna Unicorn step function\
                      \ (e.g.\n                        tibanna_unicorn_monty).This\
                      \ works as serverless\n                        scheduler/resource\
                      \ allocator and must be deployed\n                        first\
                      \ using tibanna cli. (e.g. tibanna deploy_unicorn\n        \
                      \                --usergroup=monty --buckets=bucketname)\n \
                      \ --precommand PRECOMMAND\n                        Any command\
                      \ to execute before snakemake command on AWS\n             \
                      \           cloud such as wget, git clone, unzip, etc. This\
                      \ is\n                        used with --tibanna.Do not include\
                      \ input/output\n                        download/upload commands\
                      \ - file transfer between S3\n                        bucket\
                      \ and the run environment (container) is\n                 \
                      \       automatically handled by Tibanna.\n  --tibanna-config\
                      \ TIBANNA_CONFIG [TIBANNA_CONFIG ...]\n                    \
                      \    Additional tibanan config e.g. --tibanna-config\n     \
                      \                   spot_instance=true subnet=<subnet_id> security\n\
                      \                        group=<security_group_id>\n\nCONDA:\n\
                      \  --use-conda           If defined in the rule, run job in\
                      \ a conda\n                        environment. If this flag\
                      \ is not set, the conda\n                        directive is\
                      \ ignored.\n  --list-conda-envs     List all conda environments\
                      \ and their location on\n                        disk.\n  --cleanup-conda\
                      \       Cleanup unused conda environments.\n  --conda-prefix\
                      \ DIR    Specify a directory in which the 'conda' and 'conda-\n\
                      \                        archive' directories are created. These\
                      \ are used to\n                        store conda environments\
                      \ and their archives,\n                        respectively.\
                      \ If not supplied, the value is set to the\n               \
                      \         '.snakemake' directory relative to the invocation\n\
                      \                        directory. If supplied, the `--use-conda`\
                      \ flag must\n                        also be set. The value\
                      \ may be given as a relative\n                        path,\
                      \ which will be extrapolated to the invocation\n           \
                      \             directory, or as an absolute path.\n  --create-envs-only\
                      \    If specified, only creates the job-specific conda\n   \
                      \                     environments then exits. The `--use-conda`\
                      \ flag must\n                        also be set.\n\nSINGULARITY:\n\
                      \  --use-singularity     If defined in the rule, run job within\
                      \ a singularity\n                        container. If this\
                      \ flag is not set, the singularity\n                       \
                      \ directive is ignored.\n  --singularity-prefix DIR\n      \
                      \                  Specify a directory in which singularity\
                      \ images will\n                        be stored.If not supplied,\
                      \ the value is set to the\n                        '.snakemake'\
                      \ directory relative to the invocation\n                   \
                      \     directory. If supplied, the `--use-singularity` flag\n\
                      \                        must also be set. The value may be\
                      \ given as a relative\n                        path, which will\
                      \ be extrapolated to the invocation\n                      \
                      \  directory, or as an absolute path.\n  --singularity-args\
                      \ ARGS\n                        Pass additional args to singularity.\n\
                      \nENVIRONMENT MODULES:\n  --use-envmodules      If defined in\
                      \ the rule, run job within the given\n                     \
                      \   environment modules, loaded in the given order. This\n \
                      \                       can be combined with --use-conda and\
                      \ --use-\n                        singularity, which will then\
                      \ be only used as a\n                        fallback for rules\
                      \ which don't define environment\n                        modules.\n"
                    generated_using: *id004
                    docker_image:
                  subcommands: []
                  usage: []
                  help_flag: !Flag
                    optional: true
                    synonyms:
                    - --help
                    description: show Snakemake help (or snakemake -h)
                    args: !EmptyFlagArg {}
                  usage_flag:
                  version_flag:
                  help_text: "ERROR\ntarget: No such file or directory\nProphane Pipeline\
                    \ (powered by Snakemake)\n\nUsage: /usr/local/bin/prophane CONFIG_FILE\
                    \ [Snakemake options]\n\n Full list of parameters:\n   --help\
                    \                 show Snakemake help (or snakemake -h)\n   --list-dbs\
                    \             print list of configured databases\n           \
                    \               databases are looked up in 'db_base_dir' configured\
                    \ in:\n                              /usr/local/opt/prophane/general_config.yaml\n\
                    \   --list-styles          print list of available input file\
                    \ styles\n                          styles are looked up in the\
                    \ following folder:\n                              /usr/local/opt/prophane/styles\n\
                    \   --db-maintenance       trigger database maintenance scripts\n\
                    \                          will migrate database structure from\
                    \ deprecated structure to most recent\n\n Useful Snakemake parameters:\n\
                    \   -j, --cores            number of cores\n   -k, --keep-going\
                    \       go on with independent jobs if a job fails\n   -n, --dryrun\
                    \           do not execute anything\n   -p, --printshellcmds \
                    \  print out the shell commands that will be executed\n   -t,\
                    \ --timestamp  \t\tadd a timestamp to all logging output\n\n"
                  generated_using: *id004
                  docker_image:
                - !Command
                  command: *id005
                  positional: []
                  named:
                  - !Flag
                    optional: true
                    synonyms:
                    - --list-dbs
                    description: "print list of configured databases\ndatabases are\
                      \ looked up in 'db_base_dir' configured in:\n/usr/local/opt/prophane/general_config.yaml"
                    args: !EmptyFlagArg {}
                  - !Flag
                    optional: true
                    synonyms:
                    - --list-styles
                    description: "print list of available input file styles\nstyles\
                      \ are looked up in the following folder:\n/usr/local/opt/prophane/styles"
                    args: !EmptyFlagArg {}
                  - !Flag
                    optional: true
                    synonyms:
                    - --db-maintenance
                    description: "trigger database maintenance scripts\nwill migrate\
                      \ database structure from deprecated structure to most recent"
                    args: !EmptyFlagArg {}
                  - !Flag
                    optional: true
                    synonyms:
                    - -j
                    - --cores
                    description: number of cores
                    args: !EmptyFlagArg {}
                  - !Flag
                    optional: true
                    synonyms:
                    - -k
                    - --keep-going
                    description: go on with independent jobs if a job fails
                    args: !EmptyFlagArg {}
                  - !Flag
                    optional: true
                    synonyms:
                    - -n
                    - --dryrun
                    description: do not execute anything
                    args: !EmptyFlagArg {}
                  - !Flag
                    optional: true
                    synonyms:
                    - -p
                    - --printshellcmds
                    description: print out the shell commands that will be executed
                    args: !EmptyFlagArg {}
                  - !Flag
                    optional: true
                    synonyms:
                    - -t
                    - --timestamp
                    description: add a timestamp to all logging output
                    args: !EmptyFlagArg {}
                  parent: *id003
                  subcommands: []
                  usage: []
                  help_flag: !Flag
                    optional: true
                    synonyms:
                    - --help
                    description: show Snakemake help (or snakemake -h)
                    args: !EmptyFlagArg {}
                  usage_flag:
                  version_flag:
                  help_text: "ERROR\ndisk.: No such file or directory\nProphane Pipeline\
                    \ (powered by Snakemake)\n\nUsage: /usr/local/bin/prophane CONFIG_FILE\
                    \ [Snakemake options]\n\n Full list of parameters:\n   --help\
                    \                 show Snakemake help (or snakemake -h)\n   --list-dbs\
                    \             print list of configured databases\n           \
                    \               databases are looked up in 'db_base_dir' configured\
                    \ in:\n                              /usr/local/opt/prophane/general_config.yaml\n\
                    \   --list-styles          print list of available input file\
                    \ styles\n                          styles are looked up in the\
                    \ following folder:\n                              /usr/local/opt/prophane/styles\n\
                    \   --db-maintenance       trigger database maintenance scripts\n\
                    \                          will migrate database structure from\
                    \ deprecated structure to most recent\n\n Useful Snakemake parameters:\n\
                    \   -j, --cores            number of cores\n   -k, --keep-going\
                    \       go on with independent jobs if a job fails\n   -n, --dryrun\
                    \           do not execute anything\n   -p, --printshellcmds \
                    \  print out the shell commands that will be executed\n   -t,\
                    \ --timestamp  \t\tadd a timestamp to all logging output\n\n"
                  generated_using: *id004
                  docker_image:
                usage: []
                help_flag:
                usage_flag:
                version_flag:
                help_text: "usage: snakemake [-h] [--dry-run] [--profile PROFILE]\n\
                  \                 [--cache RULE [RULE ...]] [--snakefile FILE] [--cores\
                  \ [N]]\n                 [--local-cores N] [--resources [NAME=INT\
                  \ [NAME=INT ...]]]\n                 [--set-threads [RULE=THREADS\
                  \ [RULE=THREADS ...]]]\n                 [--default-resources [NAME=INT\
                  \ [NAME=INT ...]]]\n                 [--config [KEY=VALUE [KEY=VALUE\
                  \ ...]]]\n                 [--configfile FILE [FILE ...]] [--directory\
                  \ DIR] [--touch]\n                 [--keep-going] [--force] [--forceall]\n\
                  \                 [--forcerun [TARGET [TARGET ...]]]\n         \
                  \        [--prioritize TARGET [TARGET ...]]\n                 [--batch\
                  \ RULE=BATCH/BATCHES] [--until TARGET [TARGET ...]]\n          \
                  \       [--omit-from TARGET [TARGET ...]] [--rerun-incomplete]\n\
                  \                 [--shadow-prefix DIR] [--report [HTMLFILE]]\n\
                  \                 [--lint [{text,json}]] [--export-cwl FILE] [--list]\n\
                  \                 [--list-target-rules] [--dag] [--rulegraph] [--filegraph]\n\
                  \                 [--d3dag] [--summary] [--detailed-summary] [--archive\
                  \ FILE]\n                 [--cleanup-metadata FILE [FILE ...]] [--cleanup-shadow]\n\
                  \                 [--skip-script-cleanup] [--unlock] [--list-version-changes]\n\
                  \                 [--list-code-changes] [--list-input-changes]\n\
                  \                 [--list-params-changes] [--list-untracked]\n \
                  \                [--delete-all-output] [--delete-temp-output]\n\
                  \                 [--bash-completion] [--keep-incomplete] [--version]\n\
                  \                 [--reason] [--gui [PORT]] [--printshellcmds] [--debug-dag]\n\
                  \                 [--stats FILE] [--nocolor] [--quiet] [--print-compilation]\n\
                  \                 [--verbose] [--force-use-threads] [--allow-ambiguity]\n\
                  \                 [--nolock] [--ignore-incomplete] [--latency-wait\
                  \ SECONDS]\n                 [--wait-for-files [FILE [FILE ...]]]\
                  \ [--notemp]\n                 [--keep-remote] [--keep-target-files]\n\
                  \                 [--allowed-rules ALLOWED_RULES [ALLOWED_RULES\
                  \ ...]]\n                 [--max-jobs-per-second MAX_JOBS_PER_SECOND]\n\
                  \                 [--max-status-checks-per-second MAX_STATUS_CHECKS_PER_SECOND]\n\
                  \                 [--restart-times RESTART_TIMES] [--attempt ATTEMPT]\n\
                  \                 [--wrapper-prefix WRAPPER_PREFIX]\n          \
                  \       [--default-remote-provider {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp,iRODS}]\n\
                  \                 [--default-remote-prefix DEFAULT_REMOTE_PREFIX]\n\
                  \                 [--no-shared-fs] [--greediness GREEDINESS] [--no-hooks]\n\
                  \                 [--overwrite-shellcmd OVERWRITE_SHELLCMD] [--debug]\n\
                  \                 [--runtime-profile FILE] [--mode {0,1,2}]\n  \
                  \               [--show-failed-logs] [--log-handler-script FILE]\n\
                  \                 [--log-service {none,slack}]\n               \
                  \  [--cluster CMD | --cluster-sync CMD | --drmaa [ARGS]]\n     \
                  \            [--cluster-config FILE] [--immediate-submit]\n    \
                  \             [--jobscript SCRIPT] [--jobname NAME]\n          \
                  \       [--cluster-status CLUSTER_STATUS] [--drmaa-log-dir DIR]\n\
                  \                 [--kubernetes [NAMESPACE]] [--container-image\
                  \ IMAGE]\n                 [--tibanna] [--tibanna-sfn TIBANNA_SFN]\n\
                  \                 [--precommand PRECOMMAND]\n                 [--tibanna-config\
                  \ TIBANNA_CONFIG [TIBANNA_CONFIG ...]]\n                 [--use-conda]\
                  \ [--list-conda-envs] [--cleanup-conda]\n                 [--conda-prefix\
                  \ DIR] [--create-envs-only] [--use-singularity]\n              \
                  \   [--singularity-prefix DIR] [--singularity-args ARGS]\n     \
                  \            [--use-envmodules]\n                 [target [target\
                  \ ...]]\n\nSnakemake is a Python based language and execution environment\
                  \ for GNU Make-\nlike workflows.\n\noptional arguments:\n  -h, --help\
                  \            show this help message and exit\n\nEXECUTION:\n  target\
                  \                Targets to build. May be rules or files.\n  --dry-run,\
                  \ --dryrun, -n\n                        Do not execute anything,\
                  \ and display what would be\n                        done. If you\
                  \ have a very large workflow, use --dry-run\n                  \
                  \      --quiet to just print a summary of the DAG of jobs.\n  --profile\
                  \ PROFILE     Name of profile to use for configuring Snakemake.\n\
                  \                        Snakemake will search for a corresponding\
                  \ folder in\n                        /etc/xdg/snakemake and /root/.config/snakemake.\n\
                  \                        Alternatively, this can be an absolute\
                  \ or relative\n                        path. The profile folder\
                  \ has to contain a file\n                        'config.yaml'.\
                  \ This file can be used to set default\n                       \
                  \ values for command line options in YAML format. For\n        \
                  \                example, '--cluster qsub' becomes 'cluster: qsub'\
                  \ in\n                        the YAML file. Profiles can be obtained\
                  \ from\n                        https://github.com/snakemake-profiles.\n\
                  \  --cache RULE [RULE ...]\n                        Store output\
                  \ files of given rules in a central cache\n                    \
                  \    given by the environment variable\n                       \
                  \ $SNAKEMAKE_OUTPUT_CACHE. Likewise, retrieve output\n         \
                  \               files of the given rules from this cache if they\
                  \ have\n                        been created before (by anybody\
                  \ writing to the same\n                        cache), instead of\
                  \ actually executing the rules.\n                        Output\
                  \ files are identified by hashing all steps,\n                 \
                  \       parameters and software stack (conda envs or\n         \
                  \               containers) needed to create them.\n  --snakefile\
                  \ FILE, -s FILE\n                        The workflow definition\
                  \ in form of a\n                        snakefile.Usually, you should\
                  \ not need to specify\n                        this. By default,\
                  \ Snakemake will search for\n                        'Snakefile',\
                  \ 'snakefile', 'workflow/Snakefile',\n                        'workflow/snakefile'\
                  \ beneath the current working\n                        directory,\
                  \ in this order. Only if you definitely want\n                 \
                  \       a different layout, you need to use this parameter.\n  --cores\
                  \ [N], --jobs [N], -j [N]\n                        Use at most N\
                  \ CPU cores/jobs in parallel. If N is\n                        omitted\
                  \ or 'all', the limit is set to the number of\n                \
                  \        available CPU cores.\n  --local-cores N       In cluster\
                  \ mode, use at most N cores of the host\n                      \
                  \  machine in parallel (default: number of CPU cores of\n      \
                  \                  the host). The cores are used to execute local\
                  \ rules.\n                        This option is ignored when not\
                  \ in cluster mode.\n  --resources [NAME=INT [NAME=INT ...]], --res\
                  \ [NAME=INT [NAME=INT ...]]\n                        Define additional\
                  \ resources that shall constrain the\n                        scheduling\
                  \ analogously to threads (see above). A\n                      \
                  \  resource is defined as a name and an integer value.\n       \
                  \                 E.g. --resources gpu=1. Rules can use resources\
                  \ by\n                        defining the resource keyword, e.g.\
                  \ resources: gpu=1.\n                        If now two rules require\
                  \ 1 of the resource 'gpu' they\n                        won't be\
                  \ run in parallel by the scheduler.\n  --set-threads [RULE=THREADS\
                  \ [RULE=THREADS ...]]\n                        Overwrite thread\
                  \ usage of rules. This allows to fine-\n                       \
                  \ tune workflow parallelization. In particular, this is\n      \
                  \                  helpful to target certain cluster nodes by e.g.\n\
                  \                        shifting a rule to use more, or less threads\
                  \ than\n                        defined in the workflow. Thereby,\
                  \ THREADS has to be a\n                        positive integer,\
                  \ and RULE has to be the name of the\n                        rule.\n\
                  \  --default-resources [NAME=INT [NAME=INT ...]], --default-res\
                  \ [NAME=INT [NAME=INT ...]]\n                        Define default\
                  \ values of resources for rules that do\n                      \
                  \  not define their own values. In addition to plain\n         \
                  \               integers, python expressions over inputsize are\n\
                  \                        allowed (e.g. '2*input.size_mb').When specifying\
                  \ this\n                        without any arguments (--default-resources),\
                  \ it\n                        defines 'mem_mb=max(2*input.size_mb,\
                  \ 1000)'\n                        'disk_mb=max(2*input.size_mb,\
                  \ 1000)', i.e., default\n                        disk and mem usage\
                  \ is twice the input file size but at\n                        least\
                  \ 1GB.\n  --config [KEY=VALUE [KEY=VALUE ...]], -C [KEY=VALUE [KEY=VALUE\
                  \ ...]]\n                        Set or overwrite values in the\
                  \ workflow config object.\n                        The workflow\
                  \ config object is accessible as variable\n                    \
                  \    config inside the workflow. Default values can be set\n   \
                  \                     by providing a JSON file (see Documentation).\n\
                  \  --configfile FILE [FILE ...], --configfiles FILE [FILE ...]\n\
                  \                        Specify or overwrite the config file of\
                  \ the workflow\n                        (see the docs). Values specified\
                  \ in JSON or YAML\n                        format are available\
                  \ in the global config dictionary\n                        inside\
                  \ the workflow. Multiple files overwrite each\n                \
                  \        other in the given order.\n  --directory DIR, -d DIR\n\
                  \                        Specify working directory (relative paths\
                  \ in the\n                        snakefile will use this as their\
                  \ origin).\n  --touch, -t           Touch output files (mark them\
                  \ up to date without\n                        really changing them)\
                  \ instead of running their\n                        commands. This\
                  \ is used to pretend that the rules were\n                     \
                  \   executed, in order to fool future invocations of\n         \
                  \               snakemake. Fails if a file does not yet exist. Note\n\
                  \                        that this will only touch files that would\
                  \ otherwise\n                        be recreated by Snakemake (e.g.\
                  \ because their input\n                        files are newer).\
                  \ For enforcing a touch, combine this\n                        with\
                  \ --force, --forceall, or --forcerun. Note however\n           \
                  \             that you loose the provenance information when the\n\
                  \                        files have been created in realitiy. Hence,\
                  \ this\n                        should be used only as a last resort.\n\
                  \  --keep-going, -k      Go on with independent jobs if a job fails.\n\
                  \  --force, -f           Force the execution of the selected target\
                  \ or the\n                        first rule regardless of already\
                  \ created output.\n  --forceall, -F        Force the execution of\
                  \ the selected (or the first)\n                        rule and\
                  \ all rules it is dependent on regardless of\n                 \
                  \       already created output.\n  --forcerun [TARGET [TARGET ...]],\
                  \ -R [TARGET [TARGET ...]]\n                        Force the re-execution\
                  \ or creation of the given rules\n                        or files.\
                  \ Use this option if you changed a rule and\n                  \
                  \      want to have all its output in your workflow updated.\n \
                  \ --prioritize TARGET [TARGET ...], -P TARGET [TARGET ...]\n   \
                  \                     Tell the scheduler to assign creation of given\
                  \ targets\n                        (and all their dependencies)\
                  \ highest priority.\n                        (EXPERIMENTAL)\n  --batch\
                  \ RULE=BATCH/BATCHES\n                        Only create the given\
                  \ BATCH of the input files of the\n                        given\
                  \ RULE. This can be used to iteratively run parts\n            \
                  \            of very large workflows. Only the execution plan of\n\
                  \                        the relevant part of the workflow has to\
                  \ be\n                        calculated, thereby speeding up DAG\
                  \ computation. It is\n                        recommended to provide\
                  \ the most suitable rule for\n                        batching when\
                  \ documenting a workflow. It should be\n                       \
                  \ some aggregating rule that would be executed only\n          \
                  \              once, and has a large number of input files. For\n\
                  \                        example, it can be a rule that aggregates\
                  \ over\n                        samples.\n  --until TARGET [TARGET\
                  \ ...], -U TARGET [TARGET ...]\n                        Runs the\
                  \ pipeline until it reaches the specified rules\n              \
                  \          or files. Only runs jobs that are dependencies of the\n\
                  \                        specified rule or files, does not run sibling\
                  \ DAGs.\n  --omit-from TARGET [TARGET ...], -O TARGET [TARGET ...]\n\
                  \                        Prevent the execution or creation of the\
                  \ given rules\n                        or files as well as any rules\
                  \ or files that are\n                        downstream of these\
                  \ targets in the DAG. Also runs jobs\n                        in\
                  \ sibling DAGs that are independent of the rules or\n          \
                  \              files specified here.\n  --rerun-incomplete, --ri\n\
                  \                        Re-run all jobs the output of which is\
                  \ recognized as\n                        incomplete.\n  --shadow-prefix\
                  \ DIR   Specify a directory in which the 'shadow' directory is\n\
                  \                        created. If not supplied, the value is\
                  \ set to the\n                        '.snakemake' directory relative\
                  \ to the working\n                        directory.\n\nUTILITIES:\n\
                  \  --report [HTMLFILE]   Create an HTML report with results and\
                  \ statistics. If\n                        no filename is given,\
                  \ report.html is the default.\n  --lint [{text,json}]  Perform linting\
                  \ on the given workflow. This will print\n                     \
                  \   snakemake specific suggestions to improve code quality\n   \
                  \                     (work in progress, more lints to be added\
                  \ in the\n                        future). If no argument is provided,\
                  \ plain text output\n                        is used.\n  --export-cwl\
                  \ FILE     Compile workflow to CWL and store it in given FILE.\n\
                  \  --list, -l            Show available rules in given Snakefile.\n\
                  \  --list-target-rules, --lt\n                        Show available\
                  \ target rules in given Snakefile.\n  --dag                 Do not\
                  \ execute anything and print the directed acyclic\n            \
                  \            graph of jobs in the dot language. Recommended use\
                  \ on\n                        Unix systems: snakemake --dag | dot\
                  \ | display\n  --rulegraph           Do not execute anything and\
                  \ print the dependency graph\n                        of rules in\
                  \ the dot language. This will be less\n                        crowded\
                  \ than above DAG of jobs, but also show less\n                 \
                  \       information. Note that each rule is displayed once,\n  \
                  \                      hence the displayed graph will be cyclic\
                  \ if a rule\n                        appears in several steps of\
                  \ the workflow. Use this if\n                        above option\
                  \ leads to a DAG that is too large.\n                        Recommended\
                  \ use on Unix systems: snakemake --rulegraph\n                 \
                  \       | dot | display\n  --filegraph           Do not execute\
                  \ anything and print the dependency graph\n                    \
                  \    of rules with their input and output files in the dot\n   \
                  \                     language. This is an intermediate solution\
                  \ between\n                        above DAG of jobs and the rule\
                  \ graph. Note that each\n                        rule is displayed\
                  \ once, hence the displayed graph will\n                       \
                  \ be cyclic if a rule appears in several steps of the\n        \
                  \                workflow. Use this if above option leads to a DAG\
                  \ that\n                        is too large. Recommended use on\
                  \ Unix systems:\n                        snakemake --filegraph |\
                  \ dot | display\n  --d3dag               Print the DAG in D3.js\
                  \ compatible JSON format.\n  --summary, -S         Print a summary\
                  \ of all files created by the workflow.\n                      \
                  \  The has the following columns: filename, modification\n     \
                  \                   time, rule version, status, plan. Thereby rule\
                  \ version\n                        contains the versionthe file\
                  \ was created with (see the\n                        version keyword\
                  \ of rules), and status denotes whether\n                      \
                  \  the file is missing, its input files are newer or if\n      \
                  \                  version or implementation of the rule changed\
                  \ since\n                        file creation. Finally the last\
                  \ column denotes whether\n                        the file will\
                  \ be updated or created during the next\n                      \
                  \  workflow execution.\n  --detailed-summary, -D\n             \
                  \           Print a summary of all files created by the workflow.\n\
                  \                        The has the following columns: filename,\
                  \ modification\n                        time, rule version, input\
                  \ file(s), shell command,\n                        status, plan.\
                  \ Thereby rule version contains the\n                        version\
                  \ the file was created with (see the version\n                 \
                  \       keyword of rules), and status denotes whether the file\n\
                  \                        is missing, its input files are newer or\
                  \ if version or\n                        implementation of the rule\
                  \ changed since file\n                        creation. The input\
                  \ file and shell command columns are\n                        self\
                  \ explanatory. Finally the last column denotes\n               \
                  \         whether the file will be updated or created during the\n\
                  \                        next workflow execution.\n  --archive FILE\
                  \        Archive the workflow into the given tar archive FILE.\n\
                  \                        The archive will be created such that the\
                  \ workflow can\n                        be re-executed on a vanilla\
                  \ system. The function needs\n                        conda and\
                  \ git to be installed. It will archive every\n                 \
                  \       file that is under git version control. Note that it\n \
                  \                       is best practice to have the Snakefile,\
                  \ config files,\n                        and scripts under version\
                  \ control. Hence, they will be\n                        included\
                  \ in the archive. Further, it will add input\n                 \
                  \       files that are not generated by by the workflow itself\n\
                  \                        and conda environments. Note that symlinks\
                  \ are\n                        dereferenced. Supported formats are\
                  \ .tar, .tar.gz,\n                        .tar.bz2 and .tar.xz.\n\
                  \  --cleanup-metadata FILE [FILE ...], --cm FILE [FILE ...]\n  \
                  \                      Cleanup the metadata of given files. That\
                  \ means that\n                        snakemake removes any tracked\
                  \ version info, and any\n                        marks that files\
                  \ are incomplete.\n  --cleanup-shadow      Cleanup old shadow directories\
                  \ which have not been\n                        deleted due to failures\
                  \ or power loss.\n  --skip-script-cleanup\n                    \
                  \    Don't delete wrapper scripts used for execution\n  --unlock\
                  \              Remove a lock on the working directory.\n  --list-version-changes,\
                  \ --lv\n                        List all output files that have\
                  \ been created with a\n                        different version\
                  \ (as determined by the version\n                        keyword).\n\
                  \  --list-code-changes, --lc\n                        List all output\
                  \ files for which the rule body (run or\n                      \
                  \  shell) have changed in the Snakefile.\n  --list-input-changes,\
                  \ --li\n                        List all output files for which\
                  \ the defined input\n                        files have changed\
                  \ in the Snakefile (e.g. new input\n                        files\
                  \ were added in the rule definition or files were\n            \
                  \            renamed). For listing input file modification in the\n\
                  \                        filesystem, use --summary.\n  --list-params-changes,\
                  \ --lp\n                        List all output files for which\
                  \ the defined params\n                        have changed in the\
                  \ Snakefile.\n  --list-untracked, --lu\n                       \
                  \ List all files in the working directory that are not\n       \
                  \                 used in the workflow. This can be used e.g. for\n\
                  \                        identifying leftover files. Hidden files\
                  \ and\n                        directories are ignored.\n  --delete-all-output\
                  \   Remove all files generated by the workflow. Use\n          \
                  \              together with --dry-run to list files without actually\n\
                  \                        deleting anything. Note that this will\
                  \ not recurse\n                        into subworkflows. Write-protected\
                  \ files are not\n                        removed. Nevertheless,\
                  \ use with care!\n  --delete-temp-output  Remove all temporary files\
                  \ generated by the workflow.\n                        Use together\
                  \ with --dry-run to list files without\n                       \
                  \ actually deleting anything. Note that this will not\n        \
                  \                recurse into subworkflows.\n  --bash-completion\
                  \     Output code to register bash completion for snakemake.\n \
                  \                       Put the following in your .bashrc (including\
                  \ the\n                        accents): `snakemake --bash-completion`\
                  \ or issue it in\n                        an open terminal session.\n\
                  \  --keep-incomplete     Do not remove incomplete output files by\
                  \ failed jobs.\n  --version, -v         show program's version number\
                  \ and exit\n\nOUTPUT:\n  --reason, -r          Print the reason\
                  \ for each executed rule.\n  --gui [PORT]          Serve an HTML\
                  \ based user interface to the given\n                        network\
                  \ and port e.g. 168.129.10.15:8000. By default\n               \
                  \         Snakemake is only available in the local network\n   \
                  \                     (default port: 8000). To make Snakemake listen\
                  \ to all\n                        ip addresses add the special host\
                  \ address 0.0.0.0 to\n                        the url (0.0.0.0:8000).\
                  \ This is important if Snakemake\n                        is used\
                  \ in a virtualised environment like Docker. If\n               \
                  \         possible, a browser window is opened.\n  --printshellcmds,\
                  \ -p  Print out the shell commands that will be executed.\n  --debug-dag\
                  \           Print candidate and selected jobs (including their\n\
                  \                        wildcards) while inferring DAG. This can\
                  \ help to debug\n                        unexpected DAG topology\
                  \ or errors.\n  --stats FILE          Write stats about Snakefile\
                  \ execution in JSON format\n                        to the given\
                  \ file.\n  --nocolor             Do not use a colored output.\n\
                  \  --quiet, -q           Do not output any progress or rule information.\n\
                  \  --print-compilation   Print the python representation of the\
                  \ workflow.\n  --verbose             Print debugging output.\n\n\
                  BEHAVIOR:\n  --force-use-threads   Force threads rather than processes.\
                  \ Helpful if shared\n                        memory (/dev/shm) is\
                  \ full or unavailable.\n  --allow-ambiguity, -a\n              \
                  \          Don't check for ambiguous rules and simply use the\n\
                  \                        first if several can produce the same file.\
                  \ This\n                        allows the user to prioritize rules\
                  \ by their order in\n                        the snakefile.\n  --nolock\
                  \              Do not lock the working directory\n  --ignore-incomplete,\
                  \ --ii\n                        Do not check for incomplete output\
                  \ files.\n  --latency-wait SECONDS, --output-wait SECONDS, -w SECONDS\n\
                  \                        Wait given seconds if an output file of\
                  \ a job is not\n                        present after the job finished.\
                  \ This helps if your\n                        filesystem suffers\
                  \ from latency (default 5).\n  --wait-for-files [FILE [FILE ...]]\n\
                  \                        Wait --latency-wait seconds for these files\
                  \ to be\n                        present before executing the workflow.\
                  \ This option is\n                        used internally to handle\
                  \ filesystem latency in\n                        cluster environments.\n\
                  \  --notemp, --nt        Ignore temp() declarations. This is useful\
                  \ when\n                        running only a part of the workflow,\
                  \ since temp()\n                        would lead to deletion of\
                  \ probably needed files by\n                        other parts\
                  \ of the workflow.\n  --keep-remote         Keep local copies of\
                  \ remote input files.\n  --keep-target-files   Do not adjust the\
                  \ paths of given target files relative\n                       \
                  \ to the working directory.\n  --allowed-rules ALLOWED_RULES [ALLOWED_RULES\
                  \ ...]\n                        Only consider given rules. If omitted,\
                  \ all rules in\n                        Snakefile are used. Note\
                  \ that this is intended\n                        primarily for internal\
                  \ use and may lead to unexpected\n                        results\
                  \ otherwise.\n  --max-jobs-per-second MAX_JOBS_PER_SECOND\n    \
                  \                    Maximal number of cluster/drmaa jobs per second,\n\
                  \                        default is 10, fractions allowed.\n  --max-status-checks-per-second\
                  \ MAX_STATUS_CHECKS_PER_SECOND\n                        Maximal\
                  \ number of job status checks per second,\n                    \
                  \    default is 10, fractions allowed.\n  --restart-times RESTART_TIMES\n\
                  \                        Number of times to restart failing jobs\
                  \ (defaults to\n                        0).\n  --attempt ATTEMPT\
                  \     Internal use only: define the initial value of the\n     \
                  \                   attempt parameter (default: 1).\n  --wrapper-prefix\
                  \ WRAPPER_PREFIX\n                        Prefix for URL created\
                  \ from wrapper directive\n                        (default: https://github.com/snakemake/snakemake-\n\
                  \                        wrappers/raw/). Set this to a different\
                  \ URL to use\n                        your fork or a local clone\
                  \ of the repository, e.g.,\n                        use a git URL\
                  \ like\n                        'git+file://path/to/your/local/clone@'.\n\
                  \  --default-remote-provider {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp,iRODS}\n\
                  \                        Specify default remote provider to be used\
                  \ for all\n                        input and output files that don't\
                  \ yet specify one.\n  --default-remote-prefix DEFAULT_REMOTE_PREFIX\n\
                  \                        Specify prefix for default remote provider.\
                  \ E.g. a\n                        bucket name.\n  --no-shared-fs\
                  \        Do not assume that jobs share a common file system.\n \
                  \                       When this flag is activated, Snakemake will\
                  \ assume\n                        that the filesystem on a cluster\
                  \ node is not shared\n                        with other nodes.\
                  \ For example, this will lead to\n                        downloading\
                  \ remote files on each cluster node\n                        separately.\
                  \ Further, it won't take special measures to\n                 \
                  \       deal with filesystem latency issues. This option will\n\
                  \                        in most cases only make sense in combination\
                  \ with\n                        --default-remote-provider. Further,\
                  \ when using\n                        --cluster you will have to\
                  \ also provide --cluster-\n                        status. Only\
                  \ activate this if you know what you are\n                     \
                  \   doing.\n  --greediness GREEDINESS\n                        Set\
                  \ the greediness of scheduling. This value between 0\n         \
                  \               and 1 determines how careful jobs are selected for\n\
                  \                        execution. The default value (1.0) provides\
                  \ the best\n                        speed and still acceptable scheduling\
                  \ quality.\n  --no-hooks            Do not invoke onstart, onsuccess\
                  \ or onerror hooks\n                        after execution.\n \
                  \ --overwrite-shellcmd OVERWRITE_SHELLCMD\n                    \
                  \    Provide a shell command that shall be executed instead\n  \
                  \                      of those given in the workflow. This is for\
                  \ debugging\n                        purposes only.\n  --debug \
                  \              Allow to debug rules with e.g. PDB. This flag allows\n\
                  \                        to set breakpoints in run blocks.\n  --runtime-profile\
                  \ FILE\n                        Profile Snakemake and write the\
                  \ output to FILE. This\n                        requires yappi to\
                  \ be installed.\n  --mode {0,1,2}        Set execution mode of Snakemake\
                  \ (internal use only).\n  --show-failed-logs    Automatically display\
                  \ logs of failed jobs.\n  --log-handler-script FILE\n          \
                  \              Provide a custom script containing a function 'def\n\
                  \                        log_handler(msg):'. Snakemake will call\
                  \ this function\n                        for every logging output\
                  \ (given as a dictionary\n                        msg)allowing to\
                  \ e.g. send notifications in the form of\n                     \
                  \   e.g. slack messages or emails.\n  --log-service {none,slack}\n\
                  \                        Set a specific messaging service for logging\n\
                  \                        output.Snakemake will notify the service\
                  \ on errors and\n                        completed execution.Currently\
                  \ only slack is supported.\n\nCLUSTER:\n  --cluster CMD, -c CMD\n\
                  \                        Execute snakemake rules with the given\
                  \ submit command,\n                        e.g. qsub. Snakemake\
                  \ compiles jobs into scripts that\n                        are submitted\
                  \ to the cluster with the given command,\n                     \
                  \   once all input files for a particular job are present.\n   \
                  \                     The submit command can be decorated to make\
                  \ it aware\n                        of certain job properties (name,\
                  \ rulename, input,\n                        output, params, wildcards,\
                  \ log, threads and\n                        dependencies (see the\
                  \ argument below)), e.g.: $\n                        snakemake --cluster\
                  \ 'qsub -pe threaded {threads}'.\n  --cluster-sync CMD    cluster\
                  \ submission command will block, returning the\n               \
                  \         remote exitstatus upon remote termination (for\n     \
                  \                   example, this should be usedif the cluster command\
                  \ is\n                        'qsub -sync y' (SGE)\n  --drmaa [ARGS]\
                  \        Execute snakemake on a cluster accessed via DRMAA,\n  \
                  \                      Snakemake compiles jobs into scripts that\
                  \ are\n                        submitted to the cluster with the\
                  \ given command, once\n                        all input files for\
                  \ a particular job are present. ARGS\n                        can\
                  \ be used to specify options of the underlying\n               \
                  \         cluster system, thereby using the job properties name,\n\
                  \                        rulename, input, output, params, wildcards,\
                  \ log,\n                        threads and dependencies, e.g.:\
                  \ --drmaa ' -pe threaded\n                        {threads}'. Note\
                  \ that ARGS must be given in quotes and\n                      \
                  \  with a leading whitespace.\n  --cluster-config FILE, -u FILE\n\
                  \                        A JSON or YAML file that defines the wildcards\
                  \ used in\n                        'cluster'for specific rules,\
                  \ instead of having them\n                        specified in the\
                  \ Snakefile. For example, for rule\n                        'job'\
                  \ you may define: { 'job' : { 'time' : '24:00:00'\n            \
                  \            } } to specify the time for rule 'job'. You can\n \
                  \                       specify more than one file. The configuration\
                  \ files\n                        are merged with later values overriding\
                  \ earlier ones.\n                        This option is deprecated\
                  \ in favor of using --profile,\n                        see docs.\n\
                  \  --immediate-submit, --is\n                        Immediately\
                  \ submit all jobs to the cluster instead of\n                  \
                  \      waiting for present input files. This will fail,\n      \
                  \                  unless you make the cluster aware of job dependencies,\n\
                  \                        e.g. via: $ snakemake --cluster 'sbatch\
                  \ --dependency\n                        {dependencies}. Assuming\
                  \ that your submit script (here\n                        sbatch)\
                  \ outputs the generated job id to the first\n                  \
                  \      stdout line, {dependencies} will be filled with space\n \
                  \                       separated job ids this job depends on.\n\
                  \  --jobscript SCRIPT, --js SCRIPT\n                        Provide\
                  \ a custom job script for submission to the\n                  \
                  \      cluster. The default script resides as 'jobscript.sh'\n \
                  \                       in the installation directory.\n  --jobname\
                  \ NAME, --jn NAME\n                        Provide a custom name\
                  \ for the jobscript that is\n                        submitted to\
                  \ the cluster (see --cluster). NAME is\n                       \
                  \ \"snakejob.{name}.{jobid}.sh\" per default. The wildcard\n   \
                  \                     {jobid} has to be present in the name.\n \
                  \ --cluster-status CLUSTER_STATUS\n                        Status\
                  \ command for cluster execution. This is only\n                \
                  \        considered in combination with the --cluster flag. If\n\
                  \                        provided, Snakemake will use the status\
                  \ command to\n                        determine if a job has finished\
                  \ successfully or\n                        failed. For this it is\
                  \ necessary that the submit\n                        command provided\
                  \ to --cluster returns the cluster job\n                       \
                  \ id. Then, the status command will be invoked with the\n      \
                  \                  job id. Snakemake expects it to return 'success'\
                  \ if\n                        the job was successfull, 'failed'\
                  \ if the job failed\n                        and 'running' if the\
                  \ job still runs.\n  --drmaa-log-dir DIR   Specify a directory in\
                  \ which stdout and stderr files\n                        of DRMAA\
                  \ jobs will be written. The value may be given\n               \
                  \         as a relative path, in which case Snakemake will use\n\
                  \                        the current invocation directory as the\
                  \ origin. If\n                        given, this will override\
                  \ any given '-o' and/or '-e'\n                        native specification.\
                  \ If not given, all DRMAA stdout\n                        and stderr\
                  \ files are written to the current working\n                   \
                  \     directory.\n\nKUBERNETES:\n  --kubernetes [NAMESPACE]\n  \
                  \                      Execute workflow in a kubernetes cluster\
                  \ (in the\n                        cloud). NAMESPACE is the namespace\
                  \ you want to use for\n                        your job (if nothing\
                  \ specified: 'default'). Usually,\n                        this\
                  \ requires --default-remote-provider and --default-\n          \
                  \              remote-prefix to be set to a S3 or GS bucket where\n\
                  \                        your . data shall be stored. It is further\
                  \ advisable\n                        to activate conda integration\
                  \ via --use-conda.\n  --container-image IMAGE\n                \
                  \        Docker image to use, e.g., when submitting jobs to\n  \
                  \                      kubernetes. By default, this is\n       \
                  \                 'https://hub.docker.com/r/snakemake/snakemake',\
                  \ tagged\n                        with the same version as the currently\
                  \ running\n                        Snakemake instance. Note that\
                  \ overwriting this value\n                        is up to your\
                  \ responsibility. Any used image has to\n                      \
                  \  contain a working snakemake installation that is\n          \
                  \              compatible with (or ideally the same as) the currently\n\
                  \                        running version.\n\nTIBANNA:\n  --tibanna\
                  \             Execute workflow on AWS cloud using Tibanna. This\n\
                  \                        requires --default-remote-prefix to be\
                  \ set to S3\n                        bucket name and prefix (e.g.\n\
                  \                        'bucketname/subdirectory') where input\
                  \ is already\n                        stored and output will be\
                  \ sent to. Using --tibanna\n                        implies --default-resources\
                  \ is set as default.\n                        Optionally, use --precommand\
                  \ to specify any\n                        preparation command to\
                  \ run before snakemake command on\n                        the cloud\
                  \ (inside snakemake container on Tibanna VM).\n                \
                  \        Also, --use-conda, --use-singularity, --config,\n     \
                  \                   --configfile are supported and will be carried\
                  \ over.\n  --tibanna-sfn TIBANNA_SFN\n                        Name\
                  \ of Tibanna Unicorn step function (e.g.\n                     \
                  \   tibanna_unicorn_monty).This works as serverless\n          \
                  \              scheduler/resource allocator and must be deployed\n\
                  \                        first using tibanna cli. (e.g. tibanna\
                  \ deploy_unicorn\n                        --usergroup=monty --buckets=bucketname)\n\
                  \  --precommand PRECOMMAND\n                        Any command\
                  \ to execute before snakemake command on AWS\n                 \
                  \       cloud such as wget, git clone, unzip, etc. This is\n   \
                  \                     used with --tibanna.Do not include input/output\n\
                  \                        download/upload commands - file transfer\
                  \ between S3\n                        bucket and the run environment\
                  \ (container) is\n                        automatically handled\
                  \ by Tibanna.\n  --tibanna-config TIBANNA_CONFIG [TIBANNA_CONFIG\
                  \ ...]\n                        Additional tibanan config e.g. --tibanna-config\n\
                  \                        spot_instance=true subnet=<subnet_id> security\n\
                  \                        group=<security_group_id>\n\nCONDA:\n \
                  \ --use-conda           If defined in the rule, run job in a conda\n\
                  \                        environment. If this flag is not set, the\
                  \ conda\n                        directive is ignored.\n  --list-conda-envs\
                  \     List all conda environments and their location on\n      \
                  \                  disk.\n  --cleanup-conda       Cleanup unused\
                  \ conda environments.\n  --conda-prefix DIR    Specify a directory\
                  \ in which the 'conda' and 'conda-\n                        archive'\
                  \ directories are created. These are used to\n                 \
                  \       store conda environments and their archives,\n         \
                  \               respectively. If not supplied, the value is set\
                  \ to the\n                        '.snakemake' directory relative\
                  \ to the invocation\n                        directory. If supplied,\
                  \ the `--use-conda` flag must\n                        also be set.\
                  \ The value may be given as a relative\n                       \
                  \ path, which will be extrapolated to the invocation\n         \
                  \               directory, or as an absolute path.\n  --create-envs-only\
                  \    If specified, only creates the job-specific conda\n       \
                  \                 environments then exits. The `--use-conda` flag\
                  \ must\n                        also be set.\n\nSINGULARITY:\n \
                  \ --use-singularity     If defined in the rule, run job within a\
                  \ singularity\n                        container. If this flag is\
                  \ not set, the singularity\n                        directive is\
                  \ ignored.\n  --singularity-prefix DIR\n                       \
                  \ Specify a directory in which singularity images will\n       \
                  \                 be stored.If not supplied, the value is set to\
                  \ the\n                        '.snakemake' directory relative to\
                  \ the invocation\n                        directory. If supplied,\
                  \ the `--use-singularity` flag\n                        must also\
                  \ be set. The value may be given as a relative\n               \
                  \         path, which will be extrapolated to the invocation\n \
                  \                       directory, or as an absolute path.\n  --singularity-args\
                  \ ARGS\n                        Pass additional args to singularity.\n\
                  \nENVIRONMENT MODULES:\n  --use-envmodules      If defined in the\
                  \ rule, run job within the given\n                        environment\
                  \ modules, loaded in the given order. This\n                   \
                  \     can be combined with --use-conda and --use-\n            \
                  \            singularity, which will then be only used as a\n  \
                  \                      fallback for rules which don't define environment\n\
                  \                        modules.\n"
                generated_using: *id004
                docker_image:
              subcommands: []
              usage: []
              help_flag: !Flag
                optional: true
                synonyms:
                - --help
                description: show Snakemake help (or snakemake -h)
                args: !EmptyFlagArg {}
              usage_flag:
              version_flag:
              help_text: "ERROR\ntarget: No such file or directory\nProphane Pipeline\
                \ (powered by Snakemake)\n\nUsage: /usr/local/bin/prophane CONFIG_FILE\
                \ [Snakemake options]\n\n Full list of parameters:\n   --help    \
                \             show Snakemake help (or snakemake -h)\n   --list-dbs\
                \             print list of configured databases\n               \
                \           databases are looked up in 'db_base_dir' configured in:\n\
                \                              /usr/local/opt/prophane/general_config.yaml\n\
                \   --list-styles          print list of available input file styles\n\
                \                          styles are looked up in the following folder:\n\
                \                              /usr/local/opt/prophane/styles\n  \
                \ --db-maintenance       trigger database maintenance scripts\n  \
                \                        will migrate database structure from deprecated\
                \ structure to most recent\n\n Useful Snakemake parameters:\n   -j,\
                \ --cores            number of cores\n   -k, --keep-going       go\
                \ on with independent jobs if a job fails\n   -n, --dryrun       \
                \    do not execute anything\n   -p, --printshellcmds   print out\
                \ the shell commands that will be executed\n   -t, --timestamp  \t\
                \tadd a timestamp to all logging output\n\n"
              generated_using: *id004
              docker_image:
            - !Command
              command: *id005
              positional: []
              named:
              - !Flag
                optional: true
                synonyms:
                - --list-dbs
                description: "print list of configured databases\ndatabases are looked\
                  \ up in 'db_base_dir' configured in:\n/usr/local/opt/prophane/general_config.yaml"
                args: !EmptyFlagArg {}
              - !Flag
                optional: true
                synonyms:
                - --list-styles
                description: "print list of available input file styles\nstyles are\
                  \ looked up in the following folder:\n/usr/local/opt/prophane/styles"
                args: !EmptyFlagArg {}
              - !Flag
                optional: true
                synonyms:
                - --db-maintenance
                description: "trigger database maintenance scripts\nwill migrate database\
                  \ structure from deprecated structure to most recent"
                args: !EmptyFlagArg {}
              - !Flag
                optional: true
                synonyms:
                - -j
                - --cores
                description: number of cores
                args: !EmptyFlagArg {}
              - !Flag
                optional: true
                synonyms:
                - -k
                - --keep-going
                description: go on with independent jobs if a job fails
                args: !EmptyFlagArg {}
              - !Flag
                optional: true
                synonyms:
                - -n
                - --dryrun
                description: do not execute anything
                args: !EmptyFlagArg {}
              - !Flag
                optional: true
                synonyms:
                - -p
                - --printshellcmds
                description: print out the shell commands that will be executed
                args: !EmptyFlagArg {}
              - !Flag
                optional: true
                synonyms:
                - -t
                - --timestamp
                description: add a timestamp to all logging output
                args: !EmptyFlagArg {}
              parent: *id006
              subcommands: []
              usage: []
              help_flag: !Flag
                optional: true
                synonyms:
                - --help
                description: show Snakemake help (or snakemake -h)
                args: !EmptyFlagArg {}
              usage_flag:
              version_flag:
              help_text: "ERROR\ndisk.: No such file or directory\nProphane Pipeline\
                \ (powered by Snakemake)\n\nUsage: /usr/local/bin/prophane CONFIG_FILE\
                \ [Snakemake options]\n\n Full list of parameters:\n   --help    \
                \             show Snakemake help (or snakemake -h)\n   --list-dbs\
                \             print list of configured databases\n               \
                \           databases are looked up in 'db_base_dir' configured in:\n\
                \                              /usr/local/opt/prophane/general_config.yaml\n\
                \   --list-styles          print list of available input file styles\n\
                \                          styles are looked up in the following folder:\n\
                \                              /usr/local/opt/prophane/styles\n  \
                \ --db-maintenance       trigger database maintenance scripts\n  \
                \                        will migrate database structure from deprecated\
                \ structure to most recent\n\n Useful Snakemake parameters:\n   -j,\
                \ --cores            number of cores\n   -k, --keep-going       go\
                \ on with independent jobs if a job fails\n   -n, --dryrun       \
                \    do not execute anything\n   -p, --printshellcmds   print out\
                \ the shell commands that will be executed\n   -t, --timestamp  \t\
                \tadd a timestamp to all logging output\n\n"
              generated_using: *id004
              docker_image:
            usage: []
            help_flag:
            usage_flag:
            version_flag:
            help_text: "usage: snakemake [-h] [--dry-run] [--profile PROFILE]\n  \
              \               [--cache RULE [RULE ...]] [--snakefile FILE] [--cores\
              \ [N]]\n                 [--local-cores N] [--resources [NAME=INT [NAME=INT\
              \ ...]]]\n                 [--set-threads [RULE=THREADS [RULE=THREADS\
              \ ...]]]\n                 [--default-resources [NAME=INT [NAME=INT\
              \ ...]]]\n                 [--config [KEY=VALUE [KEY=VALUE ...]]]\n\
              \                 [--configfile FILE [FILE ...]] [--directory DIR] [--touch]\n\
              \                 [--keep-going] [--force] [--forceall]\n          \
              \       [--forcerun [TARGET [TARGET ...]]]\n                 [--prioritize\
              \ TARGET [TARGET ...]]\n                 [--batch RULE=BATCH/BATCHES]\
              \ [--until TARGET [TARGET ...]]\n                 [--omit-from TARGET\
              \ [TARGET ...]] [--rerun-incomplete]\n                 [--shadow-prefix\
              \ DIR] [--report [HTMLFILE]]\n                 [--lint [{text,json}]]\
              \ [--export-cwl FILE] [--list]\n                 [--list-target-rules]\
              \ [--dag] [--rulegraph] [--filegraph]\n                 [--d3dag] [--summary]\
              \ [--detailed-summary] [--archive FILE]\n                 [--cleanup-metadata\
              \ FILE [FILE ...]] [--cleanup-shadow]\n                 [--skip-script-cleanup]\
              \ [--unlock] [--list-version-changes]\n                 [--list-code-changes]\
              \ [--list-input-changes]\n                 [--list-params-changes] [--list-untracked]\n\
              \                 [--delete-all-output] [--delete-temp-output]\n   \
              \              [--bash-completion] [--keep-incomplete] [--version]\n\
              \                 [--reason] [--gui [PORT]] [--printshellcmds] [--debug-dag]\n\
              \                 [--stats FILE] [--nocolor] [--quiet] [--print-compilation]\n\
              \                 [--verbose] [--force-use-threads] [--allow-ambiguity]\n\
              \                 [--nolock] [--ignore-incomplete] [--latency-wait SECONDS]\n\
              \                 [--wait-for-files [FILE [FILE ...]]] [--notemp]\n\
              \                 [--keep-remote] [--keep-target-files]\n          \
              \       [--allowed-rules ALLOWED_RULES [ALLOWED_RULES ...]]\n      \
              \           [--max-jobs-per-second MAX_JOBS_PER_SECOND]\n          \
              \       [--max-status-checks-per-second MAX_STATUS_CHECKS_PER_SECOND]\n\
              \                 [--restart-times RESTART_TIMES] [--attempt ATTEMPT]\n\
              \                 [--wrapper-prefix WRAPPER_PREFIX]\n              \
              \   [--default-remote-provider {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp,iRODS}]\n\
              \                 [--default-remote-prefix DEFAULT_REMOTE_PREFIX]\n\
              \                 [--no-shared-fs] [--greediness GREEDINESS] [--no-hooks]\n\
              \                 [--overwrite-shellcmd OVERWRITE_SHELLCMD] [--debug]\n\
              \                 [--runtime-profile FILE] [--mode {0,1,2}]\n      \
              \           [--show-failed-logs] [--log-handler-script FILE]\n     \
              \            [--log-service {none,slack}]\n                 [--cluster\
              \ CMD | --cluster-sync CMD | --drmaa [ARGS]]\n                 [--cluster-config\
              \ FILE] [--immediate-submit]\n                 [--jobscript SCRIPT]\
              \ [--jobname NAME]\n                 [--cluster-status CLUSTER_STATUS]\
              \ [--drmaa-log-dir DIR]\n                 [--kubernetes [NAMESPACE]]\
              \ [--container-image IMAGE]\n                 [--tibanna] [--tibanna-sfn\
              \ TIBANNA_SFN]\n                 [--precommand PRECOMMAND]\n       \
              \          [--tibanna-config TIBANNA_CONFIG [TIBANNA_CONFIG ...]]\n\
              \                 [--use-conda] [--list-conda-envs] [--cleanup-conda]\n\
              \                 [--conda-prefix DIR] [--create-envs-only] [--use-singularity]\n\
              \                 [--singularity-prefix DIR] [--singularity-args ARGS]\n\
              \                 [--use-envmodules]\n                 [target [target\
              \ ...]]\n\nSnakemake is a Python based language and execution environment\
              \ for GNU Make-\nlike workflows.\n\noptional arguments:\n  -h, --help\
              \            show this help message and exit\n\nEXECUTION:\n  target\
              \                Targets to build. May be rules or files.\n  --dry-run,\
              \ --dryrun, -n\n                        Do not execute anything, and\
              \ display what would be\n                        done. If you have a\
              \ very large workflow, use --dry-run\n                        --quiet\
              \ to just print a summary of the DAG of jobs.\n  --profile PROFILE \
              \    Name of profile to use for configuring Snakemake.\n           \
              \             Snakemake will search for a corresponding folder in\n\
              \                        /etc/xdg/snakemake and /root/.config/snakemake.\n\
              \                        Alternatively, this can be an absolute or relative\n\
              \                        path. The profile folder has to contain a file\n\
              \                        'config.yaml'. This file can be used to set\
              \ default\n                        values for command line options in\
              \ YAML format. For\n                        example, '--cluster qsub'\
              \ becomes 'cluster: qsub' in\n                        the YAML file.\
              \ Profiles can be obtained from\n                        https://github.com/snakemake-profiles.\n\
              \  --cache RULE [RULE ...]\n                        Store output files\
              \ of given rules in a central cache\n                        given by\
              \ the environment variable\n                        $SNAKEMAKE_OUTPUT_CACHE.\
              \ Likewise, retrieve output\n                        files of the given\
              \ rules from this cache if they have\n                        been created\
              \ before (by anybody writing to the same\n                        cache),\
              \ instead of actually executing the rules.\n                       \
              \ Output files are identified by hashing all steps,\n              \
              \          parameters and software stack (conda envs or\n          \
              \              containers) needed to create them.\n  --snakefile FILE,\
              \ -s FILE\n                        The workflow definition in form of\
              \ a\n                        snakefile.Usually, you should not need\
              \ to specify\n                        this. By default, Snakemake will\
              \ search for\n                        'Snakefile', 'snakefile', 'workflow/Snakefile',\n\
              \                        'workflow/snakefile' beneath the current working\n\
              \                        directory, in this order. Only if you definitely\
              \ want\n                        a different layout, you need to use\
              \ this parameter.\n  --cores [N], --jobs [N], -j [N]\n             \
              \           Use at most N CPU cores/jobs in parallel. If N is\n    \
              \                    omitted or 'all', the limit is set to the number\
              \ of\n                        available CPU cores.\n  --local-cores\
              \ N       In cluster mode, use at most N cores of the host\n       \
              \                 machine in parallel (default: number of CPU cores\
              \ of\n                        the host). The cores are used to execute\
              \ local rules.\n                        This option is ignored when\
              \ not in cluster mode.\n  --resources [NAME=INT [NAME=INT ...]], --res\
              \ [NAME=INT [NAME=INT ...]]\n                        Define additional\
              \ resources that shall constrain the\n                        scheduling\
              \ analogously to threads (see above). A\n                        resource\
              \ is defined as a name and an integer value.\n                     \
              \   E.g. --resources gpu=1. Rules can use resources by\n           \
              \             defining the resource keyword, e.g. resources: gpu=1.\n\
              \                        If now two rules require 1 of the resource\
              \ 'gpu' they\n                        won't be run in parallel by the\
              \ scheduler.\n  --set-threads [RULE=THREADS [RULE=THREADS ...]]\n  \
              \                      Overwrite thread usage of rules. This allows\
              \ to fine-\n                        tune workflow parallelization. In\
              \ particular, this is\n                        helpful to target certain\
              \ cluster nodes by e.g.\n                        shifting a rule to\
              \ use more, or less threads than\n                        defined in\
              \ the workflow. Thereby, THREADS has to be a\n                     \
              \   positive integer, and RULE has to be the name of the\n         \
              \               rule.\n  --default-resources [NAME=INT [NAME=INT ...]],\
              \ --default-res [NAME=INT [NAME=INT ...]]\n                        Define\
              \ default values of resources for rules that do\n                  \
              \      not define their own values. In addition to plain\n         \
              \               integers, python expressions over inputsize are\n  \
              \                      allowed (e.g. '2*input.size_mb').When specifying\
              \ this\n                        without any arguments (--default-resources),\
              \ it\n                        defines 'mem_mb=max(2*input.size_mb, 1000)'\n\
              \                        'disk_mb=max(2*input.size_mb, 1000)', i.e.,\
              \ default\n                        disk and mem usage is twice the input\
              \ file size but at\n                        least 1GB.\n  --config [KEY=VALUE\
              \ [KEY=VALUE ...]], -C [KEY=VALUE [KEY=VALUE ...]]\n               \
              \         Set or overwrite values in the workflow config object.\n \
              \                       The workflow config object is accessible as\
              \ variable\n                        config inside the workflow. Default\
              \ values can be set\n                        by providing a JSON file\
              \ (see Documentation).\n  --configfile FILE [FILE ...], --configfiles\
              \ FILE [FILE ...]\n                        Specify or overwrite the\
              \ config file of the workflow\n                        (see the docs).\
              \ Values specified in JSON or YAML\n                        format are\
              \ available in the global config dictionary\n                      \
              \  inside the workflow. Multiple files overwrite each\n            \
              \            other in the given order.\n  --directory DIR, -d DIR\n\
              \                        Specify working directory (relative paths in\
              \ the\n                        snakefile will use this as their origin).\n\
              \  --touch, -t           Touch output files (mark them up to date without\n\
              \                        really changing them) instead of running their\n\
              \                        commands. This is used to pretend that the\
              \ rules were\n                        executed, in order to fool future\
              \ invocations of\n                        snakemake. Fails if a file\
              \ does not yet exist. Note\n                        that this will only\
              \ touch files that would otherwise\n                        be recreated\
              \ by Snakemake (e.g. because their input\n                        files\
              \ are newer). For enforcing a touch, combine this\n                \
              \        with --force, --forceall, or --forcerun. Note however\n   \
              \                     that you loose the provenance information when\
              \ the\n                        files have been created in realitiy.\
              \ Hence, this\n                        should be used only as a last\
              \ resort.\n  --keep-going, -k      Go on with independent jobs if a\
              \ job fails.\n  --force, -f           Force the execution of the selected\
              \ target or the\n                        first rule regardless of already\
              \ created output.\n  --forceall, -F        Force the execution of the\
              \ selected (or the first)\n                        rule and all rules\
              \ it is dependent on regardless of\n                        already\
              \ created output.\n  --forcerun [TARGET [TARGET ...]], -R [TARGET [TARGET\
              \ ...]]\n                        Force the re-execution or creation\
              \ of the given rules\n                        or files. Use this option\
              \ if you changed a rule and\n                        want to have all\
              \ its output in your workflow updated.\n  --prioritize TARGET [TARGET\
              \ ...], -P TARGET [TARGET ...]\n                        Tell the scheduler\
              \ to assign creation of given targets\n                        (and\
              \ all their dependencies) highest priority.\n                      \
              \  (EXPERIMENTAL)\n  --batch RULE=BATCH/BATCHES\n                  \
              \      Only create the given BATCH of the input files of the\n     \
              \                   given RULE. This can be used to iteratively run\
              \ parts\n                        of very large workflows. Only the execution\
              \ plan of\n                        the relevant part of the workflow\
              \ has to be\n                        calculated, thereby speeding up\
              \ DAG computation. It is\n                        recommended to provide\
              \ the most suitable rule for\n                        batching when\
              \ documenting a workflow. It should be\n                        some\
              \ aggregating rule that would be executed only\n                   \
              \     once, and has a large number of input files. For\n           \
              \             example, it can be a rule that aggregates over\n     \
              \                   samples.\n  --until TARGET [TARGET ...], -U TARGET\
              \ [TARGET ...]\n                        Runs the pipeline until it reaches\
              \ the specified rules\n                        or files. Only runs jobs\
              \ that are dependencies of the\n                        specified rule\
              \ or files, does not run sibling DAGs.\n  --omit-from TARGET [TARGET\
              \ ...], -O TARGET [TARGET ...]\n                        Prevent the\
              \ execution or creation of the given rules\n                       \
              \ or files as well as any rules or files that are\n                \
              \        downstream of these targets in the DAG. Also runs jobs\n  \
              \                      in sibling DAGs that are independent of the rules\
              \ or\n                        files specified here.\n  --rerun-incomplete,\
              \ --ri\n                        Re-run all jobs the output of which\
              \ is recognized as\n                        incomplete.\n  --shadow-prefix\
              \ DIR   Specify a directory in which the 'shadow' directory is\n   \
              \                     created. If not supplied, the value is set to\
              \ the\n                        '.snakemake' directory relative to the\
              \ working\n                        directory.\n\nUTILITIES:\n  --report\
              \ [HTMLFILE]   Create an HTML report with results and statistics. If\n\
              \                        no filename is given, report.html is the default.\n\
              \  --lint [{text,json}]  Perform linting on the given workflow. This\
              \ will print\n                        snakemake specific suggestions\
              \ to improve code quality\n                        (work in progress,\
              \ more lints to be added in the\n                        future). If\
              \ no argument is provided, plain text output\n                     \
              \   is used.\n  --export-cwl FILE     Compile workflow to CWL and store\
              \ it in given FILE.\n  --list, -l            Show available rules in\
              \ given Snakefile.\n  --list-target-rules, --lt\n                  \
              \      Show available target rules in given Snakefile.\n  --dag    \
              \             Do not execute anything and print the directed acyclic\n\
              \                        graph of jobs in the dot language. Recommended\
              \ use on\n                        Unix systems: snakemake --dag | dot\
              \ | display\n  --rulegraph           Do not execute anything and print\
              \ the dependency graph\n                        of rules in the dot\
              \ language. This will be less\n                        crowded than\
              \ above DAG of jobs, but also show less\n                        information.\
              \ Note that each rule is displayed once,\n                        hence\
              \ the displayed graph will be cyclic if a rule\n                   \
              \     appears in several steps of the workflow. Use this if\n      \
              \                  above option leads to a DAG that is too large.\n\
              \                        Recommended use on Unix systems: snakemake\
              \ --rulegraph\n                        | dot | display\n  --filegraph\
              \           Do not execute anything and print the dependency graph\n\
              \                        of rules with their input and output files\
              \ in the dot\n                        language. This is an intermediate\
              \ solution between\n                        above DAG of jobs and the\
              \ rule graph. Note that each\n                        rule is displayed\
              \ once, hence the displayed graph will\n                        be cyclic\
              \ if a rule appears in several steps of the\n                      \
              \  workflow. Use this if above option leads to a DAG that\n        \
              \                is too large. Recommended use on Unix systems:\n  \
              \                      snakemake --filegraph | dot | display\n  --d3dag\
              \               Print the DAG in D3.js compatible JSON format.\n  --summary,\
              \ -S         Print a summary of all files created by the workflow.\n\
              \                        The has the following columns: filename, modification\n\
              \                        time, rule version, status, plan. Thereby rule\
              \ version\n                        contains the versionthe file was\
              \ created with (see the\n                        version keyword of\
              \ rules), and status denotes whether\n                        the file\
              \ is missing, its input files are newer or if\n                    \
              \    version or implementation of the rule changed since\n         \
              \               file creation. Finally the last column denotes whether\n\
              \                        the file will be updated or created during\
              \ the next\n                        workflow execution.\n  --detailed-summary,\
              \ -D\n                        Print a summary of all files created by\
              \ the workflow.\n                        The has the following columns:\
              \ filename, modification\n                        time, rule version,\
              \ input file(s), shell command,\n                        status, plan.\
              \ Thereby rule version contains the\n                        version\
              \ the file was created with (see the version\n                     \
              \   keyword of rules), and status denotes whether the file\n       \
              \                 is missing, its input files are newer or if version\
              \ or\n                        implementation of the rule changed since\
              \ file\n                        creation. The input file and shell command\
              \ columns are\n                        self explanatory. Finally the\
              \ last column denotes\n                        whether the file will\
              \ be updated or created during the\n                        next workflow\
              \ execution.\n  --archive FILE        Archive the workflow into the\
              \ given tar archive FILE.\n                        The archive will\
              \ be created such that the workflow can\n                        be\
              \ re-executed on a vanilla system. The function needs\n            \
              \            conda and git to be installed. It will archive every\n\
              \                        file that is under git version control. Note\
              \ that it\n                        is best practice to have the Snakefile,\
              \ config files,\n                        and scripts under version control.\
              \ Hence, they will be\n                        included in the archive.\
              \ Further, it will add input\n                        files that are\
              \ not generated by by the workflow itself\n                        and\
              \ conda environments. Note that symlinks are\n                     \
              \   dereferenced. Supported formats are .tar, .tar.gz,\n           \
              \             .tar.bz2 and .tar.xz.\n  --cleanup-metadata FILE [FILE\
              \ ...], --cm FILE [FILE ...]\n                        Cleanup the metadata\
              \ of given files. That means that\n                        snakemake\
              \ removes any tracked version info, and any\n                      \
              \  marks that files are incomplete.\n  --cleanup-shadow      Cleanup\
              \ old shadow directories which have not been\n                     \
              \   deleted due to failures or power loss.\n  --skip-script-cleanup\n\
              \                        Don't delete wrapper scripts used for execution\n\
              \  --unlock              Remove a lock on the working directory.\n \
              \ --list-version-changes, --lv\n                        List all output\
              \ files that have been created with a\n                        different\
              \ version (as determined by the version\n                        keyword).\n\
              \  --list-code-changes, --lc\n                        List all output\
              \ files for which the rule body (run or\n                        shell)\
              \ have changed in the Snakefile.\n  --list-input-changes, --li\n   \
              \                     List all output files for which the defined input\n\
              \                        files have changed in the Snakefile (e.g. new\
              \ input\n                        files were added in the rule definition\
              \ or files were\n                        renamed). For listing input\
              \ file modification in the\n                        filesystem, use\
              \ --summary.\n  --list-params-changes, --lp\n                      \
              \  List all output files for which the defined params\n            \
              \            have changed in the Snakefile.\n  --list-untracked, --lu\n\
              \                        List all files in the working directory that\
              \ are not\n                        used in the workflow. This can be\
              \ used e.g. for\n                        identifying leftover files.\
              \ Hidden files and\n                        directories are ignored.\n\
              \  --delete-all-output   Remove all files generated by the workflow.\
              \ Use\n                        together with --dry-run to list files\
              \ without actually\n                        deleting anything. Note\
              \ that this will not recurse\n                        into subworkflows.\
              \ Write-protected files are not\n                        removed. Nevertheless,\
              \ use with care!\n  --delete-temp-output  Remove all temporary files\
              \ generated by the workflow.\n                        Use together with\
              \ --dry-run to list files without\n                        actually\
              \ deleting anything. Note that this will not\n                     \
              \   recurse into subworkflows.\n  --bash-completion     Output code\
              \ to register bash completion for snakemake.\n                     \
              \   Put the following in your .bashrc (including the\n             \
              \           accents): `snakemake --bash-completion` or issue it in\n\
              \                        an open terminal session.\n  --keep-incomplete\
              \     Do not remove incomplete output files by failed jobs.\n  --version,\
              \ -v         show program's version number and exit\n\nOUTPUT:\n  --reason,\
              \ -r          Print the reason for each executed rule.\n  --gui [PORT]\
              \          Serve an HTML based user interface to the given\n       \
              \                 network and port e.g. 168.129.10.15:8000. By default\n\
              \                        Snakemake is only available in the local network\n\
              \                        (default port: 8000). To make Snakemake listen\
              \ to all\n                        ip addresses add the special host\
              \ address 0.0.0.0 to\n                        the url (0.0.0.0:8000).\
              \ This is important if Snakemake\n                        is used in\
              \ a virtualised environment like Docker. If\n                      \
              \  possible, a browser window is opened.\n  --printshellcmds, -p  Print\
              \ out the shell commands that will be executed.\n  --debug-dag     \
              \      Print candidate and selected jobs (including their\n        \
              \                wildcards) while inferring DAG. This can help to debug\n\
              \                        unexpected DAG topology or errors.\n  --stats\
              \ FILE          Write stats about Snakefile execution in JSON format\n\
              \                        to the given file.\n  --nocolor           \
              \  Do not use a colored output.\n  --quiet, -q           Do not output\
              \ any progress or rule information.\n  --print-compilation   Print the\
              \ python representation of the workflow.\n  --verbose             Print\
              \ debugging output.\n\nBEHAVIOR:\n  --force-use-threads   Force threads\
              \ rather than processes. Helpful if shared\n                       \
              \ memory (/dev/shm) is full or unavailable.\n  --allow-ambiguity, -a\n\
              \                        Don't check for ambiguous rules and simply\
              \ use the\n                        first if several can produce the\
              \ same file. This\n                        allows the user to prioritize\
              \ rules by their order in\n                        the snakefile.\n\
              \  --nolock              Do not lock the working directory\n  --ignore-incomplete,\
              \ --ii\n                        Do not check for incomplete output files.\n\
              \  --latency-wait SECONDS, --output-wait SECONDS, -w SECONDS\n     \
              \                   Wait given seconds if an output file of a job is\
              \ not\n                        present after the job finished. This\
              \ helps if your\n                        filesystem suffers from latency\
              \ (default 5).\n  --wait-for-files [FILE [FILE ...]]\n             \
              \           Wait --latency-wait seconds for these files to be\n    \
              \                    present before executing the workflow. This option\
              \ is\n                        used internally to handle filesystem latency\
              \ in\n                        cluster environments.\n  --notemp, --nt\
              \        Ignore temp() declarations. This is useful when\n         \
              \               running only a part of the workflow, since temp()\n\
              \                        would lead to deletion of probably needed files\
              \ by\n                        other parts of the workflow.\n  --keep-remote\
              \         Keep local copies of remote input files.\n  --keep-target-files\
              \   Do not adjust the paths of given target files relative\n       \
              \                 to the working directory.\n  --allowed-rules ALLOWED_RULES\
              \ [ALLOWED_RULES ...]\n                        Only consider given rules.\
              \ If omitted, all rules in\n                        Snakefile are used.\
              \ Note that this is intended\n                        primarily for\
              \ internal use and may lead to unexpected\n                        results\
              \ otherwise.\n  --max-jobs-per-second MAX_JOBS_PER_SECOND\n        \
              \                Maximal number of cluster/drmaa jobs per second,\n\
              \                        default is 10, fractions allowed.\n  --max-status-checks-per-second\
              \ MAX_STATUS_CHECKS_PER_SECOND\n                        Maximal number\
              \ of job status checks per second,\n                        default\
              \ is 10, fractions allowed.\n  --restart-times RESTART_TIMES\n     \
              \                   Number of times to restart failing jobs (defaults\
              \ to\n                        0).\n  --attempt ATTEMPT     Internal\
              \ use only: define the initial value of the\n                      \
              \  attempt parameter (default: 1).\n  --wrapper-prefix WRAPPER_PREFIX\n\
              \                        Prefix for URL created from wrapper directive\n\
              \                        (default: https://github.com/snakemake/snakemake-\n\
              \                        wrappers/raw/). Set this to a different URL\
              \ to use\n                        your fork or a local clone of the\
              \ repository, e.g.,\n                        use a git URL like\n  \
              \                      'git+file://path/to/your/local/clone@'.\n  --default-remote-provider\
              \ {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp,iRODS}\n                   \
              \     Specify default remote provider to be used for all\n         \
              \               input and output files that don't yet specify one.\n\
              \  --default-remote-prefix DEFAULT_REMOTE_PREFIX\n                 \
              \       Specify prefix for default remote provider. E.g. a\n       \
              \                 bucket name.\n  --no-shared-fs        Do not assume\
              \ that jobs share a common file system.\n                        When\
              \ this flag is activated, Snakemake will assume\n                  \
              \      that the filesystem on a cluster node is not shared\n       \
              \                 with other nodes. For example, this will lead to\n\
              \                        downloading remote files on each cluster node\n\
              \                        separately. Further, it won't take special\
              \ measures to\n                        deal with filesystem latency\
              \ issues. This option will\n                        in most cases only\
              \ make sense in combination with\n                        --default-remote-provider.\
              \ Further, when using\n                        --cluster you will have\
              \ to also provide --cluster-\n                        status. Only activate\
              \ this if you know what you are\n                        doing.\n  --greediness\
              \ GREEDINESS\n                        Set the greediness of scheduling.\
              \ This value between 0\n                        and 1 determines how\
              \ careful jobs are selected for\n                        execution.\
              \ The default value (1.0) provides the best\n                      \
              \  speed and still acceptable scheduling quality.\n  --no-hooks    \
              \        Do not invoke onstart, onsuccess or onerror hooks\n       \
              \                 after execution.\n  --overwrite-shellcmd OVERWRITE_SHELLCMD\n\
              \                        Provide a shell command that shall be executed\
              \ instead\n                        of those given in the workflow. This\
              \ is for debugging\n                        purposes only.\n  --debug\
              \               Allow to debug rules with e.g. PDB. This flag allows\n\
              \                        to set breakpoints in run blocks.\n  --runtime-profile\
              \ FILE\n                        Profile Snakemake and write the output\
              \ to FILE. This\n                        requires yappi to be installed.\n\
              \  --mode {0,1,2}        Set execution mode of Snakemake (internal use\
              \ only).\n  --show-failed-logs    Automatically display logs of failed\
              \ jobs.\n  --log-handler-script FILE\n                        Provide\
              \ a custom script containing a function 'def\n                     \
              \   log_handler(msg):'. Snakemake will call this function\n        \
              \                for every logging output (given as a dictionary\n \
              \                       msg)allowing to e.g. send notifications in the\
              \ form of\n                        e.g. slack messages or emails.\n\
              \  --log-service {none,slack}\n                        Set a specific\
              \ messaging service for logging\n                        output.Snakemake\
              \ will notify the service on errors and\n                        completed\
              \ execution.Currently only slack is supported.\n\nCLUSTER:\n  --cluster\
              \ CMD, -c CMD\n                        Execute snakemake rules with\
              \ the given submit command,\n                        e.g. qsub. Snakemake\
              \ compiles jobs into scripts that\n                        are submitted\
              \ to the cluster with the given command,\n                        once\
              \ all input files for a particular job are present.\n              \
              \          The submit command can be decorated to make it aware\n  \
              \                      of certain job properties (name, rulename, input,\n\
              \                        output, params, wildcards, log, threads and\n\
              \                        dependencies (see the argument below)), e.g.:\
              \ $\n                        snakemake --cluster 'qsub -pe threaded\
              \ {threads}'.\n  --cluster-sync CMD    cluster submission command will\
              \ block, returning the\n                        remote exitstatus upon\
              \ remote termination (for\n                        example, this should\
              \ be usedif the cluster command is\n                        'qsub -sync\
              \ y' (SGE)\n  --drmaa [ARGS]        Execute snakemake on a cluster accessed\
              \ via DRMAA,\n                        Snakemake compiles jobs into scripts\
              \ that are\n                        submitted to the cluster with the\
              \ given command, once\n                        all input files for a\
              \ particular job are present. ARGS\n                        can be used\
              \ to specify options of the underlying\n                        cluster\
              \ system, thereby using the job properties name,\n                 \
              \       rulename, input, output, params, wildcards, log,\n         \
              \               threads and dependencies, e.g.: --drmaa ' -pe threaded\n\
              \                        {threads}'. Note that ARGS must be given in\
              \ quotes and\n                        with a leading whitespace.\n \
              \ --cluster-config FILE, -u FILE\n                        A JSON or\
              \ YAML file that defines the wildcards used in\n                   \
              \     'cluster'for specific rules, instead of having them\n        \
              \                specified in the Snakefile. For example, for rule\n\
              \                        'job' you may define: { 'job' : { 'time' :\
              \ '24:00:00'\n                        } } to specify the time for rule\
              \ 'job'. You can\n                        specify more than one file.\
              \ The configuration files\n                        are merged with later\
              \ values overriding earlier ones.\n                        This option\
              \ is deprecated in favor of using --profile,\n                     \
              \   see docs.\n  --immediate-submit, --is\n                        Immediately\
              \ submit all jobs to the cluster instead of\n                      \
              \  waiting for present input files. This will fail,\n              \
              \          unless you make the cluster aware of job dependencies,\n\
              \                        e.g. via: $ snakemake --cluster 'sbatch --dependency\n\
              \                        {dependencies}. Assuming that your submit script\
              \ (here\n                        sbatch) outputs the generated job id\
              \ to the first\n                        stdout line, {dependencies}\
              \ will be filled with space\n                        separated job ids\
              \ this job depends on.\n  --jobscript SCRIPT, --js SCRIPT\n        \
              \                Provide a custom job script for submission to the\n\
              \                        cluster. The default script resides as 'jobscript.sh'\n\
              \                        in the installation directory.\n  --jobname\
              \ NAME, --jn NAME\n                        Provide a custom name for\
              \ the jobscript that is\n                        submitted to the cluster\
              \ (see --cluster). NAME is\n                        \"snakejob.{name}.{jobid}.sh\"\
              \ per default. The wildcard\n                        {jobid} has to\
              \ be present in the name.\n  --cluster-status CLUSTER_STATUS\n     \
              \                   Status command for cluster execution. This is only\n\
              \                        considered in combination with the --cluster\
              \ flag. If\n                        provided, Snakemake will use the\
              \ status command to\n                        determine if a job has\
              \ finished successfully or\n                        failed. For this\
              \ it is necessary that the submit\n                        command provided\
              \ to --cluster returns the cluster job\n                        id.\
              \ Then, the status command will be invoked with the\n              \
              \          job id. Snakemake expects it to return 'success' if\n   \
              \                     the job was successfull, 'failed' if the job failed\n\
              \                        and 'running' if the job still runs.\n  --drmaa-log-dir\
              \ DIR   Specify a directory in which stdout and stderr files\n     \
              \                   of DRMAA jobs will be written. The value may be\
              \ given\n                        as a relative path, in which case Snakemake\
              \ will use\n                        the current invocation directory\
              \ as the origin. If\n                        given, this will override\
              \ any given '-o' and/or '-e'\n                        native specification.\
              \ If not given, all DRMAA stdout\n                        and stderr\
              \ files are written to the current working\n                       \
              \ directory.\n\nKUBERNETES:\n  --kubernetes [NAMESPACE]\n          \
              \              Execute workflow in a kubernetes cluster (in the\n  \
              \                      cloud). NAMESPACE is the namespace you want to\
              \ use for\n                        your job (if nothing specified: 'default').\
              \ Usually,\n                        this requires --default-remote-provider\
              \ and --default-\n                        remote-prefix to be set to\
              \ a S3 or GS bucket where\n                        your . data shall\
              \ be stored. It is further advisable\n                        to activate\
              \ conda integration via --use-conda.\n  --container-image IMAGE\n  \
              \                      Docker image to use, e.g., when submitting jobs\
              \ to\n                        kubernetes. By default, this is\n    \
              \                    'https://hub.docker.com/r/snakemake/snakemake',\
              \ tagged\n                        with the same version as the currently\
              \ running\n                        Snakemake instance. Note that overwriting\
              \ this value\n                        is up to your responsibility.\
              \ Any used image has to\n                        contain a working snakemake\
              \ installation that is\n                        compatible with (or\
              \ ideally the same as) the currently\n                        running\
              \ version.\n\nTIBANNA:\n  --tibanna             Execute workflow on\
              \ AWS cloud using Tibanna. This\n                        requires --default-remote-prefix\
              \ to be set to S3\n                        bucket name and prefix (e.g.\n\
              \                        'bucketname/subdirectory') where input is already\n\
              \                        stored and output will be sent to. Using --tibanna\n\
              \                        implies --default-resources is set as default.\n\
              \                        Optionally, use --precommand to specify any\n\
              \                        preparation command to run before snakemake\
              \ command on\n                        the cloud (inside snakemake container\
              \ on Tibanna VM).\n                        Also, --use-conda, --use-singularity,\
              \ --config,\n                        --configfile are supported and\
              \ will be carried over.\n  --tibanna-sfn TIBANNA_SFN\n             \
              \           Name of Tibanna Unicorn step function (e.g.\n          \
              \              tibanna_unicorn_monty).This works as serverless\n   \
              \                     scheduler/resource allocator and must be deployed\n\
              \                        first using tibanna cli. (e.g. tibanna deploy_unicorn\n\
              \                        --usergroup=monty --buckets=bucketname)\n \
              \ --precommand PRECOMMAND\n                        Any command to execute\
              \ before snakemake command on AWS\n                        cloud such\
              \ as wget, git clone, unzip, etc. This is\n                        used\
              \ with --tibanna.Do not include input/output\n                     \
              \   download/upload commands - file transfer between S3\n          \
              \              bucket and the run environment (container) is\n     \
              \                   automatically handled by Tibanna.\n  --tibanna-config\
              \ TIBANNA_CONFIG [TIBANNA_CONFIG ...]\n                        Additional\
              \ tibanan config e.g. --tibanna-config\n                        spot_instance=true\
              \ subnet=<subnet_id> security\n                        group=<security_group_id>\n\
              \nCONDA:\n  --use-conda           If defined in the rule, run job in\
              \ a conda\n                        environment. If this flag is not\
              \ set, the conda\n                        directive is ignored.\n  --list-conda-envs\
              \     List all conda environments and their location on\n          \
              \              disk.\n  --cleanup-conda       Cleanup unused conda environments.\n\
              \  --conda-prefix DIR    Specify a directory in which the 'conda' and\
              \ 'conda-\n                        archive' directories are created.\
              \ These are used to\n                        store conda environments\
              \ and their archives,\n                        respectively. If not\
              \ supplied, the value is set to the\n                        '.snakemake'\
              \ directory relative to the invocation\n                        directory.\
              \ If supplied, the `--use-conda` flag must\n                       \
              \ also be set. The value may be given as a relative\n              \
              \          path, which will be extrapolated to the invocation\n    \
              \                    directory, or as an absolute path.\n  --create-envs-only\
              \    If specified, only creates the job-specific conda\n           \
              \             environments then exits. The `--use-conda` flag must\n\
              \                        also be set.\n\nSINGULARITY:\n  --use-singularity\
              \     If defined in the rule, run job within a singularity\n       \
              \                 container. If this flag is not set, the singularity\n\
              \                        directive is ignored.\n  --singularity-prefix\
              \ DIR\n                        Specify a directory in which singularity\
              \ images will\n                        be stored.If not supplied, the\
              \ value is set to the\n                        '.snakemake' directory\
              \ relative to the invocation\n                        directory. If\
              \ supplied, the `--use-singularity` flag\n                        must\
              \ also be set. The value may be given as a relative\n              \
              \          path, which will be extrapolated to the invocation\n    \
              \                    directory, or as an absolute path.\n  --singularity-args\
              \ ARGS\n                        Pass additional args to singularity.\n\
              \nENVIRONMENT MODULES:\n  --use-envmodules      If defined in the rule,\
              \ run job within the given\n                        environment modules,\
              \ loaded in the given order. This\n                        can be combined\
              \ with --use-conda and --use-\n                        singularity,\
              \ which will then be only used as a\n                        fallback\
              \ for rules which don't define environment\n                       \
              \ modules.\n"
            generated_using: *id004
            docker_image:
          subcommands: []
          usage: []
          help_flag: !Flag
            optional: true
            synonyms:
            - --help
            description: show Snakemake help (or snakemake -h)
            args: !EmptyFlagArg {}
          usage_flag:
          version_flag:
          help_text: "ERROR\ntarget: No such file or directory\nProphane Pipeline\
            \ (powered by Snakemake)\n\nUsage: /usr/local/bin/prophane CONFIG_FILE\
            \ [Snakemake options]\n\n Full list of parameters:\n   --help        \
            \         show Snakemake help (or snakemake -h)\n   --list-dbs       \
            \      print list of configured databases\n                          databases\
            \ are looked up in 'db_base_dir' configured in:\n                    \
            \          /usr/local/opt/prophane/general_config.yaml\n   --list-styles\
            \          print list of available input file styles\n               \
            \           styles are looked up in the following folder:\n          \
            \                    /usr/local/opt/prophane/styles\n   --db-maintenance\
            \       trigger database maintenance scripts\n                       \
            \   will migrate database structure from deprecated structure to most\
            \ recent\n\n Useful Snakemake parameters:\n   -j, --cores            number\
            \ of cores\n   -k, --keep-going       go on with independent jobs if a\
            \ job fails\n   -n, --dryrun           do not execute anything\n   -p,\
            \ --printshellcmds   print out the shell commands that will be executed\n\
            \   -t, --timestamp  \t\tadd a timestamp to all logging output\n\n"
          generated_using: *id004
          docker_image:
        - !Command
          command: *id005
          positional: []
          named:
          - !Flag
            optional: true
            synonyms:
            - --list-dbs
            description: "print list of configured databases\ndatabases are looked\
              \ up in 'db_base_dir' configured in:\n/usr/local/opt/prophane/general_config.yaml"
            args: !EmptyFlagArg {}
          - !Flag
            optional: true
            synonyms:
            - --list-styles
            description: "print list of available input file styles\nstyles are looked\
              \ up in the following folder:\n/usr/local/opt/prophane/styles"
            args: !EmptyFlagArg {}
          - !Flag
            optional: true
            synonyms:
            - --db-maintenance
            description: "trigger database maintenance scripts\nwill migrate database\
              \ structure from deprecated structure to most recent"
            args: !EmptyFlagArg {}
          - !Flag
            optional: true
            synonyms:
            - -j
            - --cores
            description: number of cores
            args: !EmptyFlagArg {}
          - !Flag
            optional: true
            synonyms:
            - -k
            - --keep-going
            description: go on with independent jobs if a job fails
            args: !EmptyFlagArg {}
          - !Flag
            optional: true
            synonyms:
            - -n
            - --dryrun
            description: do not execute anything
            args: !EmptyFlagArg {}
          - !Flag
            optional: true
            synonyms:
            - -p
            - --printshellcmds
            description: print out the shell commands that will be executed
            args: !EmptyFlagArg {}
          - !Flag
            optional: true
            synonyms:
            - -t
            - --timestamp
            description: add a timestamp to all logging output
            args: !EmptyFlagArg {}
          parent: *id007
          subcommands: []
          usage: []
          help_flag: !Flag
            optional: true
            synonyms:
            - --help
            description: show Snakemake help (or snakemake -h)
            args: !EmptyFlagArg {}
          usage_flag:
          version_flag:
          help_text: "ERROR\ndisk.: No such file or directory\nProphane Pipeline (powered\
            \ by Snakemake)\n\nUsage: /usr/local/bin/prophane CONFIG_FILE [Snakemake\
            \ options]\n\n Full list of parameters:\n   --help                 show\
            \ Snakemake help (or snakemake -h)\n   --list-dbs             print list\
            \ of configured databases\n                          databases are looked\
            \ up in 'db_base_dir' configured in:\n                              /usr/local/opt/prophane/general_config.yaml\n\
            \   --list-styles          print list of available input file styles\n\
            \                          styles are looked up in the following folder:\n\
            \                              /usr/local/opt/prophane/styles\n   --db-maintenance\
            \       trigger database maintenance scripts\n                       \
            \   will migrate database structure from deprecated structure to most\
            \ recent\n\n Useful Snakemake parameters:\n   -j, --cores            number\
            \ of cores\n   -k, --keep-going       go on with independent jobs if a\
            \ job fails\n   -n, --dryrun           do not execute anything\n   -p,\
            \ --printshellcmds   print out the shell commands that will be executed\n\
            \   -t, --timestamp  \t\tadd a timestamp to all logging output\n\n"
          generated_using: *id004
          docker_image:
        usage: []
        help_flag:
        usage_flag:
        version_flag:
        help_text: "usage: snakemake [-h] [--dry-run] [--profile PROFILE]\n      \
          \           [--cache RULE [RULE ...]] [--snakefile FILE] [--cores [N]]\n\
          \                 [--local-cores N] [--resources [NAME=INT [NAME=INT ...]]]\n\
          \                 [--set-threads [RULE=THREADS [RULE=THREADS ...]]]\n  \
          \               [--default-resources [NAME=INT [NAME=INT ...]]]\n      \
          \           [--config [KEY=VALUE [KEY=VALUE ...]]]\n                 [--configfile\
          \ FILE [FILE ...]] [--directory DIR] [--touch]\n                 [--keep-going]\
          \ [--force] [--forceall]\n                 [--forcerun [TARGET [TARGET ...]]]\n\
          \                 [--prioritize TARGET [TARGET ...]]\n                 [--batch\
          \ RULE=BATCH/BATCHES] [--until TARGET [TARGET ...]]\n                 [--omit-from\
          \ TARGET [TARGET ...]] [--rerun-incomplete]\n                 [--shadow-prefix\
          \ DIR] [--report [HTMLFILE]]\n                 [--lint [{text,json}]] [--export-cwl\
          \ FILE] [--list]\n                 [--list-target-rules] [--dag] [--rulegraph]\
          \ [--filegraph]\n                 [--d3dag] [--summary] [--detailed-summary]\
          \ [--archive FILE]\n                 [--cleanup-metadata FILE [FILE ...]]\
          \ [--cleanup-shadow]\n                 [--skip-script-cleanup] [--unlock]\
          \ [--list-version-changes]\n                 [--list-code-changes] [--list-input-changes]\n\
          \                 [--list-params-changes] [--list-untracked]\n         \
          \        [--delete-all-output] [--delete-temp-output]\n                \
          \ [--bash-completion] [--keep-incomplete] [--version]\n                \
          \ [--reason] [--gui [PORT]] [--printshellcmds] [--debug-dag]\n         \
          \        [--stats FILE] [--nocolor] [--quiet] [--print-compilation]\n  \
          \               [--verbose] [--force-use-threads] [--allow-ambiguity]\n\
          \                 [--nolock] [--ignore-incomplete] [--latency-wait SECONDS]\n\
          \                 [--wait-for-files [FILE [FILE ...]]] [--notemp]\n    \
          \             [--keep-remote] [--keep-target-files]\n                 [--allowed-rules\
          \ ALLOWED_RULES [ALLOWED_RULES ...]]\n                 [--max-jobs-per-second\
          \ MAX_JOBS_PER_SECOND]\n                 [--max-status-checks-per-second\
          \ MAX_STATUS_CHECKS_PER_SECOND]\n                 [--restart-times RESTART_TIMES]\
          \ [--attempt ATTEMPT]\n                 [--wrapper-prefix WRAPPER_PREFIX]\n\
          \                 [--default-remote-provider {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp,iRODS}]\n\
          \                 [--default-remote-prefix DEFAULT_REMOTE_PREFIX]\n    \
          \             [--no-shared-fs] [--greediness GREEDINESS] [--no-hooks]\n\
          \                 [--overwrite-shellcmd OVERWRITE_SHELLCMD] [--debug]\n\
          \                 [--runtime-profile FILE] [--mode {0,1,2}]\n          \
          \       [--show-failed-logs] [--log-handler-script FILE]\n             \
          \    [--log-service {none,slack}]\n                 [--cluster CMD | --cluster-sync\
          \ CMD | --drmaa [ARGS]]\n                 [--cluster-config FILE] [--immediate-submit]\n\
          \                 [--jobscript SCRIPT] [--jobname NAME]\n              \
          \   [--cluster-status CLUSTER_STATUS] [--drmaa-log-dir DIR]\n          \
          \       [--kubernetes [NAMESPACE]] [--container-image IMAGE]\n         \
          \        [--tibanna] [--tibanna-sfn TIBANNA_SFN]\n                 [--precommand\
          \ PRECOMMAND]\n                 [--tibanna-config TIBANNA_CONFIG [TIBANNA_CONFIG\
          \ ...]]\n                 [--use-conda] [--list-conda-envs] [--cleanup-conda]\n\
          \                 [--conda-prefix DIR] [--create-envs-only] [--use-singularity]\n\
          \                 [--singularity-prefix DIR] [--singularity-args ARGS]\n\
          \                 [--use-envmodules]\n                 [target [target ...]]\n\
          \nSnakemake is a Python based language and execution environment for GNU\
          \ Make-\nlike workflows.\n\noptional arguments:\n  -h, --help          \
          \  show this help message and exit\n\nEXECUTION:\n  target             \
          \   Targets to build. May be rules or files.\n  --dry-run, --dryrun, -n\n\
          \                        Do not execute anything, and display what would\
          \ be\n                        done. If you have a very large workflow, use\
          \ --dry-run\n                        --quiet to just print a summary of\
          \ the DAG of jobs.\n  --profile PROFILE     Name of profile to use for configuring\
          \ Snakemake.\n                        Snakemake will search for a corresponding\
          \ folder in\n                        /etc/xdg/snakemake and /root/.config/snakemake.\n\
          \                        Alternatively, this can be an absolute or relative\n\
          \                        path. The profile folder has to contain a file\n\
          \                        'config.yaml'. This file can be used to set default\n\
          \                        values for command line options in YAML format.\
          \ For\n                        example, '--cluster qsub' becomes 'cluster:\
          \ qsub' in\n                        the YAML file. Profiles can be obtained\
          \ from\n                        https://github.com/snakemake-profiles.\n\
          \  --cache RULE [RULE ...]\n                        Store output files of\
          \ given rules in a central cache\n                        given by the environment\
          \ variable\n                        $SNAKEMAKE_OUTPUT_CACHE. Likewise, retrieve\
          \ output\n                        files of the given rules from this cache\
          \ if they have\n                        been created before (by anybody\
          \ writing to the same\n                        cache), instead of actually\
          \ executing the rules.\n                        Output files are identified\
          \ by hashing all steps,\n                        parameters and software\
          \ stack (conda envs or\n                        containers) needed to create\
          \ them.\n  --snakefile FILE, -s FILE\n                        The workflow\
          \ definition in form of a\n                        snakefile.Usually, you\
          \ should not need to specify\n                        this. By default,\
          \ Snakemake will search for\n                        'Snakefile', 'snakefile',\
          \ 'workflow/Snakefile',\n                        'workflow/snakefile' beneath\
          \ the current working\n                        directory, in this order.\
          \ Only if you definitely want\n                        a different layout,\
          \ you need to use this parameter.\n  --cores [N], --jobs [N], -j [N]\n \
          \                       Use at most N CPU cores/jobs in parallel. If N is\n\
          \                        omitted or 'all', the limit is set to the number\
          \ of\n                        available CPU cores.\n  --local-cores N  \
          \     In cluster mode, use at most N cores of the host\n               \
          \         machine in parallel (default: number of CPU cores of\n       \
          \                 the host). The cores are used to execute local rules.\n\
          \                        This option is ignored when not in cluster mode.\n\
          \  --resources [NAME=INT [NAME=INT ...]], --res [NAME=INT [NAME=INT ...]]\n\
          \                        Define additional resources that shall constrain\
          \ the\n                        scheduling analogously to threads (see above).\
          \ A\n                        resource is defined as a name and an integer\
          \ value.\n                        E.g. --resources gpu=1. Rules can use\
          \ resources by\n                        defining the resource keyword, e.g.\
          \ resources: gpu=1.\n                        If now two rules require 1\
          \ of the resource 'gpu' they\n                        won't be run in parallel\
          \ by the scheduler.\n  --set-threads [RULE=THREADS [RULE=THREADS ...]]\n\
          \                        Overwrite thread usage of rules. This allows to\
          \ fine-\n                        tune workflow parallelization. In particular,\
          \ this is\n                        helpful to target certain cluster nodes\
          \ by e.g.\n                        shifting a rule to use more, or less\
          \ threads than\n                        defined in the workflow. Thereby,\
          \ THREADS has to be a\n                        positive integer, and RULE\
          \ has to be the name of the\n                        rule.\n  --default-resources\
          \ [NAME=INT [NAME=INT ...]], --default-res [NAME=INT [NAME=INT ...]]\n \
          \                       Define default values of resources for rules that\
          \ do\n                        not define their own values. In addition to\
          \ plain\n                        integers, python expressions over inputsize\
          \ are\n                        allowed (e.g. '2*input.size_mb').When specifying\
          \ this\n                        without any arguments (--default-resources),\
          \ it\n                        defines 'mem_mb=max(2*input.size_mb, 1000)'\n\
          \                        'disk_mb=max(2*input.size_mb, 1000)', i.e., default\n\
          \                        disk and mem usage is twice the input file size\
          \ but at\n                        least 1GB.\n  --config [KEY=VALUE [KEY=VALUE\
          \ ...]], -C [KEY=VALUE [KEY=VALUE ...]]\n                        Set or\
          \ overwrite values in the workflow config object.\n                    \
          \    The workflow config object is accessible as variable\n            \
          \            config inside the workflow. Default values can be set\n   \
          \                     by providing a JSON file (see Documentation).\n  --configfile\
          \ FILE [FILE ...], --configfiles FILE [FILE ...]\n                     \
          \   Specify or overwrite the config file of the workflow\n             \
          \           (see the docs). Values specified in JSON or YAML\n         \
          \               format are available in the global config dictionary\n \
          \                       inside the workflow. Multiple files overwrite each\n\
          \                        other in the given order.\n  --directory DIR, -d\
          \ DIR\n                        Specify working directory (relative paths\
          \ in the\n                        snakefile will use this as their origin).\n\
          \  --touch, -t           Touch output files (mark them up to date without\n\
          \                        really changing them) instead of running their\n\
          \                        commands. This is used to pretend that the rules\
          \ were\n                        executed, in order to fool future invocations\
          \ of\n                        snakemake. Fails if a file does not yet exist.\
          \ Note\n                        that this will only touch files that would\
          \ otherwise\n                        be recreated by Snakemake (e.g. because\
          \ their input\n                        files are newer). For enforcing a\
          \ touch, combine this\n                        with --force, --forceall,\
          \ or --forcerun. Note however\n                        that you loose the\
          \ provenance information when the\n                        files have been\
          \ created in realitiy. Hence, this\n                        should be used\
          \ only as a last resort.\n  --keep-going, -k      Go on with independent\
          \ jobs if a job fails.\n  --force, -f           Force the execution of the\
          \ selected target or the\n                        first rule regardless\
          \ of already created output.\n  --forceall, -F        Force the execution\
          \ of the selected (or the first)\n                        rule and all rules\
          \ it is dependent on regardless of\n                        already created\
          \ output.\n  --forcerun [TARGET [TARGET ...]], -R [TARGET [TARGET ...]]\n\
          \                        Force the re-execution or creation of the given\
          \ rules\n                        or files. Use this option if you changed\
          \ a rule and\n                        want to have all its output in your\
          \ workflow updated.\n  --prioritize TARGET [TARGET ...], -P TARGET [TARGET\
          \ ...]\n                        Tell the scheduler to assign creation of\
          \ given targets\n                        (and all their dependencies) highest\
          \ priority.\n                        (EXPERIMENTAL)\n  --batch RULE=BATCH/BATCHES\n\
          \                        Only create the given BATCH of the input files\
          \ of the\n                        given RULE. This can be used to iteratively\
          \ run parts\n                        of very large workflows. Only the execution\
          \ plan of\n                        the relevant part of the workflow has\
          \ to be\n                        calculated, thereby speeding up DAG computation.\
          \ It is\n                        recommended to provide the most suitable\
          \ rule for\n                        batching when documenting a workflow.\
          \ It should be\n                        some aggregating rule that would\
          \ be executed only\n                        once, and has a large number\
          \ of input files. For\n                        example, it can be a rule\
          \ that aggregates over\n                        samples.\n  --until TARGET\
          \ [TARGET ...], -U TARGET [TARGET ...]\n                        Runs the\
          \ pipeline until it reaches the specified rules\n                      \
          \  or files. Only runs jobs that are dependencies of the\n             \
          \           specified rule or files, does not run sibling DAGs.\n  --omit-from\
          \ TARGET [TARGET ...], -O TARGET [TARGET ...]\n                        Prevent\
          \ the execution or creation of the given rules\n                       \
          \ or files as well as any rules or files that are\n                    \
          \    downstream of these targets in the DAG. Also runs jobs\n          \
          \              in sibling DAGs that are independent of the rules or\n  \
          \                      files specified here.\n  --rerun-incomplete, --ri\n\
          \                        Re-run all jobs the output of which is recognized\
          \ as\n                        incomplete.\n  --shadow-prefix DIR   Specify\
          \ a directory in which the 'shadow' directory is\n                     \
          \   created. If not supplied, the value is set to the\n                \
          \        '.snakemake' directory relative to the working\n              \
          \          directory.\n\nUTILITIES:\n  --report [HTMLFILE]   Create an HTML\
          \ report with results and statistics. If\n                        no filename\
          \ is given, report.html is the default.\n  --lint [{text,json}]  Perform\
          \ linting on the given workflow. This will print\n                     \
          \   snakemake specific suggestions to improve code quality\n           \
          \             (work in progress, more lints to be added in the\n       \
          \                 future). If no argument is provided, plain text output\n\
          \                        is used.\n  --export-cwl FILE     Compile workflow\
          \ to CWL and store it in given FILE.\n  --list, -l            Show available\
          \ rules in given Snakefile.\n  --list-target-rules, --lt\n             \
          \           Show available target rules in given Snakefile.\n  --dag   \
          \              Do not execute anything and print the directed acyclic\n\
          \                        graph of jobs in the dot language. Recommended\
          \ use on\n                        Unix systems: snakemake --dag | dot |\
          \ display\n  --rulegraph           Do not execute anything and print the\
          \ dependency graph\n                        of rules in the dot language.\
          \ This will be less\n                        crowded than above DAG of jobs,\
          \ but also show less\n                        information. Note that each\
          \ rule is displayed once,\n                        hence the displayed graph\
          \ will be cyclic if a rule\n                        appears in several steps\
          \ of the workflow. Use this if\n                        above option leads\
          \ to a DAG that is too large.\n                        Recommended use on\
          \ Unix systems: snakemake --rulegraph\n                        | dot | display\n\
          \  --filegraph           Do not execute anything and print the dependency\
          \ graph\n                        of rules with their input and output files\
          \ in the dot\n                        language. This is an intermediate\
          \ solution between\n                        above DAG of jobs and the rule\
          \ graph. Note that each\n                        rule is displayed once,\
          \ hence the displayed graph will\n                        be cyclic if a\
          \ rule appears in several steps of the\n                        workflow.\
          \ Use this if above option leads to a DAG that\n                       \
          \ is too large. Recommended use on Unix systems:\n                     \
          \   snakemake --filegraph | dot | display\n  --d3dag               Print\
          \ the DAG in D3.js compatible JSON format.\n  --summary, -S         Print\
          \ a summary of all files created by the workflow.\n                    \
          \    The has the following columns: filename, modification\n           \
          \             time, rule version, status, plan. Thereby rule version\n \
          \                       contains the versionthe file was created with (see\
          \ the\n                        version keyword of rules), and status denotes\
          \ whether\n                        the file is missing, its input files\
          \ are newer or if\n                        version or implementation of\
          \ the rule changed since\n                        file creation. Finally\
          \ the last column denotes whether\n                        the file will\
          \ be updated or created during the next\n                        workflow\
          \ execution.\n  --detailed-summary, -D\n                        Print a\
          \ summary of all files created by the workflow.\n                      \
          \  The has the following columns: filename, modification\n             \
          \           time, rule version, input file(s), shell command,\n        \
          \                status, plan. Thereby rule version contains the\n     \
          \                   version the file was created with (see the version\n\
          \                        keyword of rules), and status denotes whether the\
          \ file\n                        is missing, its input files are newer or\
          \ if version or\n                        implementation of the rule changed\
          \ since file\n                        creation. The input file and shell\
          \ command columns are\n                        self explanatory. Finally\
          \ the last column denotes\n                        whether the file will\
          \ be updated or created during the\n                        next workflow\
          \ execution.\n  --archive FILE        Archive the workflow into the given\
          \ tar archive FILE.\n                        The archive will be created\
          \ such that the workflow can\n                        be re-executed on\
          \ a vanilla system. The function needs\n                        conda and\
          \ git to be installed. It will archive every\n                        file\
          \ that is under git version control. Note that it\n                    \
          \    is best practice to have the Snakefile, config files,\n           \
          \             and scripts under version control. Hence, they will be\n \
          \                       included in the archive. Further, it will add input\n\
          \                        files that are not generated by by the workflow\
          \ itself\n                        and conda environments. Note that symlinks\
          \ are\n                        dereferenced. Supported formats are .tar,\
          \ .tar.gz,\n                        .tar.bz2 and .tar.xz.\n  --cleanup-metadata\
          \ FILE [FILE ...], --cm FILE [FILE ...]\n                        Cleanup\
          \ the metadata of given files. That means that\n                       \
          \ snakemake removes any tracked version info, and any\n                \
          \        marks that files are incomplete.\n  --cleanup-shadow      Cleanup\
          \ old shadow directories which have not been\n                        deleted\
          \ due to failures or power loss.\n  --skip-script-cleanup\n            \
          \            Don't delete wrapper scripts used for execution\n  --unlock\
          \              Remove a lock on the working directory.\n  --list-version-changes,\
          \ --lv\n                        List all output files that have been created\
          \ with a\n                        different version (as determined by the\
          \ version\n                        keyword).\n  --list-code-changes, --lc\n\
          \                        List all output files for which the rule body (run\
          \ or\n                        shell) have changed in the Snakefile.\n  --list-input-changes,\
          \ --li\n                        List all output files for which the defined\
          \ input\n                        files have changed in the Snakefile (e.g.\
          \ new input\n                        files were added in the rule definition\
          \ or files were\n                        renamed). For listing input file\
          \ modification in the\n                        filesystem, use --summary.\n\
          \  --list-params-changes, --lp\n                        List all output\
          \ files for which the defined params\n                        have changed\
          \ in the Snakefile.\n  --list-untracked, --lu\n                        List\
          \ all files in the working directory that are not\n                    \
          \    used in the workflow. This can be used e.g. for\n                 \
          \       identifying leftover files. Hidden files and\n                 \
          \       directories are ignored.\n  --delete-all-output   Remove all files\
          \ generated by the workflow. Use\n                        together with\
          \ --dry-run to list files without actually\n                        deleting\
          \ anything. Note that this will not recurse\n                        into\
          \ subworkflows. Write-protected files are not\n                        removed.\
          \ Nevertheless, use with care!\n  --delete-temp-output  Remove all temporary\
          \ files generated by the workflow.\n                        Use together\
          \ with --dry-run to list files without\n                        actually\
          \ deleting anything. Note that this will not\n                        recurse\
          \ into subworkflows.\n  --bash-completion     Output code to register bash\
          \ completion for snakemake.\n                        Put the following in\
          \ your .bashrc (including the\n                        accents): `snakemake\
          \ --bash-completion` or issue it in\n                        an open terminal\
          \ session.\n  --keep-incomplete     Do not remove incomplete output files\
          \ by failed jobs.\n  --version, -v         show program's version number\
          \ and exit\n\nOUTPUT:\n  --reason, -r          Print the reason for each\
          \ executed rule.\n  --gui [PORT]          Serve an HTML based user interface\
          \ to the given\n                        network and port e.g. 168.129.10.15:8000.\
          \ By default\n                        Snakemake is only available in the\
          \ local network\n                        (default port: 8000). To make Snakemake\
          \ listen to all\n                        ip addresses add the special host\
          \ address 0.0.0.0 to\n                        the url (0.0.0.0:8000). This\
          \ is important if Snakemake\n                        is used in a virtualised\
          \ environment like Docker. If\n                        possible, a browser\
          \ window is opened.\n  --printshellcmds, -p  Print out the shell commands\
          \ that will be executed.\n  --debug-dag           Print candidate and selected\
          \ jobs (including their\n                        wildcards) while inferring\
          \ DAG. This can help to debug\n                        unexpected DAG topology\
          \ or errors.\n  --stats FILE          Write stats about Snakefile execution\
          \ in JSON format\n                        to the given file.\n  --nocolor\
          \             Do not use a colored output.\n  --quiet, -q           Do not\
          \ output any progress or rule information.\n  --print-compilation   Print\
          \ the python representation of the workflow.\n  --verbose             Print\
          \ debugging output.\n\nBEHAVIOR:\n  --force-use-threads   Force threads\
          \ rather than processes. Helpful if shared\n                        memory\
          \ (/dev/shm) is full or unavailable.\n  --allow-ambiguity, -a\n        \
          \                Don't check for ambiguous rules and simply use the\n  \
          \                      first if several can produce the same file. This\n\
          \                        allows the user to prioritize rules by their order\
          \ in\n                        the snakefile.\n  --nolock              Do\
          \ not lock the working directory\n  --ignore-incomplete, --ii\n        \
          \                Do not check for incomplete output files.\n  --latency-wait\
          \ SECONDS, --output-wait SECONDS, -w SECONDS\n                        Wait\
          \ given seconds if an output file of a job is not\n                    \
          \    present after the job finished. This helps if your\n              \
          \          filesystem suffers from latency (default 5).\n  --wait-for-files\
          \ [FILE [FILE ...]]\n                        Wait --latency-wait seconds\
          \ for these files to be\n                        present before executing\
          \ the workflow. This option is\n                        used internally\
          \ to handle filesystem latency in\n                        cluster environments.\n\
          \  --notemp, --nt        Ignore temp() declarations. This is useful when\n\
          \                        running only a part of the workflow, since temp()\n\
          \                        would lead to deletion of probably needed files\
          \ by\n                        other parts of the workflow.\n  --keep-remote\
          \         Keep local copies of remote input files.\n  --keep-target-files\
          \   Do not adjust the paths of given target files relative\n           \
          \             to the working directory.\n  --allowed-rules ALLOWED_RULES\
          \ [ALLOWED_RULES ...]\n                        Only consider given rules.\
          \ If omitted, all rules in\n                        Snakefile are used.\
          \ Note that this is intended\n                        primarily for internal\
          \ use and may lead to unexpected\n                        results otherwise.\n\
          \  --max-jobs-per-second MAX_JOBS_PER_SECOND\n                        Maximal\
          \ number of cluster/drmaa jobs per second,\n                        default\
          \ is 10, fractions allowed.\n  --max-status-checks-per-second MAX_STATUS_CHECKS_PER_SECOND\n\
          \                        Maximal number of job status checks per second,\n\
          \                        default is 10, fractions allowed.\n  --restart-times\
          \ RESTART_TIMES\n                        Number of times to restart failing\
          \ jobs (defaults to\n                        0).\n  --attempt ATTEMPT  \
          \   Internal use only: define the initial value of the\n               \
          \         attempt parameter (default: 1).\n  --wrapper-prefix WRAPPER_PREFIX\n\
          \                        Prefix for URL created from wrapper directive\n\
          \                        (default: https://github.com/snakemake/snakemake-\n\
          \                        wrappers/raw/). Set this to a different URL to\
          \ use\n                        your fork or a local clone of the repository,\
          \ e.g.,\n                        use a git URL like\n                  \
          \      'git+file://path/to/your/local/clone@'.\n  --default-remote-provider\
          \ {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp,iRODS}\n                       \
          \ Specify default remote provider to be used for all\n                 \
          \       input and output files that don't yet specify one.\n  --default-remote-prefix\
          \ DEFAULT_REMOTE_PREFIX\n                        Specify prefix for default\
          \ remote provider. E.g. a\n                        bucket name.\n  --no-shared-fs\
          \        Do not assume that jobs share a common file system.\n         \
          \               When this flag is activated, Snakemake will assume\n   \
          \                     that the filesystem on a cluster node is not shared\n\
          \                        with other nodes. For example, this will lead to\n\
          \                        downloading remote files on each cluster node\n\
          \                        separately. Further, it won't take special measures\
          \ to\n                        deal with filesystem latency issues. This\
          \ option will\n                        in most cases only make sense in\
          \ combination with\n                        --default-remote-provider. Further,\
          \ when using\n                        --cluster you will have to also provide\
          \ --cluster-\n                        status. Only activate this if you\
          \ know what you are\n                        doing.\n  --greediness GREEDINESS\n\
          \                        Set the greediness of scheduling. This value between\
          \ 0\n                        and 1 determines how careful jobs are selected\
          \ for\n                        execution. The default value (1.0) provides\
          \ the best\n                        speed and still acceptable scheduling\
          \ quality.\n  --no-hooks            Do not invoke onstart, onsuccess or\
          \ onerror hooks\n                        after execution.\n  --overwrite-shellcmd\
          \ OVERWRITE_SHELLCMD\n                        Provide a shell command that\
          \ shall be executed instead\n                        of those given in the\
          \ workflow. This is for debugging\n                        purposes only.\n\
          \  --debug               Allow to debug rules with e.g. PDB. This flag allows\n\
          \                        to set breakpoints in run blocks.\n  --runtime-profile\
          \ FILE\n                        Profile Snakemake and write the output to\
          \ FILE. This\n                        requires yappi to be installed.\n\
          \  --mode {0,1,2}        Set execution mode of Snakemake (internal use only).\n\
          \  --show-failed-logs    Automatically display logs of failed jobs.\n  --log-handler-script\
          \ FILE\n                        Provide a custom script containing a function\
          \ 'def\n                        log_handler(msg):'. Snakemake will call\
          \ this function\n                        for every logging output (given\
          \ as a dictionary\n                        msg)allowing to e.g. send notifications\
          \ in the form of\n                        e.g. slack messages or emails.\n\
          \  --log-service {none,slack}\n                        Set a specific messaging\
          \ service for logging\n                        output.Snakemake will notify\
          \ the service on errors and\n                        completed execution.Currently\
          \ only slack is supported.\n\nCLUSTER:\n  --cluster CMD, -c CMD\n      \
          \                  Execute snakemake rules with the given submit command,\n\
          \                        e.g. qsub. Snakemake compiles jobs into scripts\
          \ that\n                        are submitted to the cluster with the given\
          \ command,\n                        once all input files for a particular\
          \ job are present.\n                        The submit command can be decorated\
          \ to make it aware\n                        of certain job properties (name,\
          \ rulename, input,\n                        output, params, wildcards, log,\
          \ threads and\n                        dependencies (see the argument below)),\
          \ e.g.: $\n                        snakemake --cluster 'qsub -pe threaded\
          \ {threads}'.\n  --cluster-sync CMD    cluster submission command will block,\
          \ returning the\n                        remote exitstatus upon remote termination\
          \ (for\n                        example, this should be usedif the cluster\
          \ command is\n                        'qsub -sync y' (SGE)\n  --drmaa [ARGS]\
          \        Execute snakemake on a cluster accessed via DRMAA,\n          \
          \              Snakemake compiles jobs into scripts that are\n         \
          \               submitted to the cluster with the given command, once\n\
          \                        all input files for a particular job are present.\
          \ ARGS\n                        can be used to specify options of the underlying\n\
          \                        cluster system, thereby using the job properties\
          \ name,\n                        rulename, input, output, params, wildcards,\
          \ log,\n                        threads and dependencies, e.g.: --drmaa\
          \ ' -pe threaded\n                        {threads}'. Note that ARGS must\
          \ be given in quotes and\n                        with a leading whitespace.\n\
          \  --cluster-config FILE, -u FILE\n                        A JSON or YAML\
          \ file that defines the wildcards used in\n                        'cluster'for\
          \ specific rules, instead of having them\n                        specified\
          \ in the Snakefile. For example, for rule\n                        'job'\
          \ you may define: { 'job' : { 'time' : '24:00:00'\n                    \
          \    } } to specify the time for rule 'job'. You can\n                 \
          \       specify more than one file. The configuration files\n          \
          \              are merged with later values overriding earlier ones.\n \
          \                       This option is deprecated in favor of using --profile,\n\
          \                        see docs.\n  --immediate-submit, --is\n       \
          \                 Immediately submit all jobs to the cluster instead of\n\
          \                        waiting for present input files. This will fail,\n\
          \                        unless you make the cluster aware of job dependencies,\n\
          \                        e.g. via: $ snakemake --cluster 'sbatch --dependency\n\
          \                        {dependencies}. Assuming that your submit script\
          \ (here\n                        sbatch) outputs the generated job id to\
          \ the first\n                        stdout line, {dependencies} will be\
          \ filled with space\n                        separated job ids this job\
          \ depends on.\n  --jobscript SCRIPT, --js SCRIPT\n                     \
          \   Provide a custom job script for submission to the\n                \
          \        cluster. The default script resides as 'jobscript.sh'\n       \
          \                 in the installation directory.\n  --jobname NAME, --jn\
          \ NAME\n                        Provide a custom name for the jobscript\
          \ that is\n                        submitted to the cluster (see --cluster).\
          \ NAME is\n                        \"snakejob.{name}.{jobid}.sh\" per default.\
          \ The wildcard\n                        {jobid} has to be present in the\
          \ name.\n  --cluster-status CLUSTER_STATUS\n                        Status\
          \ command for cluster execution. This is only\n                        considered\
          \ in combination with the --cluster flag. If\n                        provided,\
          \ Snakemake will use the status command to\n                        determine\
          \ if a job has finished successfully or\n                        failed.\
          \ For this it is necessary that the submit\n                        command\
          \ provided to --cluster returns the cluster job\n                      \
          \  id. Then, the status command will be invoked with the\n             \
          \           job id. Snakemake expects it to return 'success' if\n      \
          \                  the job was successfull, 'failed' if the job failed\n\
          \                        and 'running' if the job still runs.\n  --drmaa-log-dir\
          \ DIR   Specify a directory in which stdout and stderr files\n         \
          \               of DRMAA jobs will be written. The value may be given\n\
          \                        as a relative path, in which case Snakemake will\
          \ use\n                        the current invocation directory as the origin.\
          \ If\n                        given, this will override any given '-o' and/or\
          \ '-e'\n                        native specification. If not given, all\
          \ DRMAA stdout\n                        and stderr files are written to\
          \ the current working\n                        directory.\n\nKUBERNETES:\n\
          \  --kubernetes [NAMESPACE]\n                        Execute workflow in\
          \ a kubernetes cluster (in the\n                        cloud). NAMESPACE\
          \ is the namespace you want to use for\n                        your job\
          \ (if nothing specified: 'default'). Usually,\n                        this\
          \ requires --default-remote-provider and --default-\n                  \
          \      remote-prefix to be set to a S3 or GS bucket where\n            \
          \            your . data shall be stored. It is further advisable\n    \
          \                    to activate conda integration via --use-conda.\n  --container-image\
          \ IMAGE\n                        Docker image to use, e.g., when submitting\
          \ jobs to\n                        kubernetes. By default, this is\n   \
          \                     'https://hub.docker.com/r/snakemake/snakemake', tagged\n\
          \                        with the same version as the currently running\n\
          \                        Snakemake instance. Note that overwriting this\
          \ value\n                        is up to your responsibility. Any used\
          \ image has to\n                        contain a working snakemake installation\
          \ that is\n                        compatible with (or ideally the same\
          \ as) the currently\n                        running version.\n\nTIBANNA:\n\
          \  --tibanna             Execute workflow on AWS cloud using Tibanna. This\n\
          \                        requires --default-remote-prefix to be set to S3\n\
          \                        bucket name and prefix (e.g.\n                \
          \        'bucketname/subdirectory') where input is already\n           \
          \             stored and output will be sent to. Using --tibanna\n     \
          \                   implies --default-resources is set as default.\n   \
          \                     Optionally, use --precommand to specify any\n    \
          \                    preparation command to run before snakemake command\
          \ on\n                        the cloud (inside snakemake container on Tibanna\
          \ VM).\n                        Also, --use-conda, --use-singularity, --config,\n\
          \                        --configfile are supported and will be carried\
          \ over.\n  --tibanna-sfn TIBANNA_SFN\n                        Name of Tibanna\
          \ Unicorn step function (e.g.\n                        tibanna_unicorn_monty).This\
          \ works as serverless\n                        scheduler/resource allocator\
          \ and must be deployed\n                        first using tibanna cli.\
          \ (e.g. tibanna deploy_unicorn\n                        --usergroup=monty\
          \ --buckets=bucketname)\n  --precommand PRECOMMAND\n                   \
          \     Any command to execute before snakemake command on AWS\n         \
          \               cloud such as wget, git clone, unzip, etc. This is\n   \
          \                     used with --tibanna.Do not include input/output\n\
          \                        download/upload commands - file transfer between\
          \ S3\n                        bucket and the run environment (container)\
          \ is\n                        automatically handled by Tibanna.\n  --tibanna-config\
          \ TIBANNA_CONFIG [TIBANNA_CONFIG ...]\n                        Additional\
          \ tibanan config e.g. --tibanna-config\n                        spot_instance=true\
          \ subnet=<subnet_id> security\n                        group=<security_group_id>\n\
          \nCONDA:\n  --use-conda           If defined in the rule, run job in a conda\n\
          \                        environment. If this flag is not set, the conda\n\
          \                        directive is ignored.\n  --list-conda-envs    \
          \ List all conda environments and their location on\n                  \
          \      disk.\n  --cleanup-conda       Cleanup unused conda environments.\n\
          \  --conda-prefix DIR    Specify a directory in which the 'conda' and 'conda-\n\
          \                        archive' directories are created. These are used\
          \ to\n                        store conda environments and their archives,\n\
          \                        respectively. If not supplied, the value is set\
          \ to the\n                        '.snakemake' directory relative to the\
          \ invocation\n                        directory. If supplied, the `--use-conda`\
          \ flag must\n                        also be set. The value may be given\
          \ as a relative\n                        path, which will be extrapolated\
          \ to the invocation\n                        directory, or as an absolute\
          \ path.\n  --create-envs-only    If specified, only creates the job-specific\
          \ conda\n                        environments then exits. The `--use-conda`\
          \ flag must\n                        also be set.\n\nSINGULARITY:\n  --use-singularity\
          \     If defined in the rule, run job within a singularity\n           \
          \             container. If this flag is not set, the singularity\n    \
          \                    directive is ignored.\n  --singularity-prefix DIR\n\
          \                        Specify a directory in which singularity images\
          \ will\n                        be stored.If not supplied, the value is\
          \ set to the\n                        '.snakemake' directory relative to\
          \ the invocation\n                        directory. If supplied, the `--use-singularity`\
          \ flag\n                        must also be set. The value may be given\
          \ as a relative\n                        path, which will be extrapolated\
          \ to the invocation\n                        directory, or as an absolute\
          \ path.\n  --singularity-args ARGS\n                        Pass additional\
          \ args to singularity.\n\nENVIRONMENT MODULES:\n  --use-envmodules     \
          \ If defined in the rule, run job within the given\n                   \
          \     environment modules, loaded in the given order. This\n           \
          \             can be combined with --use-conda and --use-\n            \
          \            singularity, which will then be only used as a\n          \
          \              fallback for rules which don't define environment\n     \
          \                   modules.\n"
        generated_using: *id004
        docker_image:
      subcommands: []
      usage: []
      help_flag: !Flag
        optional: true
        synonyms:
        - --help
        description: show Snakemake help (or snakemake -h)
        args: !EmptyFlagArg {}
      usage_flag:
      version_flag:
      help_text: "ERROR\ntarget: No such file or directory\nProphane Pipeline (powered\
        \ by Snakemake)\n\nUsage: /usr/local/bin/prophane CONFIG_FILE [Snakemake options]\n\
        \n Full list of parameters:\n   --help                 show Snakemake help\
        \ (or snakemake -h)\n   --list-dbs             print list of configured databases\n\
        \                          databases are looked up in 'db_base_dir' configured\
        \ in:\n                              /usr/local/opt/prophane/general_config.yaml\n\
        \   --list-styles          print list of available input file styles\n   \
        \                       styles are looked up in the following folder:\n  \
        \                            /usr/local/opt/prophane/styles\n   --db-maintenance\
        \       trigger database maintenance scripts\n                          will\
        \ migrate database structure from deprecated structure to most recent\n\n\
        \ Useful Snakemake parameters:\n   -j, --cores            number of cores\n\
        \   -k, --keep-going       go on with independent jobs if a job fails\n  \
        \ -n, --dryrun           do not execute anything\n   -p, --printshellcmds\
        \   print out the shell commands that will be executed\n   -t, --timestamp\
        \  \t\tadd a timestamp to all logging output\n\n"
      generated_using: *id004
      docker_image:
    - !Command
      command: *id005
      positional: []
      named:
      - !Flag
        optional: true
        synonyms:
        - --list-dbs
        description: "print list of configured databases\ndatabases are looked up\
          \ in 'db_base_dir' configured in:\n/usr/local/opt/prophane/general_config.yaml"
        args: !EmptyFlagArg {}
      - !Flag
        optional: true
        synonyms:
        - --list-styles
        description: "print list of available input file styles\nstyles are looked\
          \ up in the following folder:\n/usr/local/opt/prophane/styles"
        args: !EmptyFlagArg {}
      - !Flag
        optional: true
        synonyms:
        - --db-maintenance
        description: "trigger database maintenance scripts\nwill migrate database\
          \ structure from deprecated structure to most recent"
        args: !EmptyFlagArg {}
      - !Flag
        optional: true
        synonyms:
        - -j
        - --cores
        description: number of cores
        args: !EmptyFlagArg {}
      - !Flag
        optional: true
        synonyms:
        - -k
        - --keep-going
        description: go on with independent jobs if a job fails
        args: !EmptyFlagArg {}
      - !Flag
        optional: true
        synonyms:
        - -n
        - --dryrun
        description: do not execute anything
        args: !EmptyFlagArg {}
      - !Flag
        optional: true
        synonyms:
        - -p
        - --printshellcmds
        description: print out the shell commands that will be executed
        args: !EmptyFlagArg {}
      - !Flag
        optional: true
        synonyms:
        - -t
        - --timestamp
        description: add a timestamp to all logging output
        args: !EmptyFlagArg {}
      parent: *id008
      subcommands: []
      usage: []
      help_flag: !Flag
        optional: true
        synonyms:
        - --help
        description: show Snakemake help (or snakemake -h)
        args: !EmptyFlagArg {}
      usage_flag:
      version_flag:
      help_text: "ERROR\ndisk.: No such file or directory\nProphane Pipeline (powered\
        \ by Snakemake)\n\nUsage: /usr/local/bin/prophane CONFIG_FILE [Snakemake options]\n\
        \n Full list of parameters:\n   --help                 show Snakemake help\
        \ (or snakemake -h)\n   --list-dbs             print list of configured databases\n\
        \                          databases are looked up in 'db_base_dir' configured\
        \ in:\n                              /usr/local/opt/prophane/general_config.yaml\n\
        \   --list-styles          print list of available input file styles\n   \
        \                       styles are looked up in the following folder:\n  \
        \                            /usr/local/opt/prophane/styles\n   --db-maintenance\
        \       trigger database maintenance scripts\n                          will\
        \ migrate database structure from deprecated structure to most recent\n\n\
        \ Useful Snakemake parameters:\n   -j, --cores            number of cores\n\
        \   -k, --keep-going       go on with independent jobs if a job fails\n  \
        \ -n, --dryrun           do not execute anything\n   -p, --printshellcmds\
        \   print out the shell commands that will be executed\n   -t, --timestamp\
        \  \t\tadd a timestamp to all logging output\n\n"
      generated_using: *id004
      docker_image:
    usage: []
    help_flag:
    usage_flag:
    version_flag:
    help_text: "usage: snakemake [-h] [--dry-run] [--profile PROFILE]\n          \
      \       [--cache RULE [RULE ...]] [--snakefile FILE] [--cores [N]]\n       \
      \          [--local-cores N] [--resources [NAME=INT [NAME=INT ...]]]\n     \
      \            [--set-threads [RULE=THREADS [RULE=THREADS ...]]]\n           \
      \      [--default-resources [NAME=INT [NAME=INT ...]]]\n                 [--config\
      \ [KEY=VALUE [KEY=VALUE ...]]]\n                 [--configfile FILE [FILE ...]]\
      \ [--directory DIR] [--touch]\n                 [--keep-going] [--force] [--forceall]\n\
      \                 [--forcerun [TARGET [TARGET ...]]]\n                 [--prioritize\
      \ TARGET [TARGET ...]]\n                 [--batch RULE=BATCH/BATCHES] [--until\
      \ TARGET [TARGET ...]]\n                 [--omit-from TARGET [TARGET ...]] [--rerun-incomplete]\n\
      \                 [--shadow-prefix DIR] [--report [HTMLFILE]]\n            \
      \     [--lint [{text,json}]] [--export-cwl FILE] [--list]\n                \
      \ [--list-target-rules] [--dag] [--rulegraph] [--filegraph]\n              \
      \   [--d3dag] [--summary] [--detailed-summary] [--archive FILE]\n          \
      \       [--cleanup-metadata FILE [FILE ...]] [--cleanup-shadow]\n          \
      \       [--skip-script-cleanup] [--unlock] [--list-version-changes]\n      \
      \           [--list-code-changes] [--list-input-changes]\n                 [--list-params-changes]\
      \ [--list-untracked]\n                 [--delete-all-output] [--delete-temp-output]\n\
      \                 [--bash-completion] [--keep-incomplete] [--version]\n    \
      \             [--reason] [--gui [PORT]] [--printshellcmds] [--debug-dag]\n \
      \                [--stats FILE] [--nocolor] [--quiet] [--print-compilation]\n\
      \                 [--verbose] [--force-use-threads] [--allow-ambiguity]\n  \
      \               [--nolock] [--ignore-incomplete] [--latency-wait SECONDS]\n\
      \                 [--wait-for-files [FILE [FILE ...]]] [--notemp]\n        \
      \         [--keep-remote] [--keep-target-files]\n                 [--allowed-rules\
      \ ALLOWED_RULES [ALLOWED_RULES ...]]\n                 [--max-jobs-per-second\
      \ MAX_JOBS_PER_SECOND]\n                 [--max-status-checks-per-second MAX_STATUS_CHECKS_PER_SECOND]\n\
      \                 [--restart-times RESTART_TIMES] [--attempt ATTEMPT]\n    \
      \             [--wrapper-prefix WRAPPER_PREFIX]\n                 [--default-remote-provider\
      \ {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp,iRODS}]\n                 [--default-remote-prefix\
      \ DEFAULT_REMOTE_PREFIX]\n                 [--no-shared-fs] [--greediness GREEDINESS]\
      \ [--no-hooks]\n                 [--overwrite-shellcmd OVERWRITE_SHELLCMD] [--debug]\n\
      \                 [--runtime-profile FILE] [--mode {0,1,2}]\n              \
      \   [--show-failed-logs] [--log-handler-script FILE]\n                 [--log-service\
      \ {none,slack}]\n                 [--cluster CMD | --cluster-sync CMD | --drmaa\
      \ [ARGS]]\n                 [--cluster-config FILE] [--immediate-submit]\n \
      \                [--jobscript SCRIPT] [--jobname NAME]\n                 [--cluster-status\
      \ CLUSTER_STATUS] [--drmaa-log-dir DIR]\n                 [--kubernetes [NAMESPACE]]\
      \ [--container-image IMAGE]\n                 [--tibanna] [--tibanna-sfn TIBANNA_SFN]\n\
      \                 [--precommand PRECOMMAND]\n                 [--tibanna-config\
      \ TIBANNA_CONFIG [TIBANNA_CONFIG ...]]\n                 [--use-conda] [--list-conda-envs]\
      \ [--cleanup-conda]\n                 [--conda-prefix DIR] [--create-envs-only]\
      \ [--use-singularity]\n                 [--singularity-prefix DIR] [--singularity-args\
      \ ARGS]\n                 [--use-envmodules]\n                 [target [target\
      \ ...]]\n\nSnakemake is a Python based language and execution environment for\
      \ GNU Make-\nlike workflows.\n\noptional arguments:\n  -h, --help          \
      \  show this help message and exit\n\nEXECUTION:\n  target                Targets\
      \ to build. May be rules or files.\n  --dry-run, --dryrun, -n\n            \
      \            Do not execute anything, and display what would be\n          \
      \              done. If you have a very large workflow, use --dry-run\n    \
      \                    --quiet to just print a summary of the DAG of jobs.\n \
      \ --profile PROFILE     Name of profile to use for configuring Snakemake.\n\
      \                        Snakemake will search for a corresponding folder in\n\
      \                        /etc/xdg/snakemake and /root/.config/snakemake.\n \
      \                       Alternatively, this can be an absolute or relative\n\
      \                        path. The profile folder has to contain a file\n  \
      \                      'config.yaml'. This file can be used to set default\n\
      \                        values for command line options in YAML format. For\n\
      \                        example, '--cluster qsub' becomes 'cluster: qsub' in\n\
      \                        the YAML file. Profiles can be obtained from\n    \
      \                    https://github.com/snakemake-profiles.\n  --cache RULE\
      \ [RULE ...]\n                        Store output files of given rules in a\
      \ central cache\n                        given by the environment variable\n\
      \                        $SNAKEMAKE_OUTPUT_CACHE. Likewise, retrieve output\n\
      \                        files of the given rules from this cache if they have\n\
      \                        been created before (by anybody writing to the same\n\
      \                        cache), instead of actually executing the rules.\n\
      \                        Output files are identified by hashing all steps,\n\
      \                        parameters and software stack (conda envs or\n    \
      \                    containers) needed to create them.\n  --snakefile FILE,\
      \ -s FILE\n                        The workflow definition in form of a\n  \
      \                      snakefile.Usually, you should not need to specify\n \
      \                       this. By default, Snakemake will search for\n      \
      \                  'Snakefile', 'snakefile', 'workflow/Snakefile',\n       \
      \                 'workflow/snakefile' beneath the current working\n       \
      \                 directory, in this order. Only if you definitely want\n  \
      \                      a different layout, you need to use this parameter.\n\
      \  --cores [N], --jobs [N], -j [N]\n                        Use at most N CPU\
      \ cores/jobs in parallel. If N is\n                        omitted or 'all',\
      \ the limit is set to the number of\n                        available CPU cores.\n\
      \  --local-cores N       In cluster mode, use at most N cores of the host\n\
      \                        machine in parallel (default: number of CPU cores of\n\
      \                        the host). The cores are used to execute local rules.\n\
      \                        This option is ignored when not in cluster mode.\n\
      \  --resources [NAME=INT [NAME=INT ...]], --res [NAME=INT [NAME=INT ...]]\n\
      \                        Define additional resources that shall constrain the\n\
      \                        scheduling analogously to threads (see above). A\n\
      \                        resource is defined as a name and an integer value.\n\
      \                        E.g. --resources gpu=1. Rules can use resources by\n\
      \                        defining the resource keyword, e.g. resources: gpu=1.\n\
      \                        If now two rules require 1 of the resource 'gpu' they\n\
      \                        won't be run in parallel by the scheduler.\n  --set-threads\
      \ [RULE=THREADS [RULE=THREADS ...]]\n                        Overwrite thread\
      \ usage of rules. This allows to fine-\n                        tune workflow\
      \ parallelization. In particular, this is\n                        helpful to\
      \ target certain cluster nodes by e.g.\n                        shifting a rule\
      \ to use more, or less threads than\n                        defined in the\
      \ workflow. Thereby, THREADS has to be a\n                        positive integer,\
      \ and RULE has to be the name of the\n                        rule.\n  --default-resources\
      \ [NAME=INT [NAME=INT ...]], --default-res [NAME=INT [NAME=INT ...]]\n     \
      \                   Define default values of resources for rules that do\n \
      \                       not define their own values. In addition to plain\n\
      \                        integers, python expressions over inputsize are\n \
      \                       allowed (e.g. '2*input.size_mb').When specifying this\n\
      \                        without any arguments (--default-resources), it\n \
      \                       defines 'mem_mb=max(2*input.size_mb, 1000)'\n      \
      \                  'disk_mb=max(2*input.size_mb, 1000)', i.e., default\n   \
      \                     disk and mem usage is twice the input file size but at\n\
      \                        least 1GB.\n  --config [KEY=VALUE [KEY=VALUE ...]],\
      \ -C [KEY=VALUE [KEY=VALUE ...]]\n                        Set or overwrite values\
      \ in the workflow config object.\n                        The workflow config\
      \ object is accessible as variable\n                        config inside the\
      \ workflow. Default values can be set\n                        by providing\
      \ a JSON file (see Documentation).\n  --configfile FILE [FILE ...], --configfiles\
      \ FILE [FILE ...]\n                        Specify or overwrite the config file\
      \ of the workflow\n                        (see the docs). Values specified\
      \ in JSON or YAML\n                        format are available in the global\
      \ config dictionary\n                        inside the workflow. Multiple files\
      \ overwrite each\n                        other in the given order.\n  --directory\
      \ DIR, -d DIR\n                        Specify working directory (relative paths\
      \ in the\n                        snakefile will use this as their origin).\n\
      \  --touch, -t           Touch output files (mark them up to date without\n\
      \                        really changing them) instead of running their\n  \
      \                      commands. This is used to pretend that the rules were\n\
      \                        executed, in order to fool future invocations of\n\
      \                        snakemake. Fails if a file does not yet exist. Note\n\
      \                        that this will only touch files that would otherwise\n\
      \                        be recreated by Snakemake (e.g. because their input\n\
      \                        files are newer). For enforcing a touch, combine this\n\
      \                        with --force, --forceall, or --forcerun. Note however\n\
      \                        that you loose the provenance information when the\n\
      \                        files have been created in realitiy. Hence, this\n\
      \                        should be used only as a last resort.\n  --keep-going,\
      \ -k      Go on with independent jobs if a job fails.\n  --force, -f       \
      \    Force the execution of the selected target or the\n                   \
      \     first rule regardless of already created output.\n  --forceall, -F   \
      \     Force the execution of the selected (or the first)\n                 \
      \       rule and all rules it is dependent on regardless of\n              \
      \          already created output.\n  --forcerun [TARGET [TARGET ...]], -R [TARGET\
      \ [TARGET ...]]\n                        Force the re-execution or creation\
      \ of the given rules\n                        or files. Use this option if you\
      \ changed a rule and\n                        want to have all its output in\
      \ your workflow updated.\n  --prioritize TARGET [TARGET ...], -P TARGET [TARGET\
      \ ...]\n                        Tell the scheduler to assign creation of given\
      \ targets\n                        (and all their dependencies) highest priority.\n\
      \                        (EXPERIMENTAL)\n  --batch RULE=BATCH/BATCHES\n    \
      \                    Only create the given BATCH of the input files of the\n\
      \                        given RULE. This can be used to iteratively run parts\n\
      \                        of very large workflows. Only the execution plan of\n\
      \                        the relevant part of the workflow has to be\n     \
      \                   calculated, thereby speeding up DAG computation. It is\n\
      \                        recommended to provide the most suitable rule for\n\
      \                        batching when documenting a workflow. It should be\n\
      \                        some aggregating rule that would be executed only\n\
      \                        once, and has a large number of input files. For\n\
      \                        example, it can be a rule that aggregates over\n  \
      \                      samples.\n  --until TARGET [TARGET ...], -U TARGET [TARGET\
      \ ...]\n                        Runs the pipeline until it reaches the specified\
      \ rules\n                        or files. Only runs jobs that are dependencies\
      \ of the\n                        specified rule or files, does not run sibling\
      \ DAGs.\n  --omit-from TARGET [TARGET ...], -O TARGET [TARGET ...]\n       \
      \                 Prevent the execution or creation of the given rules\n   \
      \                     or files as well as any rules or files that are\n    \
      \                    downstream of these targets in the DAG. Also runs jobs\n\
      \                        in sibling DAGs that are independent of the rules or\n\
      \                        files specified here.\n  --rerun-incomplete, --ri\n\
      \                        Re-run all jobs the output of which is recognized as\n\
      \                        incomplete.\n  --shadow-prefix DIR   Specify a directory\
      \ in which the 'shadow' directory is\n                        created. If not\
      \ supplied, the value is set to the\n                        '.snakemake' directory\
      \ relative to the working\n                        directory.\n\nUTILITIES:\n\
      \  --report [HTMLFILE]   Create an HTML report with results and statistics.\
      \ If\n                        no filename is given, report.html is the default.\n\
      \  --lint [{text,json}]  Perform linting on the given workflow. This will print\n\
      \                        snakemake specific suggestions to improve code quality\n\
      \                        (work in progress, more lints to be added in the\n\
      \                        future). If no argument is provided, plain text output\n\
      \                        is used.\n  --export-cwl FILE     Compile workflow\
      \ to CWL and store it in given FILE.\n  --list, -l            Show available\
      \ rules in given Snakefile.\n  --list-target-rules, --lt\n                 \
      \       Show available target rules in given Snakefile.\n  --dag           \
      \      Do not execute anything and print the directed acyclic\n            \
      \            graph of jobs in the dot language. Recommended use on\n       \
      \                 Unix systems: snakemake --dag | dot | display\n  --rulegraph\
      \           Do not execute anything and print the dependency graph\n       \
      \                 of rules in the dot language. This will be less\n        \
      \                crowded than above DAG of jobs, but also show less\n      \
      \                  information. Note that each rule is displayed once,\n   \
      \                     hence the displayed graph will be cyclic if a rule\n \
      \                       appears in several steps of the workflow. Use this if\n\
      \                        above option leads to a DAG that is too large.\n  \
      \                      Recommended use on Unix systems: snakemake --rulegraph\n\
      \                        | dot | display\n  --filegraph           Do not execute\
      \ anything and print the dependency graph\n                        of rules\
      \ with their input and output files in the dot\n                        language.\
      \ This is an intermediate solution between\n                        above DAG\
      \ of jobs and the rule graph. Note that each\n                        rule is\
      \ displayed once, hence the displayed graph will\n                        be\
      \ cyclic if a rule appears in several steps of the\n                       \
      \ workflow. Use this if above option leads to a DAG that\n                 \
      \       is too large. Recommended use on Unix systems:\n                   \
      \     snakemake --filegraph | dot | display\n  --d3dag               Print the\
      \ DAG in D3.js compatible JSON format.\n  --summary, -S         Print a summary\
      \ of all files created by the workflow.\n                        The has the\
      \ following columns: filename, modification\n                        time, rule\
      \ version, status, plan. Thereby rule version\n                        contains\
      \ the versionthe file was created with (see the\n                        version\
      \ keyword of rules), and status denotes whether\n                        the\
      \ file is missing, its input files are newer or if\n                       \
      \ version or implementation of the rule changed since\n                    \
      \    file creation. Finally the last column denotes whether\n              \
      \          the file will be updated or created during the next\n           \
      \             workflow execution.\n  --detailed-summary, -D\n              \
      \          Print a summary of all files created by the workflow.\n         \
      \               The has the following columns: filename, modification\n    \
      \                    time, rule version, input file(s), shell command,\n   \
      \                     status, plan. Thereby rule version contains the\n    \
      \                    version the file was created with (see the version\n  \
      \                      keyword of rules), and status denotes whether the file\n\
      \                        is missing, its input files are newer or if version\
      \ or\n                        implementation of the rule changed since file\n\
      \                        creation. The input file and shell command columns\
      \ are\n                        self explanatory. Finally the last column denotes\n\
      \                        whether the file will be updated or created during\
      \ the\n                        next workflow execution.\n  --archive FILE  \
      \      Archive the workflow into the given tar archive FILE.\n             \
      \           The archive will be created such that the workflow can\n       \
      \                 be re-executed on a vanilla system. The function needs\n \
      \                       conda and git to be installed. It will archive every\n\
      \                        file that is under git version control. Note that it\n\
      \                        is best practice to have the Snakefile, config files,\n\
      \                        and scripts under version control. Hence, they will\
      \ be\n                        included in the archive. Further, it will add\
      \ input\n                        files that are not generated by by the workflow\
      \ itself\n                        and conda environments. Note that symlinks\
      \ are\n                        dereferenced. Supported formats are .tar, .tar.gz,\n\
      \                        .tar.bz2 and .tar.xz.\n  --cleanup-metadata FILE [FILE\
      \ ...], --cm FILE [FILE ...]\n                        Cleanup the metadata of\
      \ given files. That means that\n                        snakemake removes any\
      \ tracked version info, and any\n                        marks that files are\
      \ incomplete.\n  --cleanup-shadow      Cleanup old shadow directories which\
      \ have not been\n                        deleted due to failures or power loss.\n\
      \  --skip-script-cleanup\n                        Don't delete wrapper scripts\
      \ used for execution\n  --unlock              Remove a lock on the working directory.\n\
      \  --list-version-changes, --lv\n                        List all output files\
      \ that have been created with a\n                        different version (as\
      \ determined by the version\n                        keyword).\n  --list-code-changes,\
      \ --lc\n                        List all output files for which the rule body\
      \ (run or\n                        shell) have changed in the Snakefile.\n \
      \ --list-input-changes, --li\n                        List all output files\
      \ for which the defined input\n                        files have changed in\
      \ the Snakefile (e.g. new input\n                        files were added in\
      \ the rule definition or files were\n                        renamed). For listing\
      \ input file modification in the\n                        filesystem, use --summary.\n\
      \  --list-params-changes, --lp\n                        List all output files\
      \ for which the defined params\n                        have changed in the\
      \ Snakefile.\n  --list-untracked, --lu\n                        List all files\
      \ in the working directory that are not\n                        used in the\
      \ workflow. This can be used e.g. for\n                        identifying leftover\
      \ files. Hidden files and\n                        directories are ignored.\n\
      \  --delete-all-output   Remove all files generated by the workflow. Use\n \
      \                       together with --dry-run to list files without actually\n\
      \                        deleting anything. Note that this will not recurse\n\
      \                        into subworkflows. Write-protected files are not\n\
      \                        removed. Nevertheless, use with care!\n  --delete-temp-output\
      \  Remove all temporary files generated by the workflow.\n                 \
      \       Use together with --dry-run to list files without\n                \
      \        actually deleting anything. Note that this will not\n             \
      \           recurse into subworkflows.\n  --bash-completion     Output code\
      \ to register bash completion for snakemake.\n                        Put the\
      \ following in your .bashrc (including the\n                        accents):\
      \ `snakemake --bash-completion` or issue it in\n                        an open\
      \ terminal session.\n  --keep-incomplete     Do not remove incomplete output\
      \ files by failed jobs.\n  --version, -v         show program's version number\
      \ and exit\n\nOUTPUT:\n  --reason, -r          Print the reason for each executed\
      \ rule.\n  --gui [PORT]          Serve an HTML based user interface to the given\n\
      \                        network and port e.g. 168.129.10.15:8000. By default\n\
      \                        Snakemake is only available in the local network\n\
      \                        (default port: 8000). To make Snakemake listen to all\n\
      \                        ip addresses add the special host address 0.0.0.0 to\n\
      \                        the url (0.0.0.0:8000). This is important if Snakemake\n\
      \                        is used in a virtualised environment like Docker. If\n\
      \                        possible, a browser window is opened.\n  --printshellcmds,\
      \ -p  Print out the shell commands that will be executed.\n  --debug-dag   \
      \        Print candidate and selected jobs (including their\n              \
      \          wildcards) while inferring DAG. This can help to debug\n        \
      \                unexpected DAG topology or errors.\n  --stats FILE        \
      \  Write stats about Snakefile execution in JSON format\n                  \
      \      to the given file.\n  --nocolor             Do not use a colored output.\n\
      \  --quiet, -q           Do not output any progress or rule information.\n \
      \ --print-compilation   Print the python representation of the workflow.\n \
      \ --verbose             Print debugging output.\n\nBEHAVIOR:\n  --force-use-threads\
      \   Force threads rather than processes. Helpful if shared\n               \
      \         memory (/dev/shm) is full or unavailable.\n  --allow-ambiguity, -a\n\
      \                        Don't check for ambiguous rules and simply use the\n\
      \                        first if several can produce the same file. This\n\
      \                        allows the user to prioritize rules by their order\
      \ in\n                        the snakefile.\n  --nolock              Do not\
      \ lock the working directory\n  --ignore-incomplete, --ii\n                \
      \        Do not check for incomplete output files.\n  --latency-wait SECONDS,\
      \ --output-wait SECONDS, -w SECONDS\n                        Wait given seconds\
      \ if an output file of a job is not\n                        present after the\
      \ job finished. This helps if your\n                        filesystem suffers\
      \ from latency (default 5).\n  --wait-for-files [FILE [FILE ...]]\n        \
      \                Wait --latency-wait seconds for these files to be\n       \
      \                 present before executing the workflow. This option is\n  \
      \                      used internally to handle filesystem latency in\n   \
      \                     cluster environments.\n  --notemp, --nt        Ignore\
      \ temp() declarations. This is useful when\n                        running\
      \ only a part of the workflow, since temp()\n                        would lead\
      \ to deletion of probably needed files by\n                        other parts\
      \ of the workflow.\n  --keep-remote         Keep local copies of remote input\
      \ files.\n  --keep-target-files   Do not adjust the paths of given target files\
      \ relative\n                        to the working directory.\n  --allowed-rules\
      \ ALLOWED_RULES [ALLOWED_RULES ...]\n                        Only consider given\
      \ rules. If omitted, all rules in\n                        Snakefile are used.\
      \ Note that this is intended\n                        primarily for internal\
      \ use and may lead to unexpected\n                        results otherwise.\n\
      \  --max-jobs-per-second MAX_JOBS_PER_SECOND\n                        Maximal\
      \ number of cluster/drmaa jobs per second,\n                        default\
      \ is 10, fractions allowed.\n  --max-status-checks-per-second MAX_STATUS_CHECKS_PER_SECOND\n\
      \                        Maximal number of job status checks per second,\n \
      \                       default is 10, fractions allowed.\n  --restart-times\
      \ RESTART_TIMES\n                        Number of times to restart failing\
      \ jobs (defaults to\n                        0).\n  --attempt ATTEMPT     Internal\
      \ use only: define the initial value of the\n                        attempt\
      \ parameter (default: 1).\n  --wrapper-prefix WRAPPER_PREFIX\n             \
      \           Prefix for URL created from wrapper directive\n                \
      \        (default: https://github.com/snakemake/snakemake-\n               \
      \         wrappers/raw/). Set this to a different URL to use\n             \
      \           your fork or a local clone of the repository, e.g.,\n          \
      \              use a git URL like\n                        'git+file://path/to/your/local/clone@'.\n\
      \  --default-remote-provider {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp,iRODS}\n\
      \                        Specify default remote provider to be used for all\n\
      \                        input and output files that don't yet specify one.\n\
      \  --default-remote-prefix DEFAULT_REMOTE_PREFIX\n                        Specify\
      \ prefix for default remote provider. E.g. a\n                        bucket\
      \ name.\n  --no-shared-fs        Do not assume that jobs share a common file\
      \ system.\n                        When this flag is activated, Snakemake will\
      \ assume\n                        that the filesystem on a cluster node is not\
      \ shared\n                        with other nodes. For example, this will lead\
      \ to\n                        downloading remote files on each cluster node\n\
      \                        separately. Further, it won't take special measures\
      \ to\n                        deal with filesystem latency issues. This option\
      \ will\n                        in most cases only make sense in combination\
      \ with\n                        --default-remote-provider. Further, when using\n\
      \                        --cluster you will have to also provide --cluster-\n\
      \                        status. Only activate this if you know what you are\n\
      \                        doing.\n  --greediness GREEDINESS\n               \
      \         Set the greediness of scheduling. This value between 0\n         \
      \               and 1 determines how careful jobs are selected for\n       \
      \                 execution. The default value (1.0) provides the best\n   \
      \                     speed and still acceptable scheduling quality.\n  --no-hooks\
      \            Do not invoke onstart, onsuccess or onerror hooks\n           \
      \             after execution.\n  --overwrite-shellcmd OVERWRITE_SHELLCMD\n\
      \                        Provide a shell command that shall be executed instead\n\
      \                        of those given in the workflow. This is for debugging\n\
      \                        purposes only.\n  --debug               Allow to debug\
      \ rules with e.g. PDB. This flag allows\n                        to set breakpoints\
      \ in run blocks.\n  --runtime-profile FILE\n                        Profile\
      \ Snakemake and write the output to FILE. This\n                        requires\
      \ yappi to be installed.\n  --mode {0,1,2}        Set execution mode of Snakemake\
      \ (internal use only).\n  --show-failed-logs    Automatically display logs of\
      \ failed jobs.\n  --log-handler-script FILE\n                        Provide\
      \ a custom script containing a function 'def\n                        log_handler(msg):'.\
      \ Snakemake will call this function\n                        for every logging\
      \ output (given as a dictionary\n                        msg)allowing to e.g.\
      \ send notifications in the form of\n                        e.g. slack messages\
      \ or emails.\n  --log-service {none,slack}\n                        Set a specific\
      \ messaging service for logging\n                        output.Snakemake will\
      \ notify the service on errors and\n                        completed execution.Currently\
      \ only slack is supported.\n\nCLUSTER:\n  --cluster CMD, -c CMD\n          \
      \              Execute snakemake rules with the given submit command,\n    \
      \                    e.g. qsub. Snakemake compiles jobs into scripts that\n\
      \                        are submitted to the cluster with the given command,\n\
      \                        once all input files for a particular job are present.\n\
      \                        The submit command can be decorated to make it aware\n\
      \                        of certain job properties (name, rulename, input,\n\
      \                        output, params, wildcards, log, threads and\n     \
      \                   dependencies (see the argument below)), e.g.: $\n      \
      \                  snakemake --cluster 'qsub -pe threaded {threads}'.\n  --cluster-sync\
      \ CMD    cluster submission command will block, returning the\n            \
      \            remote exitstatus upon remote termination (for\n              \
      \          example, this should be usedif the cluster command is\n         \
      \               'qsub -sync y' (SGE)\n  --drmaa [ARGS]        Execute snakemake\
      \ on a cluster accessed via DRMAA,\n                        Snakemake compiles\
      \ jobs into scripts that are\n                        submitted to the cluster\
      \ with the given command, once\n                        all input files for\
      \ a particular job are present. ARGS\n                        can be used to\
      \ specify options of the underlying\n                        cluster system,\
      \ thereby using the job properties name,\n                        rulename,\
      \ input, output, params, wildcards, log,\n                        threads and\
      \ dependencies, e.g.: --drmaa ' -pe threaded\n                        {threads}'.\
      \ Note that ARGS must be given in quotes and\n                        with a\
      \ leading whitespace.\n  --cluster-config FILE, -u FILE\n                  \
      \      A JSON or YAML file that defines the wildcards used in\n            \
      \            'cluster'for specific rules, instead of having them\n         \
      \               specified in the Snakefile. For example, for rule\n        \
      \                'job' you may define: { 'job' : { 'time' : '24:00:00'\n   \
      \                     } } to specify the time for rule 'job'. You can\n    \
      \                    specify more than one file. The configuration files\n \
      \                       are merged with later values overriding earlier ones.\n\
      \                        This option is deprecated in favor of using --profile,\n\
      \                        see docs.\n  --immediate-submit, --is\n           \
      \             Immediately submit all jobs to the cluster instead of\n      \
      \                  waiting for present input files. This will fail,\n      \
      \                  unless you make the cluster aware of job dependencies,\n\
      \                        e.g. via: $ snakemake --cluster 'sbatch --dependency\n\
      \                        {dependencies}. Assuming that your submit script (here\n\
      \                        sbatch) outputs the generated job id to the first\n\
      \                        stdout line, {dependencies} will be filled with space\n\
      \                        separated job ids this job depends on.\n  --jobscript\
      \ SCRIPT, --js SCRIPT\n                        Provide a custom job script for\
      \ submission to the\n                        cluster. The default script resides\
      \ as 'jobscript.sh'\n                        in the installation directory.\n\
      \  --jobname NAME, --jn NAME\n                        Provide a custom name\
      \ for the jobscript that is\n                        submitted to the cluster\
      \ (see --cluster). NAME is\n                        \"snakejob.{name}.{jobid}.sh\"\
      \ per default. The wildcard\n                        {jobid} has to be present\
      \ in the name.\n  --cluster-status CLUSTER_STATUS\n                        Status\
      \ command for cluster execution. This is only\n                        considered\
      \ in combination with the --cluster flag. If\n                        provided,\
      \ Snakemake will use the status command to\n                        determine\
      \ if a job has finished successfully or\n                        failed. For\
      \ this it is necessary that the submit\n                        command provided\
      \ to --cluster returns the cluster job\n                        id. Then, the\
      \ status command will be invoked with the\n                        job id. Snakemake\
      \ expects it to return 'success' if\n                        the job was successfull,\
      \ 'failed' if the job failed\n                        and 'running' if the job\
      \ still runs.\n  --drmaa-log-dir DIR   Specify a directory in which stdout and\
      \ stderr files\n                        of DRMAA jobs will be written. The value\
      \ may be given\n                        as a relative path, in which case Snakemake\
      \ will use\n                        the current invocation directory as the\
      \ origin. If\n                        given, this will override any given '-o'\
      \ and/or '-e'\n                        native specification. If not given, all\
      \ DRMAA stdout\n                        and stderr files are written to the\
      \ current working\n                        directory.\n\nKUBERNETES:\n  --kubernetes\
      \ [NAMESPACE]\n                        Execute workflow in a kubernetes cluster\
      \ (in the\n                        cloud). NAMESPACE is the namespace you want\
      \ to use for\n                        your job (if nothing specified: 'default').\
      \ Usually,\n                        this requires --default-remote-provider\
      \ and --default-\n                        remote-prefix to be set to a S3 or\
      \ GS bucket where\n                        your . data shall be stored. It is\
      \ further advisable\n                        to activate conda integration via\
      \ --use-conda.\n  --container-image IMAGE\n                        Docker image\
      \ to use, e.g., when submitting jobs to\n                        kubernetes.\
      \ By default, this is\n                        'https://hub.docker.com/r/snakemake/snakemake',\
      \ tagged\n                        with the same version as the currently running\n\
      \                        Snakemake instance. Note that overwriting this value\n\
      \                        is up to your responsibility. Any used image has to\n\
      \                        contain a working snakemake installation that is\n\
      \                        compatible with (or ideally the same as) the currently\n\
      \                        running version.\n\nTIBANNA:\n  --tibanna         \
      \    Execute workflow on AWS cloud using Tibanna. This\n                   \
      \     requires --default-remote-prefix to be set to S3\n                   \
      \     bucket name and prefix (e.g.\n                        'bucketname/subdirectory')\
      \ where input is already\n                        stored and output will be\
      \ sent to. Using --tibanna\n                        implies --default-resources\
      \ is set as default.\n                        Optionally, use --precommand to\
      \ specify any\n                        preparation command to run before snakemake\
      \ command on\n                        the cloud (inside snakemake container\
      \ on Tibanna VM).\n                        Also, --use-conda, --use-singularity,\
      \ --config,\n                        --configfile are supported and will be\
      \ carried over.\n  --tibanna-sfn TIBANNA_SFN\n                        Name of\
      \ Tibanna Unicorn step function (e.g.\n                        tibanna_unicorn_monty).This\
      \ works as serverless\n                        scheduler/resource allocator\
      \ and must be deployed\n                        first using tibanna cli. (e.g.\
      \ tibanna deploy_unicorn\n                        --usergroup=monty --buckets=bucketname)\n\
      \  --precommand PRECOMMAND\n                        Any command to execute before\
      \ snakemake command on AWS\n                        cloud such as wget, git\
      \ clone, unzip, etc. This is\n                        used with --tibanna.Do\
      \ not include input/output\n                        download/upload commands\
      \ - file transfer between S3\n                        bucket and the run environment\
      \ (container) is\n                        automatically handled by Tibanna.\n\
      \  --tibanna-config TIBANNA_CONFIG [TIBANNA_CONFIG ...]\n                  \
      \      Additional tibanan config e.g. --tibanna-config\n                   \
      \     spot_instance=true subnet=<subnet_id> security\n                     \
      \   group=<security_group_id>\n\nCONDA:\n  --use-conda           If defined\
      \ in the rule, run job in a conda\n                        environment. If this\
      \ flag is not set, the conda\n                        directive is ignored.\n\
      \  --list-conda-envs     List all conda environments and their location on\n\
      \                        disk.\n  --cleanup-conda       Cleanup unused conda\
      \ environments.\n  --conda-prefix DIR    Specify a directory in which the 'conda'\
      \ and 'conda-\n                        archive' directories are created. These\
      \ are used to\n                        store conda environments and their archives,\n\
      \                        respectively. If not supplied, the value is set to\
      \ the\n                        '.snakemake' directory relative to the invocation\n\
      \                        directory. If supplied, the `--use-conda` flag must\n\
      \                        also be set. The value may be given as a relative\n\
      \                        path, which will be extrapolated to the invocation\n\
      \                        directory, or as an absolute path.\n  --create-envs-only\
      \    If specified, only creates the job-specific conda\n                   \
      \     environments then exits. The `--use-conda` flag must\n               \
      \         also be set.\n\nSINGULARITY:\n  --use-singularity     If defined in\
      \ the rule, run job within a singularity\n                        container.\
      \ If this flag is not set, the singularity\n                        directive\
      \ is ignored.\n  --singularity-prefix DIR\n                        Specify a\
      \ directory in which singularity images will\n                        be stored.If\
      \ not supplied, the value is set to the\n                        '.snakemake'\
      \ directory relative to the invocation\n                        directory. If\
      \ supplied, the `--use-singularity` flag\n                        must also\
      \ be set. The value may be given as a relative\n                        path,\
      \ which will be extrapolated to the invocation\n                        directory,\
      \ or as an absolute path.\n  --singularity-args ARGS\n                     \
      \   Pass additional args to singularity.\n\nENVIRONMENT MODULES:\n  --use-envmodules\
      \      If defined in the rule, run job within the given\n                  \
      \      environment modules, loaded in the given order. This\n              \
      \          can be combined with --use-conda and --use-\n                   \
      \     singularity, which will then be only used as a\n                     \
      \   fallback for rules which don't define environment\n                    \
      \    modules.\n"
    generated_using: *id004
    docker_image:
  subcommands: []
  usage: []
  help_flag: !Flag
    optional: true
    synonyms:
    - --help
    description: show Snakemake help (or snakemake -h)
    args: !EmptyFlagArg {}
  usage_flag:
  version_flag:
  help_text: "ERROR\ntarget: No such file or directory\nProphane Pipeline (powered\
    \ by Snakemake)\n\nUsage: /usr/local/bin/prophane CONFIG_FILE [Snakemake options]\n\
    \n Full list of parameters:\n   --help                 show Snakemake help (or\
    \ snakemake -h)\n   --list-dbs             print list of configured databases\n\
    \                          databases are looked up in 'db_base_dir' configured\
    \ in:\n                              /usr/local/opt/prophane/general_config.yaml\n\
    \   --list-styles          print list of available input file styles\n       \
    \                   styles are looked up in the following folder:\n          \
    \                    /usr/local/opt/prophane/styles\n   --db-maintenance     \
    \  trigger database maintenance scripts\n                          will migrate\
    \ database structure from deprecated structure to most recent\n\n Useful Snakemake\
    \ parameters:\n   -j, --cores            number of cores\n   -k, --keep-going\
    \       go on with independent jobs if a job fails\n   -n, --dryrun          \
    \ do not execute anything\n   -p, --printshellcmds   print out the shell commands\
    \ that will be executed\n   -t, --timestamp  \t\tadd a timestamp to all logging\
    \ output\n\n"
  generated_using: *id004
  docker_image:
- !Command
  command: *id005
  positional: []
  named:
  - !Flag
    optional: true
    synonyms:
    - --list-dbs
    description: "print list of configured databases\ndatabases are looked up in 'db_base_dir'\
      \ configured in:\n/usr/local/opt/prophane/general_config.yaml"
    args: !EmptyFlagArg {}
  - !Flag
    optional: true
    synonyms:
    - --list-styles
    description: "print list of available input file styles\nstyles are looked up\
      \ in the following folder:\n/usr/local/opt/prophane/styles"
    args: !EmptyFlagArg {}
  - !Flag
    optional: true
    synonyms:
    - --db-maintenance
    description: "trigger database maintenance scripts\nwill migrate database structure\
      \ from deprecated structure to most recent"
    args: !EmptyFlagArg {}
  - !Flag
    optional: true
    synonyms:
    - -j
    - --cores
    description: number of cores
    args: !EmptyFlagArg {}
  - !Flag
    optional: true
    synonyms:
    - -k
    - --keep-going
    description: go on with independent jobs if a job fails
    args: !EmptyFlagArg {}
  - !Flag
    optional: true
    synonyms:
    - -n
    - --dryrun
    description: do not execute anything
    args: !EmptyFlagArg {}
  - !Flag
    optional: true
    synonyms:
    - -p
    - --printshellcmds
    description: print out the shell commands that will be executed
    args: !EmptyFlagArg {}
  - !Flag
    optional: true
    synonyms:
    - -t
    - --timestamp
    description: add a timestamp to all logging output
    args: !EmptyFlagArg {}
  parent: *id009
  subcommands: []
  usage: []
  help_flag: !Flag
    optional: true
    synonyms:
    - --help
    description: show Snakemake help (or snakemake -h)
    args: !EmptyFlagArg {}
  usage_flag:
  version_flag:
  help_text: "ERROR\ndisk.: No such file or directory\nProphane Pipeline (powered\
    \ by Snakemake)\n\nUsage: /usr/local/bin/prophane CONFIG_FILE [Snakemake options]\n\
    \n Full list of parameters:\n   --help                 show Snakemake help (or\
    \ snakemake -h)\n   --list-dbs             print list of configured databases\n\
    \                          databases are looked up in 'db_base_dir' configured\
    \ in:\n                              /usr/local/opt/prophane/general_config.yaml\n\
    \   --list-styles          print list of available input file styles\n       \
    \                   styles are looked up in the following folder:\n          \
    \                    /usr/local/opt/prophane/styles\n   --db-maintenance     \
    \  trigger database maintenance scripts\n                          will migrate\
    \ database structure from deprecated structure to most recent\n\n Useful Snakemake\
    \ parameters:\n   -j, --cores            number of cores\n   -k, --keep-going\
    \       go on with independent jobs if a job fails\n   -n, --dryrun          \
    \ do not execute anything\n   -p, --printshellcmds   print out the shell commands\
    \ that will be executed\n   -t, --timestamp  \t\tadd a timestamp to all logging\
    \ output\n\n"
  generated_using: *id004
  docker_image:
usage: []
help_flag:
usage_flag:
version_flag:
help_text: "usage: snakemake [-h] [--dry-run] [--profile PROFILE]\n              \
  \   [--cache RULE [RULE ...]] [--snakefile FILE] [--cores [N]]\n               \
  \  [--local-cores N] [--resources [NAME=INT [NAME=INT ...]]]\n                 [--set-threads\
  \ [RULE=THREADS [RULE=THREADS ...]]]\n                 [--default-resources [NAME=INT\
  \ [NAME=INT ...]]]\n                 [--config [KEY=VALUE [KEY=VALUE ...]]]\n  \
  \               [--configfile FILE [FILE ...]] [--directory DIR] [--touch]\n   \
  \              [--keep-going] [--force] [--forceall]\n                 [--forcerun\
  \ [TARGET [TARGET ...]]]\n                 [--prioritize TARGET [TARGET ...]]\n\
  \                 [--batch RULE=BATCH/BATCHES] [--until TARGET [TARGET ...]]\n \
  \                [--omit-from TARGET [TARGET ...]] [--rerun-incomplete]\n      \
  \           [--shadow-prefix DIR] [--report [HTMLFILE]]\n                 [--lint\
  \ [{text,json}]] [--export-cwl FILE] [--list]\n                 [--list-target-rules]\
  \ [--dag] [--rulegraph] [--filegraph]\n                 [--d3dag] [--summary] [--detailed-summary]\
  \ [--archive FILE]\n                 [--cleanup-metadata FILE [FILE ...]] [--cleanup-shadow]\n\
  \                 [--skip-script-cleanup] [--unlock] [--list-version-changes]\n\
  \                 [--list-code-changes] [--list-input-changes]\n               \
  \  [--list-params-changes] [--list-untracked]\n                 [--delete-all-output]\
  \ [--delete-temp-output]\n                 [--bash-completion] [--keep-incomplete]\
  \ [--version]\n                 [--reason] [--gui [PORT]] [--printshellcmds] [--debug-dag]\n\
  \                 [--stats FILE] [--nocolor] [--quiet] [--print-compilation]\n \
  \                [--verbose] [--force-use-threads] [--allow-ambiguity]\n       \
  \          [--nolock] [--ignore-incomplete] [--latency-wait SECONDS]\n         \
  \        [--wait-for-files [FILE [FILE ...]]] [--notemp]\n                 [--keep-remote]\
  \ [--keep-target-files]\n                 [--allowed-rules ALLOWED_RULES [ALLOWED_RULES\
  \ ...]]\n                 [--max-jobs-per-second MAX_JOBS_PER_SECOND]\n        \
  \         [--max-status-checks-per-second MAX_STATUS_CHECKS_PER_SECOND]\n      \
  \           [--restart-times RESTART_TIMES] [--attempt ATTEMPT]\n              \
  \   [--wrapper-prefix WRAPPER_PREFIX]\n                 [--default-remote-provider\
  \ {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp,iRODS}]\n                 [--default-remote-prefix\
  \ DEFAULT_REMOTE_PREFIX]\n                 [--no-shared-fs] [--greediness GREEDINESS]\
  \ [--no-hooks]\n                 [--overwrite-shellcmd OVERWRITE_SHELLCMD] [--debug]\n\
  \                 [--runtime-profile FILE] [--mode {0,1,2}]\n                 [--show-failed-logs]\
  \ [--log-handler-script FILE]\n                 [--log-service {none,slack}]\n \
  \                [--cluster CMD | --cluster-sync CMD | --drmaa [ARGS]]\n       \
  \          [--cluster-config FILE] [--immediate-submit]\n                 [--jobscript\
  \ SCRIPT] [--jobname NAME]\n                 [--cluster-status CLUSTER_STATUS] [--drmaa-log-dir\
  \ DIR]\n                 [--kubernetes [NAMESPACE]] [--container-image IMAGE]\n\
  \                 [--tibanna] [--tibanna-sfn TIBANNA_SFN]\n                 [--precommand\
  \ PRECOMMAND]\n                 [--tibanna-config TIBANNA_CONFIG [TIBANNA_CONFIG\
  \ ...]]\n                 [--use-conda] [--list-conda-envs] [--cleanup-conda]\n\
  \                 [--conda-prefix DIR] [--create-envs-only] [--use-singularity]\n\
  \                 [--singularity-prefix DIR] [--singularity-args ARGS]\n       \
  \          [--use-envmodules]\n                 [target [target ...]]\n\nSnakemake\
  \ is a Python based language and execution environment for GNU Make-\nlike workflows.\n\
  \noptional arguments:\n  -h, --help            show this help message and exit\n\
  \nEXECUTION:\n  target                Targets to build. May be rules or files.\n\
  \  --dry-run, --dryrun, -n\n                        Do not execute anything, and\
  \ display what would be\n                        done. If you have a very large\
  \ workflow, use --dry-run\n                        --quiet to just print a summary\
  \ of the DAG of jobs.\n  --profile PROFILE     Name of profile to use for configuring\
  \ Snakemake.\n                        Snakemake will search for a corresponding\
  \ folder in\n                        /etc/xdg/snakemake and /root/.config/snakemake.\n\
  \                        Alternatively, this can be an absolute or relative\n  \
  \                      path. The profile folder has to contain a file\n        \
  \                'config.yaml'. This file can be used to set default\n         \
  \               values for command line options in YAML format. For\n          \
  \              example, '--cluster qsub' becomes 'cluster: qsub' in\n          \
  \              the YAML file. Profiles can be obtained from\n                  \
  \      https://github.com/snakemake-profiles.\n  --cache RULE [RULE ...]\n     \
  \                   Store output files of given rules in a central cache\n     \
  \                   given by the environment variable\n                        $SNAKEMAKE_OUTPUT_CACHE.\
  \ Likewise, retrieve output\n                        files of the given rules from\
  \ this cache if they have\n                        been created before (by anybody\
  \ writing to the same\n                        cache), instead of actually executing\
  \ the rules.\n                        Output files are identified by hashing all\
  \ steps,\n                        parameters and software stack (conda envs or\n\
  \                        containers) needed to create them.\n  --snakefile FILE,\
  \ -s FILE\n                        The workflow definition in form of a\n      \
  \                  snakefile.Usually, you should not need to specify\n         \
  \               this. By default, Snakemake will search for\n                  \
  \      'Snakefile', 'snakefile', 'workflow/Snakefile',\n                       \
  \ 'workflow/snakefile' beneath the current working\n                        directory,\
  \ in this order. Only if you definitely want\n                        a different\
  \ layout, you need to use this parameter.\n  --cores [N], --jobs [N], -j [N]\n \
  \                       Use at most N CPU cores/jobs in parallel. If N is\n    \
  \                    omitted or 'all', the limit is set to the number of\n     \
  \                   available CPU cores.\n  --local-cores N       In cluster mode,\
  \ use at most N cores of the host\n                        machine in parallel (default:\
  \ number of CPU cores of\n                        the host). The cores are used\
  \ to execute local rules.\n                        This option is ignored when not\
  \ in cluster mode.\n  --resources [NAME=INT [NAME=INT ...]], --res [NAME=INT [NAME=INT\
  \ ...]]\n                        Define additional resources that shall constrain\
  \ the\n                        scheduling analogously to threads (see above). A\n\
  \                        resource is defined as a name and an integer value.\n \
  \                       E.g. --resources gpu=1. Rules can use resources by\n   \
  \                     defining the resource keyword, e.g. resources: gpu=1.\n  \
  \                      If now two rules require 1 of the resource 'gpu' they\n \
  \                       won't be run in parallel by the scheduler.\n  --set-threads\
  \ [RULE=THREADS [RULE=THREADS ...]]\n                        Overwrite thread usage\
  \ of rules. This allows to fine-\n                        tune workflow parallelization.\
  \ In particular, this is\n                        helpful to target certain cluster\
  \ nodes by e.g.\n                        shifting a rule to use more, or less threads\
  \ than\n                        defined in the workflow. Thereby, THREADS has to\
  \ be a\n                        positive integer, and RULE has to be the name of\
  \ the\n                        rule.\n  --default-resources [NAME=INT [NAME=INT\
  \ ...]], --default-res [NAME=INT [NAME=INT ...]]\n                        Define\
  \ default values of resources for rules that do\n                        not define\
  \ their own values. In addition to plain\n                        integers, python\
  \ expressions over inputsize are\n                        allowed (e.g. '2*input.size_mb').When\
  \ specifying this\n                        without any arguments (--default-resources),\
  \ it\n                        defines 'mem_mb=max(2*input.size_mb, 1000)'\n    \
  \                    'disk_mb=max(2*input.size_mb, 1000)', i.e., default\n     \
  \                   disk and mem usage is twice the input file size but at\n   \
  \                     least 1GB.\n  --config [KEY=VALUE [KEY=VALUE ...]], -C [KEY=VALUE\
  \ [KEY=VALUE ...]]\n                        Set or overwrite values in the workflow\
  \ config object.\n                        The workflow config object is accessible\
  \ as variable\n                        config inside the workflow. Default values\
  \ can be set\n                        by providing a JSON file (see Documentation).\n\
  \  --configfile FILE [FILE ...], --configfiles FILE [FILE ...]\n               \
  \         Specify or overwrite the config file of the workflow\n               \
  \         (see the docs). Values specified in JSON or YAML\n                   \
  \     format are available in the global config dictionary\n                   \
  \     inside the workflow. Multiple files overwrite each\n                     \
  \   other in the given order.\n  --directory DIR, -d DIR\n                     \
  \   Specify working directory (relative paths in the\n                        snakefile\
  \ will use this as their origin).\n  --touch, -t           Touch output files (mark\
  \ them up to date without\n                        really changing them) instead\
  \ of running their\n                        commands. This is used to pretend that\
  \ the rules were\n                        executed, in order to fool future invocations\
  \ of\n                        snakemake. Fails if a file does not yet exist. Note\n\
  \                        that this will only touch files that would otherwise\n\
  \                        be recreated by Snakemake (e.g. because their input\n \
  \                       files are newer). For enforcing a touch, combine this\n\
  \                        with --force, --forceall, or --forcerun. Note however\n\
  \                        that you loose the provenance information when the\n  \
  \                      files have been created in realitiy. Hence, this\n      \
  \                  should be used only as a last resort.\n  --keep-going, -k   \
  \   Go on with independent jobs if a job fails.\n  --force, -f           Force the\
  \ execution of the selected target or the\n                        first rule regardless\
  \ of already created output.\n  --forceall, -F        Force the execution of the\
  \ selected (or the first)\n                        rule and all rules it is dependent\
  \ on regardless of\n                        already created output.\n  --forcerun\
  \ [TARGET [TARGET ...]], -R [TARGET [TARGET ...]]\n                        Force\
  \ the re-execution or creation of the given rules\n                        or files.\
  \ Use this option if you changed a rule and\n                        want to have\
  \ all its output in your workflow updated.\n  --prioritize TARGET [TARGET ...],\
  \ -P TARGET [TARGET ...]\n                        Tell the scheduler to assign creation\
  \ of given targets\n                        (and all their dependencies) highest\
  \ priority.\n                        (EXPERIMENTAL)\n  --batch RULE=BATCH/BATCHES\n\
  \                        Only create the given BATCH of the input files of the\n\
  \                        given RULE. This can be used to iteratively run parts\n\
  \                        of very large workflows. Only the execution plan of\n \
  \                       the relevant part of the workflow has to be\n          \
  \              calculated, thereby speeding up DAG computation. It is\n        \
  \                recommended to provide the most suitable rule for\n           \
  \             batching when documenting a workflow. It should be\n             \
  \           some aggregating rule that would be executed only\n                \
  \        once, and has a large number of input files. For\n                    \
  \    example, it can be a rule that aggregates over\n                        samples.\n\
  \  --until TARGET [TARGET ...], -U TARGET [TARGET ...]\n                       \
  \ Runs the pipeline until it reaches the specified rules\n                     \
  \   or files. Only runs jobs that are dependencies of the\n                    \
  \    specified rule or files, does not run sibling DAGs.\n  --omit-from TARGET [TARGET\
  \ ...], -O TARGET [TARGET ...]\n                        Prevent the execution or\
  \ creation of the given rules\n                        or files as well as any rules\
  \ or files that are\n                        downstream of these targets in the\
  \ DAG. Also runs jobs\n                        in sibling DAGs that are independent\
  \ of the rules or\n                        files specified here.\n  --rerun-incomplete,\
  \ --ri\n                        Re-run all jobs the output of which is recognized\
  \ as\n                        incomplete.\n  --shadow-prefix DIR   Specify a directory\
  \ in which the 'shadow' directory is\n                        created. If not supplied,\
  \ the value is set to the\n                        '.snakemake' directory relative\
  \ to the working\n                        directory.\n\nUTILITIES:\n  --report [HTMLFILE]\
  \   Create an HTML report with results and statistics. If\n                    \
  \    no filename is given, report.html is the default.\n  --lint [{text,json}] \
  \ Perform linting on the given workflow. This will print\n                     \
  \   snakemake specific suggestions to improve code quality\n                   \
  \     (work in progress, more lints to be added in the\n                       \
  \ future). If no argument is provided, plain text output\n                     \
  \   is used.\n  --export-cwl FILE     Compile workflow to CWL and store it in given\
  \ FILE.\n  --list, -l            Show available rules in given Snakefile.\n  --list-target-rules,\
  \ --lt\n                        Show available target rules in given Snakefile.\n\
  \  --dag                 Do not execute anything and print the directed acyclic\n\
  \                        graph of jobs in the dot language. Recommended use on\n\
  \                        Unix systems: snakemake --dag | dot | display\n  --rulegraph\
  \           Do not execute anything and print the dependency graph\n           \
  \             of rules in the dot language. This will be less\n                \
  \        crowded than above DAG of jobs, but also show less\n                  \
  \      information. Note that each rule is displayed once,\n                   \
  \     hence the displayed graph will be cyclic if a rule\n                     \
  \   appears in several steps of the workflow. Use this if\n                    \
  \    above option leads to a DAG that is too large.\n                        Recommended\
  \ use on Unix systems: snakemake --rulegraph\n                        | dot | display\n\
  \  --filegraph           Do not execute anything and print the dependency graph\n\
  \                        of rules with their input and output files in the dot\n\
  \                        language. This is an intermediate solution between\n  \
  \                      above DAG of jobs and the rule graph. Note that each\n  \
  \                      rule is displayed once, hence the displayed graph will\n\
  \                        be cyclic if a rule appears in several steps of the\n \
  \                       workflow. Use this if above option leads to a DAG that\n\
  \                        is too large. Recommended use on Unix systems:\n      \
  \                  snakemake --filegraph | dot | display\n  --d3dag            \
  \   Print the DAG in D3.js compatible JSON format.\n  --summary, -S         Print\
  \ a summary of all files created by the workflow.\n                        The has\
  \ the following columns: filename, modification\n                        time, rule\
  \ version, status, plan. Thereby rule version\n                        contains\
  \ the versionthe file was created with (see the\n                        version\
  \ keyword of rules), and status denotes whether\n                        the file\
  \ is missing, its input files are newer or if\n                        version or\
  \ implementation of the rule changed since\n                        file creation.\
  \ Finally the last column denotes whether\n                        the file will\
  \ be updated or created during the next\n                        workflow execution.\n\
  \  --detailed-summary, -D\n                        Print a summary of all files\
  \ created by the workflow.\n                        The has the following columns:\
  \ filename, modification\n                        time, rule version, input file(s),\
  \ shell command,\n                        status, plan. Thereby rule version contains\
  \ the\n                        version the file was created with (see the version\n\
  \                        keyword of rules), and status denotes whether the file\n\
  \                        is missing, its input files are newer or if version or\n\
  \                        implementation of the rule changed since file\n       \
  \                 creation. The input file and shell command columns are\n     \
  \                   self explanatory. Finally the last column denotes\n        \
  \                whether the file will be updated or created during the\n      \
  \                  next workflow execution.\n  --archive FILE        Archive the\
  \ workflow into the given tar archive FILE.\n                        The archive\
  \ will be created such that the workflow can\n                        be re-executed\
  \ on a vanilla system. The function needs\n                        conda and git\
  \ to be installed. It will archive every\n                        file that is under\
  \ git version control. Note that it\n                        is best practice to\
  \ have the Snakefile, config files,\n                        and scripts under version\
  \ control. Hence, they will be\n                        included in the archive.\
  \ Further, it will add input\n                        files that are not generated\
  \ by by the workflow itself\n                        and conda environments. Note\
  \ that symlinks are\n                        dereferenced. Supported formats are\
  \ .tar, .tar.gz,\n                        .tar.bz2 and .tar.xz.\n  --cleanup-metadata\
  \ FILE [FILE ...], --cm FILE [FILE ...]\n                        Cleanup the metadata\
  \ of given files. That means that\n                        snakemake removes any\
  \ tracked version info, and any\n                        marks that files are incomplete.\n\
  \  --cleanup-shadow      Cleanup old shadow directories which have not been\n  \
  \                      deleted due to failures or power loss.\n  --skip-script-cleanup\n\
  \                        Don't delete wrapper scripts used for execution\n  --unlock\
  \              Remove a lock on the working directory.\n  --list-version-changes,\
  \ --lv\n                        List all output files that have been created with\
  \ a\n                        different version (as determined by the version\n \
  \                       keyword).\n  --list-code-changes, --lc\n               \
  \         List all output files for which the rule body (run or\n              \
  \          shell) have changed in the Snakefile.\n  --list-input-changes, --li\n\
  \                        List all output files for which the defined input\n   \
  \                     files have changed in the Snakefile (e.g. new input\n    \
  \                    files were added in the rule definition or files were\n   \
  \                     renamed). For listing input file modification in the\n   \
  \                     filesystem, use --summary.\n  --list-params-changes, --lp\n\
  \                        List all output files for which the defined params\n  \
  \                      have changed in the Snakefile.\n  --list-untracked, --lu\n\
  \                        List all files in the working directory that are not\n\
  \                        used in the workflow. This can be used e.g. for\n     \
  \                   identifying leftover files. Hidden files and\n             \
  \           directories are ignored.\n  --delete-all-output   Remove all files generated\
  \ by the workflow. Use\n                        together with --dry-run to list\
  \ files without actually\n                        deleting anything. Note that this\
  \ will not recurse\n                        into subworkflows. Write-protected files\
  \ are not\n                        removed. Nevertheless, use with care!\n  --delete-temp-output\
  \  Remove all temporary files generated by the workflow.\n                     \
  \   Use together with --dry-run to list files without\n                        actually\
  \ deleting anything. Note that this will not\n                        recurse into\
  \ subworkflows.\n  --bash-completion     Output code to register bash completion\
  \ for snakemake.\n                        Put the following in your .bashrc (including\
  \ the\n                        accents): `snakemake --bash-completion` or issue\
  \ it in\n                        an open terminal session.\n  --keep-incomplete\
  \     Do not remove incomplete output files by failed jobs.\n  --version, -v   \
  \      show program's version number and exit\n\nOUTPUT:\n  --reason, -r       \
  \   Print the reason for each executed rule.\n  --gui [PORT]          Serve an HTML\
  \ based user interface to the given\n                        network and port e.g.\
  \ 168.129.10.15:8000. By default\n                        Snakemake is only available\
  \ in the local network\n                        (default port: 8000). To make Snakemake\
  \ listen to all\n                        ip addresses add the special host address\
  \ 0.0.0.0 to\n                        the url (0.0.0.0:8000). This is important\
  \ if Snakemake\n                        is used in a virtualised environment like\
  \ Docker. If\n                        possible, a browser window is opened.\n  --printshellcmds,\
  \ -p  Print out the shell commands that will be executed.\n  --debug-dag       \
  \    Print candidate and selected jobs (including their\n                      \
  \  wildcards) while inferring DAG. This can help to debug\n                    \
  \    unexpected DAG topology or errors.\n  --stats FILE          Write stats about\
  \ Snakefile execution in JSON format\n                        to the given file.\n\
  \  --nocolor             Do not use a colored output.\n  --quiet, -q           Do\
  \ not output any progress or rule information.\n  --print-compilation   Print the\
  \ python representation of the workflow.\n  --verbose             Print debugging\
  \ output.\n\nBEHAVIOR:\n  --force-use-threads   Force threads rather than processes.\
  \ Helpful if shared\n                        memory (/dev/shm) is full or unavailable.\n\
  \  --allow-ambiguity, -a\n                        Don't check for ambiguous rules\
  \ and simply use the\n                        first if several can produce the same\
  \ file. This\n                        allows the user to prioritize rules by their\
  \ order in\n                        the snakefile.\n  --nolock              Do not\
  \ lock the working directory\n  --ignore-incomplete, --ii\n                    \
  \    Do not check for incomplete output files.\n  --latency-wait SECONDS, --output-wait\
  \ SECONDS, -w SECONDS\n                        Wait given seconds if an output file\
  \ of a job is not\n                        present after the job finished. This\
  \ helps if your\n                        filesystem suffers from latency (default\
  \ 5).\n  --wait-for-files [FILE [FILE ...]]\n                        Wait --latency-wait\
  \ seconds for these files to be\n                        present before executing\
  \ the workflow. This option is\n                        used internally to handle\
  \ filesystem latency in\n                        cluster environments.\n  --notemp,\
  \ --nt        Ignore temp() declarations. This is useful when\n                \
  \        running only a part of the workflow, since temp()\n                   \
  \     would lead to deletion of probably needed files by\n                     \
  \   other parts of the workflow.\n  --keep-remote         Keep local copies of remote\
  \ input files.\n  --keep-target-files   Do not adjust the paths of given target\
  \ files relative\n                        to the working directory.\n  --allowed-rules\
  \ ALLOWED_RULES [ALLOWED_RULES ...]\n                        Only consider given\
  \ rules. If omitted, all rules in\n                        Snakefile are used. Note\
  \ that this is intended\n                        primarily for internal use and\
  \ may lead to unexpected\n                        results otherwise.\n  --max-jobs-per-second\
  \ MAX_JOBS_PER_SECOND\n                        Maximal number of cluster/drmaa jobs\
  \ per second,\n                        default is 10, fractions allowed.\n  --max-status-checks-per-second\
  \ MAX_STATUS_CHECKS_PER_SECOND\n                        Maximal number of job status\
  \ checks per second,\n                        default is 10, fractions allowed.\n\
  \  --restart-times RESTART_TIMES\n                        Number of times to restart\
  \ failing jobs (defaults to\n                        0).\n  --attempt ATTEMPT  \
  \   Internal use only: define the initial value of the\n                       \
  \ attempt parameter (default: 1).\n  --wrapper-prefix WRAPPER_PREFIX\n         \
  \               Prefix for URL created from wrapper directive\n                \
  \        (default: https://github.com/snakemake/snakemake-\n                   \
  \     wrappers/raw/). Set this to a different URL to use\n                     \
  \   your fork or a local clone of the repository, e.g.,\n                      \
  \  use a git URL like\n                        'git+file://path/to/your/local/clone@'.\n\
  \  --default-remote-provider {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp,iRODS}\n    \
  \                    Specify default remote provider to be used for all\n      \
  \                  input and output files that don't yet specify one.\n  --default-remote-prefix\
  \ DEFAULT_REMOTE_PREFIX\n                        Specify prefix for default remote\
  \ provider. E.g. a\n                        bucket name.\n  --no-shared-fs     \
  \   Do not assume that jobs share a common file system.\n                      \
  \  When this flag is activated, Snakemake will assume\n                        that\
  \ the filesystem on a cluster node is not shared\n                        with other\
  \ nodes. For example, this will lead to\n                        downloading remote\
  \ files on each cluster node\n                        separately. Further, it won't\
  \ take special measures to\n                        deal with filesystem latency\
  \ issues. This option will\n                        in most cases only make sense\
  \ in combination with\n                        --default-remote-provider. Further,\
  \ when using\n                        --cluster you will have to also provide --cluster-\n\
  \                        status. Only activate this if you know what you are\n \
  \                       doing.\n  --greediness GREEDINESS\n                    \
  \    Set the greediness of scheduling. This value between 0\n                  \
  \      and 1 determines how careful jobs are selected for\n                    \
  \    execution. The default value (1.0) provides the best\n                    \
  \    speed and still acceptable scheduling quality.\n  --no-hooks            Do\
  \ not invoke onstart, onsuccess or onerror hooks\n                        after\
  \ execution.\n  --overwrite-shellcmd OVERWRITE_SHELLCMD\n                      \
  \  Provide a shell command that shall be executed instead\n                    \
  \    of those given in the workflow. This is for debugging\n                   \
  \     purposes only.\n  --debug               Allow to debug rules with e.g. PDB.\
  \ This flag allows\n                        to set breakpoints in run blocks.\n\
  \  --runtime-profile FILE\n                        Profile Snakemake and write the\
  \ output to FILE. This\n                        requires yappi to be installed.\n\
  \  --mode {0,1,2}        Set execution mode of Snakemake (internal use only).\n\
  \  --show-failed-logs    Automatically display logs of failed jobs.\n  --log-handler-script\
  \ FILE\n                        Provide a custom script containing a function 'def\n\
  \                        log_handler(msg):'. Snakemake will call this function\n\
  \                        for every logging output (given as a dictionary\n     \
  \                   msg)allowing to e.g. send notifications in the form of\n   \
  \                     e.g. slack messages or emails.\n  --log-service {none,slack}\n\
  \                        Set a specific messaging service for logging\n        \
  \                output.Snakemake will notify the service on errors and\n      \
  \                  completed execution.Currently only slack is supported.\n\nCLUSTER:\n\
  \  --cluster CMD, -c CMD\n                        Execute snakemake rules with the\
  \ given submit command,\n                        e.g. qsub. Snakemake compiles jobs\
  \ into scripts that\n                        are submitted to the cluster with the\
  \ given command,\n                        once all input files for a particular\
  \ job are present.\n                        The submit command can be decorated\
  \ to make it aware\n                        of certain job properties (name, rulename,\
  \ input,\n                        output, params, wildcards, log, threads and\n\
  \                        dependencies (see the argument below)), e.g.: $\n     \
  \                   snakemake --cluster 'qsub -pe threaded {threads}'.\n  --cluster-sync\
  \ CMD    cluster submission command will block, returning the\n                \
  \        remote exitstatus upon remote termination (for\n                      \
  \  example, this should be usedif the cluster command is\n                     \
  \   'qsub -sync y' (SGE)\n  --drmaa [ARGS]        Execute snakemake on a cluster\
  \ accessed via DRMAA,\n                        Snakemake compiles jobs into scripts\
  \ that are\n                        submitted to the cluster with the given command,\
  \ once\n                        all input files for a particular job are present.\
  \ ARGS\n                        can be used to specify options of the underlying\n\
  \                        cluster system, thereby using the job properties name,\n\
  \                        rulename, input, output, params, wildcards, log,\n    \
  \                    threads and dependencies, e.g.: --drmaa ' -pe threaded\n  \
  \                      {threads}'. Note that ARGS must be given in quotes and\n\
  \                        with a leading whitespace.\n  --cluster-config FILE, -u\
  \ FILE\n                        A JSON or YAML file that defines the wildcards used\
  \ in\n                        'cluster'for specific rules, instead of having them\n\
  \                        specified in the Snakefile. For example, for rule\n   \
  \                     'job' you may define: { 'job' : { 'time' : '24:00:00'\n  \
  \                      } } to specify the time for rule 'job'. You can\n       \
  \                 specify more than one file. The configuration files\n        \
  \                are merged with later values overriding earlier ones.\n       \
  \                 This option is deprecated in favor of using --profile,\n     \
  \                   see docs.\n  --immediate-submit, --is\n                    \
  \    Immediately submit all jobs to the cluster instead of\n                   \
  \     waiting for present input files. This will fail,\n                       \
  \ unless you make the cluster aware of job dependencies,\n                     \
  \   e.g. via: $ snakemake --cluster 'sbatch --dependency\n                     \
  \   {dependencies}. Assuming that your submit script (here\n                   \
  \     sbatch) outputs the generated job id to the first\n                      \
  \  stdout line, {dependencies} will be filled with space\n                     \
  \   separated job ids this job depends on.\n  --jobscript SCRIPT, --js SCRIPT\n\
  \                        Provide a custom job script for submission to the\n   \
  \                     cluster. The default script resides as 'jobscript.sh'\n  \
  \                      in the installation directory.\n  --jobname NAME, --jn NAME\n\
  \                        Provide a custom name for the jobscript that is\n     \
  \                   submitted to the cluster (see --cluster). NAME is\n        \
  \                \"snakejob.{name}.{jobid}.sh\" per default. The wildcard\n    \
  \                    {jobid} has to be present in the name.\n  --cluster-status\
  \ CLUSTER_STATUS\n                        Status command for cluster execution.\
  \ This is only\n                        considered in combination with the --cluster\
  \ flag. If\n                        provided, Snakemake will use the status command\
  \ to\n                        determine if a job has finished successfully or\n\
  \                        failed. For this it is necessary that the submit\n    \
  \                    command provided to --cluster returns the cluster job\n   \
  \                     id. Then, the status command will be invoked with the\n  \
  \                      job id. Snakemake expects it to return 'success' if\n   \
  \                     the job was successfull, 'failed' if the job failed\n    \
  \                    and 'running' if the job still runs.\n  --drmaa-log-dir DIR\
  \   Specify a directory in which stdout and stderr files\n                     \
  \   of DRMAA jobs will be written. The value may be given\n                    \
  \    as a relative path, in which case Snakemake will use\n                    \
  \    the current invocation directory as the origin. If\n                      \
  \  given, this will override any given '-o' and/or '-e'\n                      \
  \  native specification. If not given, all DRMAA stdout\n                      \
  \  and stderr files are written to the current working\n                       \
  \ directory.\n\nKUBERNETES:\n  --kubernetes [NAMESPACE]\n                      \
  \  Execute workflow in a kubernetes cluster (in the\n                        cloud).\
  \ NAMESPACE is the namespace you want to use for\n                        your job\
  \ (if nothing specified: 'default'). Usually,\n                        this requires\
  \ --default-remote-provider and --default-\n                        remote-prefix\
  \ to be set to a S3 or GS bucket where\n                        your . data shall\
  \ be stored. It is further advisable\n                        to activate conda\
  \ integration via --use-conda.\n  --container-image IMAGE\n                    \
  \    Docker image to use, e.g., when submitting jobs to\n                      \
  \  kubernetes. By default, this is\n                        'https://hub.docker.com/r/snakemake/snakemake',\
  \ tagged\n                        with the same version as the currently running\n\
  \                        Snakemake instance. Note that overwriting this value\n\
  \                        is up to your responsibility. Any used image has to\n \
  \                       contain a working snakemake installation that is\n     \
  \                   compatible with (or ideally the same as) the currently\n   \
  \                     running version.\n\nTIBANNA:\n  --tibanna             Execute\
  \ workflow on AWS cloud using Tibanna. This\n                        requires --default-remote-prefix\
  \ to be set to S3\n                        bucket name and prefix (e.g.\n      \
  \                  'bucketname/subdirectory') where input is already\n         \
  \               stored and output will be sent to. Using --tibanna\n           \
  \             implies --default-resources is set as default.\n                 \
  \       Optionally, use --precommand to specify any\n                        preparation\
  \ command to run before snakemake command on\n                        the cloud\
  \ (inside snakemake container on Tibanna VM).\n                        Also, --use-conda,\
  \ --use-singularity, --config,\n                        --configfile are supported\
  \ and will be carried over.\n  --tibanna-sfn TIBANNA_SFN\n                     \
  \   Name of Tibanna Unicorn step function (e.g.\n                        tibanna_unicorn_monty).This\
  \ works as serverless\n                        scheduler/resource allocator and\
  \ must be deployed\n                        first using tibanna cli. (e.g. tibanna\
  \ deploy_unicorn\n                        --usergroup=monty --buckets=bucketname)\n\
  \  --precommand PRECOMMAND\n                        Any command to execute before\
  \ snakemake command on AWS\n                        cloud such as wget, git clone,\
  \ unzip, etc. This is\n                        used with --tibanna.Do not include\
  \ input/output\n                        download/upload commands - file transfer\
  \ between S3\n                        bucket and the run environment (container)\
  \ is\n                        automatically handled by Tibanna.\n  --tibanna-config\
  \ TIBANNA_CONFIG [TIBANNA_CONFIG ...]\n                        Additional tibanan\
  \ config e.g. --tibanna-config\n                        spot_instance=true subnet=<subnet_id>\
  \ security\n                        group=<security_group_id>\n\nCONDA:\n  --use-conda\
  \           If defined in the rule, run job in a conda\n                       \
  \ environment. If this flag is not set, the conda\n                        directive\
  \ is ignored.\n  --list-conda-envs     List all conda environments and their location\
  \ on\n                        disk.\n  --cleanup-conda       Cleanup unused conda\
  \ environments.\n  --conda-prefix DIR    Specify a directory in which the 'conda'\
  \ and 'conda-\n                        archive' directories are created. These are\
  \ used to\n                        store conda environments and their archives,\n\
  \                        respectively. If not supplied, the value is set to the\n\
  \                        '.snakemake' directory relative to the invocation\n   \
  \                     directory. If supplied, the `--use-conda` flag must\n    \
  \                    also be set. The value may be given as a relative\n       \
  \                 path, which will be extrapolated to the invocation\n         \
  \               directory, or as an absolute path.\n  --create-envs-only    If specified,\
  \ only creates the job-specific conda\n                        environments then\
  \ exits. The `--use-conda` flag must\n                        also be set.\n\nSINGULARITY:\n\
  \  --use-singularity     If defined in the rule, run job within a singularity\n\
  \                        container. If this flag is not set, the singularity\n \
  \                       directive is ignored.\n  --singularity-prefix DIR\n    \
  \                    Specify a directory in which singularity images will\n    \
  \                    be stored.If not supplied, the value is set to the\n      \
  \                  '.snakemake' directory relative to the invocation\n         \
  \               directory. If supplied, the `--use-singularity` flag\n         \
  \               must also be set. The value may be given as a relative\n       \
  \                 path, which will be extrapolated to the invocation\n         \
  \               directory, or as an absolute path.\n  --singularity-args ARGS\n\
  \                        Pass additional args to singularity.\n\nENVIRONMENT MODULES:\n\
  \  --use-envmodules      If defined in the rule, run job within the given\n    \
  \                    environment modules, loaded in the given order. This\n    \
  \                    can be combined with --use-conda and --use-\n             \
  \           singularity, which will then be only used as a\n                   \
  \     fallback for rules which don't define environment\n                      \
  \  modules.\n"
generated_using: *id004
docker_image:
