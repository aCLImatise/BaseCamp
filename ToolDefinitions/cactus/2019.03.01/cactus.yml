!Command
command:
- cactus
positional:
- !Positional
  description: Seq file
  position: 0
  name: seqFile
  optional: false
- !Positional
  description: Output HAL file
  position: 1
  name: outputHal
  optional: false
- !Positional
  description: "The location of the job store for the workflow. A job\nstore holds\
    \ persistent information about the jobs and\nfiles in a workflow. If the workflow\
    \ is run with a\ndistributed batch system, the job store must be\naccessible by\
    \ all worker nodes. Depending on the\ndesired job store implementation, the location\
    \ should\nbe formatted according to one of the following\nschemes: file:<path>\
    \ where <path> points to a\ndirectory on the file systen aws:<region>:<prefix>\n\
    where <region> is the name of an AWS region like us-\nwest-2 and <prefix> will\
    \ be prepended to the names of\nany top-level AWS resources in use by job store,\
    \ e.g.\nS3 buckets. azure:<account>:<prefix>\ngoogle:<project_id>:<prefix> TODO:\
    \ explain For\nbackwards compatibility, you may also specify ./foo\n(equivalent\
    \ to file:./foo or just file:foo) or /bar\n(equivalent to file:/bar)."
  position: 0
  name: jobStore
  optional: false
- !Positional
  description: "--provisioner {aws}   The provisioner for cluster auto-scaling. The\n\
    currently supported choices are'cgcloud' or 'aws'. The\ndefault is None."
  position: 0
  name: provisioning.
  optional: false
named:
- !Flag
  description: "The file containing a link to the experiment\nparameters"
  synonyms:
  - --experiment
  args: !SimpleFlagArg
    name: EXPERIMENTFILE
  optional: true
- !Flag
  description: Build trees
  synonyms:
  - --buildAvgs
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: Creates a reference ordering for the flowers
  synonyms:
  - --buildReference
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: Build a hal file
  synonyms:
  - --buildHal
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Build a fasta file of the input sequences (and\nreference sequence,\
    \ used with hal output)"
  synonyms:
  - --buildFasta
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "URL prefix to save intermediate results like DB dumps\nto (e.g. prefix-dump-caf,\
    \ prefix-dump-avg, etc.)"
  synonyms:
  - --intermediateResultsUrl
  args: !SimpleFlagArg
    name: INTERMEDIATERESULTSURL
  optional: true
- !Flag
  description: "Database type: tokyo_cabinet or kyoto_tycoon [default:\nkyoto_tycoon]"
  synonyms:
  - --database
  args: !SimpleFlagArg
    name: DATABASE
  optional: true
- !Flag
  description: Specify cactus configuration file
  synonyms:
  - --configFile
  args: !SimpleFlagArg
    name: CONFIGFILE
  optional: true
- !Flag
  description: "Name of ancestral node (which must appear in NEWICK\ntree in <seqfile>)\
    \ to use as a root for the alignment.\nAny genomes not below this node in the\
    \ tree may be\nused as outgroups but will never appear in the output.\nIf no root\
    \ is specifed then the root of the tree is\nused."
  synonyms:
  - --root
  args: !SimpleFlagArg
    name: ROOT
  optional: true
- !Flag
  description: "Use the latest version of the docker container rather\nthan pulling\
    \ one matching this version of cactus"
  synonyms:
  - --latest
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Use the the specified pre-built containter image\nrather than pulling\
    \ one from quay.io"
  synonyms:
  - --containerImage
  args: !SimpleFlagArg
    name: CONTAINERIMAGE
  optional: true
- !Flag
  description: The way to run the Cactus binaries
  synonyms:
  - --binariesMode
  args: !ChoiceFlagArg
    choices: !!set
      docker:
      singularity:
      local:
  optional: true
- !Flag
  description: Same as --logCritical
  synonyms:
  - --logOff
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Turn on logging at level CRITICAL and above. (default\nis INFO)"
  synonyms:
  - --logCritical
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Turn on logging at level ERROR and above. (default is\nINFO)"
  synonyms:
  - --logError
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Turn on logging at level WARNING and above. (default\nis INFO)"
  synonyms:
  - --logWarning
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Turn on logging at level INFO and above. (default is\nINFO)"
  synonyms:
  - --logInfo
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Turn on logging at level DEBUG and above. (default is\nINFO)"
  synonyms:
  - --logDebug
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Log at given level (may be either OFF (or CRITICAL),\nERROR, WARN\
    \ (or WARNING), INFO or DEBUG). (default is\nINFO)"
  synonyms:
  - --logLevel
  args: !SimpleFlagArg
    name: LOGLEVEL
  optional: true
- !Flag
  description: File to log in
  synonyms:
  - --logFile
  args: !SimpleFlagArg
    name: LOGFILE
  optional: true
- !Flag
  description: "Turn on rotating logging, which prevents log files\ngetting too big."
  synonyms:
  - --rotatingLogging
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Absolute path to directory where temporary files\ngenerated during\
    \ the Toil run should be placed. Temp\nfiles and folders will be placed in a directory\n\
    toil-<workflowID> within workDir (The workflowID is\ngenerated by Toil and will\
    \ be reported in the workflow\nlogs. Default is determined by the variables (TMPDIR,\n\
    TEMP, TMP) via mkdtemp. This directory needs to exist\non all machines running\
    \ jobs."
  synonyms:
  - --workDir
  args: !SimpleFlagArg
    name: WORKDIR
  optional: true
- !Flag
  description: "Records statistics about the toil workflow to be used\nby 'toil stats'."
  synonyms:
  - --stats
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Determines the deletion of the jobStore upon\ncompletion of the program.\
    \ Choices: 'always',\n'onError','never', 'onSuccess'. The --stats option\nrequires\
    \ information from the jobStore upon completion\nso the jobStore will never be\
    \ deleted withthat flag.\nIf you wish to be able to restart the run, choose\n\
    'never' or 'onSuccess'. Default is 'never' if stats is\nenabled, and 'onSuccess'\
    \ otherwise"
  synonyms:
  - --clean
  args: !ChoiceFlagArg
    choices: !!set
      never:
      onSuccess:
      always:
      onError:
  optional: true
- !Flag
  description: "Determines deletion of temporary worker directory upon\ncompletion\
    \ of a job. Choices: 'always', 'never',\n'onSuccess'. Default = always. WARNING:\
    \ This option\nshould be changed for debugging only. Running a full\npipeline\
    \ with this option could fill your disk with\nintermediate data."
  synonyms:
  - --cleanWorkDir
  args: !ChoiceFlagArg
    choices: !!set
      never:
      onSuccess:
      always:
      onError:
  optional: true
- !Flag
  description: "[CLUSTERSTATS]\nIf enabled, writes out JSON resource usage statistics\n\
    to a file. The default location for this file is the\ncurrent working directory,\
    \ but an absolute path can\nalso be passed to specify where this file should be\n\
    written. This options only applies when using scalable\nbatch systems."
  synonyms:
  - --clusterStats
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "If --restart is specified then will attempt to restart\nexisting workflow\
    \ at the location pointed to by the\n--jobStore option. Will raise an exception\
    \ if the\nworkflow does not exist"
  synonyms:
  - --restart
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "The type of batch system to run the job(s) with,\ncurrently can be\
    \ one of LSF, Mesos, Slurm, Torque,\nHTCondor, singleMachine, parasol, gridEngine'.\n\
    default=singleMachine"
  synonyms:
  - --batchSystem
  args: !SimpleFlagArg
    name: BATCHSYSTEM
  optional: true
- !Flag
  description: "Should hot-deployment of the user script be\ndeactivated? If True,\
    \ the user script/package should\nbe present at the same location on all workers.\n\
    default=false"
  synonyms:
  - --disableHotDeployment
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "The name or path of the parasol program. Will be\nlooked up on PATH\
    \ unless it starts with a\nslashdefault=parasol"
  synonyms:
  - --parasolCommand
  args: !SimpleFlagArg
    name: PARASOLCOMMAND
  optional: true
- !Flag
  description: "Maximum number of job batches the Parasol batch is\nallowed to create.\
    \ One batch is created for jobs with\na a unique set of resource requirements.\
    \ default=1000"
  synonyms:
  - --parasolMaxBatches
  args: !SimpleFlagArg
    name: PARASOLMAXBATCHES
  optional: true
- !Flag
  description: "A scaling factor to change the value of all submitted\ntasks's submitted\
    \ cores. Used in singleMachine batch\nsystem. default=1"
  synonyms:
  - --scale
  args: !SimpleFlagArg
    name: SCALE
  optional: true
- !Flag
  description: "When using Toil's importFile function for staging,\ninput files are\
    \ copied to the job store. Specifying\nthis option saves space by sym-linking\
    \ imported files.\nAs long as caching is enabled Toil will protect the\nfile automatically\
    \ by changing the permissions to\nread-only."
  synonyms:
  - --linkImports
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "The host and port of the Mesos master separated by\ncolon. (default:\
    \ 172.17.0.9:5050)"
  synonyms:
  - --mesosMaster
  args: !SimpleFlagArg
    name: MESOSMASTERADDRESS
  optional: true
- !Flag
  description: "List of node types separated by commas. The syntax for\neach node\
    \ type depends on the provisioner used. For\nthe cgcloud and AWS provisioners\
    \ this is the name of\nan EC2 instance type, optionally followed by a colon\n\
    and the price in dollars to bid for a spot instance of\nthat type, for example\
    \ 'c3.8xlarge:0.42'.If no spot\nbid is specified, nodes of this type will be non-\n\
    preemptable.It is acceptable to specify an instance as\nboth preemptable and non-preemptable,\
    \ including it\ntwice in the list. In that case,preemptable nodes of\nthat type\
    \ will be preferred when creating new nodes\nonce the maximum number of preemptable-nodes\
    \ has\nbeenreached."
  synonyms:
  - --nodeTypes
  args: !SimpleFlagArg
    name: NODETYPES
  optional: true
- !Flag
  description: "Options for provisioning the nodes. The syntax depends\non the provisioner\
    \ used. Neither the CGCloud nor the\nAWS provisioner support any node options."
  synonyms:
  - --nodeOptions
  args: !SimpleFlagArg
    name: NODEOPTIONS
  optional: true
- !Flag
  description: "Mininum number of nodes of each type in the cluster,\nif using auto-scaling.\
    \ This should be provided as a\ncomma-separated list of the same length as the\
    \ list of\nnode types. default=0"
  synonyms:
  - --minNodes
  args: !SimpleFlagArg
    name: MINNODES
  optional: true
- !Flag
  description: "Maximum number of nodes of each type in the cluster,\nif using autoscaling,\
    \ provided as a comma-separated\nlist. The first value is used as a default if\
    \ the list\nlength is less than the number of nodeTypes.\ndefault=10"
  synonyms:
  - --maxNodes
  args: !SimpleFlagArg
    name: MAXNODES
  optional: true
- !Flag
  description: "The total number of nodes estimated to be required to\ncompute the\
    \ issued jobs is multiplied by the alpha\npacking parameter to produce the actual\
    \ number of\nnodes requested. Values of this coefficient greater\nthan one will\
    \ tend to over provision and values less\nthan one will under provision. default=0.8"
  synonyms:
  - --alphaPacking
  args: !SimpleFlagArg
    name: ALPHAPACKING
  optional: true
- !Flag
  description: "A smoothing parameter to prevent unnecessary\noscillations in the\
    \ number of provisioned nodes. If\nthe number of nodes is within the beta inertia\
    \ of the\ncurrently provisioned number of nodes then no change\nis made to the\
    \ number of requested nodes. default=1.2"
  synonyms:
  - --betaInertia
  args: !SimpleFlagArg
    name: BETAINERTIA
  optional: true
- !Flag
  description: "The interval (seconds) between assessing if the scale\nof the cluster\
    \ needs to change. default=30"
  synonyms:
  - --scaleInterval
  args: !SimpleFlagArg
    name: SCALEINTERVAL
  optional: true
- !Flag
  description: "The preference of the autoscaler to replace\npreemptable nodes with\
    \ non-preemptable nodes, when\npreemptable nodes cannot be started for some reason.\n\
    Defaults to 0.0. This value must be between 0.0 and\n1.0, inclusive. A value of\
    \ 0.0 disables such\ncompensation, a value of 0.5 compensates two missing\npreemptable\
    \ nodes with a non-preemptable one. A value\nof 1.0 replaces every missing pre-emptable\
    \ node with a\nnon-preemptable one."
  synonyms:
  - --preemptableCompensation
  args: !SimpleFlagArg
    name: PREEMPTABLECOMPENSATION
  optional: true
- !Flag
  description: "Specify the size of the root volume of worker nodes\nwhen they are\
    \ launched in gigabytes. You may want to\nset this if your jobs require a lot\
    \ of disk space. The\ndefault value is 50."
  synonyms:
  - --nodeStorage
  args: !SimpleFlagArg
    name: NODESTORAGE
  optional: true
- !Flag
  description: "Enable the prometheus/grafana dashboard for monitoring\nCPU/RAM usage,\
    \ queue size, and issued jobs."
  synonyms:
  - --metrics
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "The maximum number of service jobs that can be run\nconcurrently,\
    \ excluding service jobs running on\npreemptable nodes. default=9223372036854775807"
  synonyms:
  - --maxServiceJobs
  args: !SimpleFlagArg
    name: MAXSERVICEJOBS
  optional: true
- !Flag
  description: "The maximum number of service jobs that can run\nconcurrently on preemptable\
    \ nodes.\ndefault=9223372036854775807"
  synonyms:
  - --maxPreemptableServiceJobs
  args: !SimpleFlagArg
    name: MAXPREEMPTABLESERVICEJOBS
  optional: true
- !Flag
  description: "The minimum number of seconds to observe the cluster\nstuck running\
    \ only the same service jobs before\nthrowing a deadlock exception. default=60"
  synonyms:
  - --deadlockWait
  args: !SimpleFlagArg
    name: DEADLOCKWAIT
  optional: true
- !Flag
  description: "Time, in seconds, to wait before doing a scheduler\nquery for job\
    \ state. Return cached results if within\nthe waiting period."
  synonyms:
  - --statePollingWait
  args: !SimpleFlagArg
    name: STATEPOLLINGWAIT
  optional: true
- !Flag
  description: "The default amount of memory to request for a job.\nOnly applicable\
    \ to jobs that do not specify an\nexplicit value for this requirement. Standard\
    \ suffixes\nlike K, Ki, M, Mi, G or Gi are supported. Default is\n2.0 Gi"
  synonyms:
  - --defaultMemory
  args: !SimpleFlagArg
    name: INT
  optional: true
- !Flag
  description: "The default number of CPU cores to dedicate a job.\nOnly applicable\
    \ to jobs that do not specify an\nexplicit value for this requirement. Fractions\
    \ of a\ncore (for example 0.1) are supported on some batch\nsystems, namely Mesos\
    \ and singleMachine. Default is\n1.0"
  synonyms:
  - --defaultCores
  args: !SimpleFlagArg
    name: FLOAT
  optional: true
- !Flag
  description: "The default amount of disk space to dedicate a job.\nOnly applicable\
    \ to jobs that do not specify an\nexplicit value for this requirement. Standard\
    \ suffixes\nlike K, Ki, M, Mi, G or Gi are supported. Default is\n2.0 Gi"
  synonyms:
  - --defaultDisk
  args: !SimpleFlagArg
    name: INT
  optional: true
- !Flag
  description: "--maxCores INT        The maximum number of CPU cores to request from\
    \ the\nbatch system at any one time. Standard suffixes like\nK, Ki, M, Mi, G or\
    \ Gi are supported. Default is 8.0 Ei\n--maxMemory INT       The maximum amount\
    \ of memory to request from the batch\nsystem at any one time. Standard suffixes\
    \ like K, Ki,\nM, Mi, G or Gi are supported. Default is 8.0 Ei\n--maxDisk INT\
    \         The maximum amount of disk space to request from the\nbatch system at\
    \ any one time. Standard suffixes like\nK, Ki, M, Mi, G or Gi are supported. Default\
    \ is 8.0 Ei"
  synonyms:
  - --defaultPreemptable
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Number of times to retry a failing job before giving\nup and labeling\
    \ job failed. default=1"
  synonyms:
  - --retryCount
  args: !SimpleFlagArg
    name: RETRYCOUNT
  optional: true
- !Flag
  description: "Maximum runtime of a job (in seconds) before we kill\nit (this is\
    \ a lower bound, and the actual time before\nkilling the job may be longer).\n\
    default=9223372036854775807"
  synonyms:
  - --maxJobDuration
  args: !SimpleFlagArg
    name: MAXJOBDURATION
  optional: true
- !Flag
  description: "Period of time to wait (in seconds) between checking\nfor missing/overlong\
    \ jobs, that is jobs which get lost\nby the batch system. Expert parameter. default=3600"
  synonyms:
  - --rescueJobsFrequency
  args: !SimpleFlagArg
    name: RESCUEJOBSFREQUENCY
  optional: true
- !Flag
  description: "Disables caching in the file store. This flag must be\nset to use\
    \ a batch system that does not support\ncaching such as Grid Engine, Parasol,\
    \ LSF, or Slurm"
  synonyms:
  - --disableCaching
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "The maximum size of a job log file to keep (in bytes),\nlog files\
    \ larger than this will be truncated to the\nlast X bytes. Setting this option\
    \ to zero will prevent\nany truncation. Setting this option to a negative\nvalue\
    \ will truncate from the beginning.Default=62.5 K"
  synonyms:
  - --maxLogFileSize
  args: !SimpleFlagArg
    name: MAXLOGFILESIZE
  optional: true
- !Flag
  description: "[WRITELOGS]\nWrite worker logs received by the leader into their\n\
    own files at the specified path. The current working\ndirectory will be used if\
    \ a path is not specified\nexplicitly. Note: By default only the logs of failed\n\
    jobs are returned to leader. Set log level to 'debug'\nto get logs back from successful\
    \ jobs, and adjust\n'maxLogFileSize' to control the truncation limit for\nworker\
    \ logs."
  synonyms:
  - --writeLogs
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "[WRITELOGSGZIP]\nIdentical to --writeLogs except the logs files are\n\
    gzipped on the leader."
  synonyms:
  - --writeLogsGzip
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: Enable real-time logging from workers to masters
  synonyms:
  - --realTimeLogging
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Path to file containing 32 character key to be used\nfor server-side\
    \ encryption on awsJobStore or\ngoogleJobStore. SSE will not be used if this flag\
    \ is\nnot passed."
  synonyms:
  - --sseKey
  args: !SimpleFlagArg
    name: SSEKEY
  optional: true
- !Flag
  description: "Path to file containing 256-bit key to be used for\nclient-side encryption\
    \ on azureJobStore. By default,\nno encryption is used."
  synonyms:
  - --cseKey
  args: !SimpleFlagArg
    name: CSEKEY
  optional: true
- !Flag
  description: "=VALUE or NAME, -e NAME=VALUE or NAME\nSet an environment variable\
    \ early on in the worker. If\nVALUE is omitted, it will be looked up in the current\n\
    environment. Independently of this option, the worker\nwill try to emulate the\
    \ leader's environment before\nrunning a job. Using this option, a variable can\
    \ be\ninjected into the worker process itself before it is\nstarted."
  synonyms:
  - --setEnv
  args: !SimpleFlagArg
    name: NAME
  optional: true
- !Flag
  description: "Interval of time service jobs wait between polling for\nthe existence\
    \ of the keep-alive flag (defailt=60)"
  synonyms:
  - --servicePollingInterval
  args: !SimpleFlagArg
    name: SERVICEPOLLINGINTERVAL
  optional: true
- !Flag
  description: "Experimental no forking mode for local debugging.\nSpecifically, workers\
    \ are not forked and stderr/stdout\nare not redirected to the log."
  synonyms:
  - --debugWorker
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "For testing purposes randomly kill 'badWorker'\nproportion of jobs\
    \ using SIGKILL, default=0.0"
  synonyms:
  - --badWorker
  args: !SimpleFlagArg
    name: BADWORKER
  optional: true
- !Flag
  description: "When killing the job pick uniformly within the\ninterval from 0.0\
    \ to 'badWorkerFailInterval' seconds\nafter the worker starts, default=0.01\n"
  synonyms:
  - --badWorkerFailInterval
  args: !SimpleFlagArg
    name: BADWORKERFAILINTERVAL
  optional: true
parent:
subcommands: []
usage: []
help_flag: !Flag
  description: show this help message and exit
  synonyms:
  - -h
  - --help
  args: !EmptyFlagArg {}
  optional: true
usage_flag:
version_flag:
help_text: "usage: cactus [-h] [--logOff] [--logCritical] [--logError] [--logWarning]\n\
  \              [--logInfo] [--logDebug] [--logLevel LOGLEVEL]\n              [--logFile\
  \ LOGFILE] [--rotatingLogging] [--workDir WORKDIR]\n              [--stats] [--clean\
  \ {always,onError,never,onSuccess}]\n              [--cleanWorkDir {always,never,onSuccess,onError}]\n\
  \              [--clusterStats [CLUSTERSTATS]] [--restart]\n              [--batchSystem\
  \ BATCHSYSTEM] [--disableHotDeployment]\n              [--parasolCommand PARASOLCOMMAND]\n\
  \              [--parasolMaxBatches PARASOLMAXBATCHES] [--scale SCALE]\n       \
  \       [--linkImports] [--mesosMaster MESOSMASTERADDRESS]\n              [--provisioner\
  \ {aws}] [--nodeTypes NODETYPES]\n              [--nodeOptions NODEOPTIONS] [--minNodes\
  \ MINNODES]\n              [--maxNodes MAXNODES] [--alphaPacking ALPHAPACKING]\n\
  \              [--betaInertia BETAINERTIA] [--scaleInterval SCALEINTERVAL]\n   \
  \           [--preemptableCompensation PREEMPTABLECOMPENSATION]\n              [--nodeStorage\
  \ NODESTORAGE] [--metrics]\n              [--maxServiceJobs MAXSERVICEJOBS]\n  \
  \            [--maxPreemptableServiceJobs MAXPREEMPTABLESERVICEJOBS]\n         \
  \     [--deadlockWait DEADLOCKWAIT]\n              [--statePollingWait STATEPOLLINGWAIT]\
  \ [--defaultMemory INT]\n              [--defaultCores FLOAT] [--defaultDisk INT]\n\
  \              [--defaultPreemptable] [--maxCores INT] [--maxMemory INT]\n     \
  \         [--maxDisk INT] [--retryCount RETRYCOUNT]\n              [--maxJobDuration\
  \ MAXJOBDURATION]\n              [--rescueJobsFrequency RESCUEJOBSFREQUENCY] [--disableCaching]\n\
  \              [--maxLogFileSize MAXLOGFILESIZE] [--writeLogs [WRITELOGS]]\n   \
  \           [--writeLogsGzip [WRITELOGSGZIP]] [--realTimeLogging]\n            \
  \  [--sseKey SSEKEY] [--cseKey CSEKEY]\n              [--setEnv NAME=VALUE or NAME]\n\
  \              [--servicePollingInterval SERVICEPOLLINGINTERVAL]\n             \
  \ [--debugWorker] [--badWorker BADWORKER]\n              [--badWorkerFailInterval\
  \ BADWORKERFAILINTERVAL]\n              [--experiment EXPERIMENTFILE] [--buildAvgs]\
  \ [--buildReference]\n              [--buildHal] [--buildFasta]\n              [--intermediateResultsUrl\
  \ INTERMEDIATERESULTSURL]\n              [--database DATABASE] [--configFile CONFIGFILE]\
  \ [--root ROOT]\n              [--latest] [--containerImage CONTAINERIMAGE]\n  \
  \            [--binariesMode {docker,local,singularity}]\n              jobStore\
  \ seqFile outputHal\n\npositional arguments:\n  seqFile               Seq file\n\
  \  outputHal             Output HAL file\n\noptional arguments:\n  -h, --help  \
  \          show this help message and exit\n  --experiment EXPERIMENTFILE\n    \
  \                    The file containing a link to the experiment\n            \
  \            parameters\n  --buildAvgs           Build trees\n  --buildReference\
  \      Creates a reference ordering for the flowers\n  --buildHal            Build\
  \ a hal file\n  --buildFasta          Build a fasta file of the input sequences\
  \ (and\n                        reference sequence, used with hal output)\n  --intermediateResultsUrl\
  \ INTERMEDIATERESULTSURL\n                        URL prefix to save intermediate\
  \ results like DB dumps\n                        to (e.g. prefix-dump-caf, prefix-dump-avg,\
  \ etc.)\n  --database DATABASE   Database type: tokyo_cabinet or kyoto_tycoon [default:\n\
  \                        kyoto_tycoon]\n  --configFile CONFIGFILE\n            \
  \            Specify cactus configuration file\n  --root ROOT           Name of\
  \ ancestral node (which must appear in NEWICK\n                        tree in <seqfile>)\
  \ to use as a root for the alignment.\n                        Any genomes not below\
  \ this node in the tree may be\n                        used as outgroups but will\
  \ never appear in the output.\n                        If no root is specifed then\
  \ the root of the tree is\n                        used.\n  --latest           \
  \   Use the latest version of the docker container rather\n                    \
  \    than pulling one matching this version of cactus\n  --containerImage CONTAINERIMAGE\n\
  \                        Use the the specified pre-built containter image\n    \
  \                    rather than pulling one from quay.io\n  --binariesMode {docker,local,singularity}\n\
  \                        The way to run the Cactus binaries\n\nLogging Options:\n\
  \  Options that control logging\n\n  --logOff              Same as --logCritical\n\
  \  --logCritical         Turn on logging at level CRITICAL and above. (default\n\
  \                        is INFO)\n  --logError            Turn on logging at level\
  \ ERROR and above. (default is\n                        INFO)\n  --logWarning  \
  \        Turn on logging at level WARNING and above. (default\n                \
  \        is INFO)\n  --logInfo             Turn on logging at level INFO and above.\
  \ (default is\n                        INFO)\n  --logDebug            Turn on logging\
  \ at level DEBUG and above. (default is\n                        INFO)\n  --logLevel\
  \ LOGLEVEL   Log at given level (may be either OFF (or CRITICAL),\n            \
  \            ERROR, WARN (or WARNING), INFO or DEBUG). (default is\n           \
  \             INFO)\n  --logFile LOGFILE     File to log in\n  --rotatingLogging\
  \     Turn on rotating logging, which prevents log files\n                     \
  \   getting too big.\n\ntoil core options:\n  Options to specify the location of\
  \ the Toil workflow and turn on stats\n  collation about the performance of jobs.\n\
  \n  jobStore              The location of the job store for the workflow. A job\n\
  \                        store holds persistent information about the jobs and\n\
  \                        files in a workflow. If the workflow is run with a\n  \
  \                      distributed batch system, the job store must be\n       \
  \                 accessible by all worker nodes. Depending on the\n           \
  \             desired job store implementation, the location should\n          \
  \              be formatted according to one of the following\n                \
  \        schemes: file:<path> where <path> points to a\n                       \
  \ directory on the file systen aws:<region>:<prefix>\n                        where\
  \ <region> is the name of an AWS region like us-\n                        west-2\
  \ and <prefix> will be prepended to the names of\n                        any top-level\
  \ AWS resources in use by job store, e.g.\n                        S3 buckets. azure:<account>:<prefix>\n\
  \                        google:<project_id>:<prefix> TODO: explain For\n      \
  \                  backwards compatibility, you may also specify ./foo\n       \
  \                 (equivalent to file:./foo or just file:foo) or /bar\n        \
  \                (equivalent to file:/bar).\n  --workDir WORKDIR     Absolute path\
  \ to directory where temporary files\n                        generated during the\
  \ Toil run should be placed. Temp\n                        files and folders will\
  \ be placed in a directory\n                        toil-<workflowID> within workDir\
  \ (The workflowID is\n                        generated by Toil and will be reported\
  \ in the workflow\n                        logs. Default is determined by the variables\
  \ (TMPDIR,\n                        TEMP, TMP) via mkdtemp. This directory needs\
  \ to exist\n                        on all machines running jobs.\n  --stats   \
  \            Records statistics about the toil workflow to be used\n           \
  \             by 'toil stats'.\n  --clean {always,onError,never,onSuccess}\n   \
  \                     Determines the deletion of the jobStore upon\n           \
  \             completion of the program. Choices: 'always',\n                  \
  \      'onError','never', 'onSuccess'. The --stats option\n                    \
  \    requires information from the jobStore upon completion\n                  \
  \      so the jobStore will never be deleted withthat flag.\n                  \
  \      If you wish to be able to restart the run, choose\n                     \
  \   'never' or 'onSuccess'. Default is 'never' if stats is\n                   \
  \     enabled, and 'onSuccess' otherwise\n  --cleanWorkDir {always,never,onSuccess,onError}\n\
  \                        Determines deletion of temporary worker directory upon\n\
  \                        completion of a job. Choices: 'always', 'never',\n    \
  \                    'onSuccess'. Default = always. WARNING: This option\n     \
  \                   should be changed for debugging only. Running a full\n     \
  \                   pipeline with this option could fill your disk with\n      \
  \                  intermediate data.\n  --clusterStats [CLUSTERSTATS]\n       \
  \                 If enabled, writes out JSON resource usage statistics\n      \
  \                  to a file. The default location for this file is the\n      \
  \                  current working directory, but an absolute path can\n       \
  \                 also be passed to specify where this file should be\n        \
  \                written. This options only applies when using scalable\n      \
  \                  batch systems.\n\ntoil options for restarting an existing workflow:\n\
  \  Allows the restart of an existing workflow\n\n  --restart             If --restart\
  \ is specified then will attempt to restart\n                        existing workflow\
  \ at the location pointed to by the\n                        --jobStore option.\
  \ Will raise an exception if the\n                        workflow does not exist\n\
  \ntoil options for specifying the batch system:\n  Allows the specification of the\
  \ batch system, and arguments to the batch\n  system/big batch system (see below).\n\
  \n  --batchSystem BATCHSYSTEM\n                        The type of batch system\
  \ to run the job(s) with,\n                        currently can be one of LSF,\
  \ Mesos, Slurm, Torque,\n                        HTCondor, singleMachine, parasol,\
  \ gridEngine'.\n                        default=singleMachine\n  --disableHotDeployment\n\
  \                        Should hot-deployment of the user script be\n         \
  \               deactivated? If True, the user script/package should\n         \
  \               be present at the same location on all workers.\n              \
  \          default=false\n  --parasolCommand PARASOLCOMMAND\n                  \
  \      The name or path of the parasol program. Will be\n                      \
  \  looked up on PATH unless it starts with a\n                        slashdefault=parasol\n\
  \  --parasolMaxBatches PARASOLMAXBATCHES\n                        Maximum number\
  \ of job batches the Parasol batch is\n                        allowed to create.\
  \ One batch is created for jobs with\n                        a a unique set of\
  \ resource requirements. default=1000\n  --scale SCALE         A scaling factor\
  \ to change the value of all submitted\n                        tasks's submitted\
  \ cores. Used in singleMachine batch\n                        system. default=1\n\
  \  --linkImports         When using Toil's importFile function for staging,\n  \
  \                      input files are copied to the job store. Specifying\n   \
  \                     this option saves space by sym-linking imported files.\n \
  \                       As long as caching is enabled Toil will protect the\n  \
  \                      file automatically by changing the permissions to\n     \
  \                   read-only.\n  --mesosMaster MESOSMASTERADDRESS\n           \
  \             The host and port of the Mesos master separated by\n             \
  \           colon. (default: 172.17.0.9:5050)\n\ntoil options for autoscaling the\
  \ cluster of worker nodes:\n  Allows the specification of the minimum and maximum\
  \ number of nodes in an\n  autoscaled cluster, as well as parameters to control\
  \ the level of\n  provisioning.\n\n  --provisioner {aws}   The provisioner for cluster\
  \ auto-scaling. The\n                        currently supported choices are'cgcloud'\
  \ or 'aws'. The\n                        default is None.\n  --nodeTypes NODETYPES\n\
  \                        List of node types separated by commas. The syntax for\n\
  \                        each node type depends on the provisioner used. For\n \
  \                       the cgcloud and AWS provisioners this is the name of\n \
  \                       an EC2 instance type, optionally followed by a colon\n \
  \                       and the price in dollars to bid for a spot instance of\n\
  \                        that type, for example 'c3.8xlarge:0.42'.If no spot\n \
  \                       bid is specified, nodes of this type will be non-\n    \
  \                    preemptable.It is acceptable to specify an instance as\n  \
  \                      both preemptable and non-preemptable, including it\n    \
  \                    twice in the list. In that case,preemptable nodes of\n    \
  \                    that type will be preferred when creating new nodes\n     \
  \                   once the maximum number of preemptable-nodes has\n         \
  \               beenreached.\n  --nodeOptions NODEOPTIONS\n                    \
  \    Options for provisioning the nodes. The syntax depends\n                  \
  \      on the provisioner used. Neither the CGCloud nor the\n                  \
  \      AWS provisioner support any node options.\n  --minNodes MINNODES   Mininum\
  \ number of nodes of each type in the cluster,\n                        if using\
  \ auto-scaling. This should be provided as a\n                        comma-separated\
  \ list of the same length as the list of\n                        node types. default=0\n\
  \  --maxNodes MAXNODES   Maximum number of nodes of each type in the cluster,\n\
  \                        if using autoscaling, provided as a comma-separated\n \
  \                       list. The first value is used as a default if the list\n\
  \                        length is less than the number of nodeTypes.\n        \
  \                default=10\n  --alphaPacking ALPHAPACKING\n                   \
  \     The total number of nodes estimated to be required to\n                  \
  \      compute the issued jobs is multiplied by the alpha\n                    \
  \    packing parameter to produce the actual number of\n                       \
  \ nodes requested. Values of this coefficient greater\n                        than\
  \ one will tend to over provision and values less\n                        than\
  \ one will under provision. default=0.8\n  --betaInertia BETAINERTIA\n         \
  \               A smoothing parameter to prevent unnecessary\n                 \
  \       oscillations in the number of provisioned nodes. If\n                  \
  \      the number of nodes is within the beta inertia of the\n                 \
  \       currently provisioned number of nodes then no change\n                 \
  \       is made to the number of requested nodes. default=1.2\n  --scaleInterval\
  \ SCALEINTERVAL\n                        The interval (seconds) between assessing\
  \ if the scale\n                        of the cluster needs to change. default=30\n\
  \  --preemptableCompensation PREEMPTABLECOMPENSATION\n                        The\
  \ preference of the autoscaler to replace\n                        preemptable nodes\
  \ with non-preemptable nodes, when\n                        preemptable nodes cannot\
  \ be started for some reason.\n                        Defaults to 0.0. This value\
  \ must be between 0.0 and\n                        1.0, inclusive. A value of 0.0\
  \ disables such\n                        compensation, a value of 0.5 compensates\
  \ two missing\n                        preemptable nodes with a non-preemptable\
  \ one. A value\n                        of 1.0 replaces every missing pre-emptable\
  \ node with a\n                        non-preemptable one.\n  --nodeStorage NODESTORAGE\n\
  \                        Specify the size of the root volume of worker nodes\n \
  \                       when they are launched in gigabytes. You may want to\n \
  \                       set this if your jobs require a lot of disk space. The\n\
  \                        default value is 50.\n  --metrics             Enable the\
  \ prometheus/grafana dashboard for monitoring\n                        CPU/RAM usage,\
  \ queue size, and issued jobs.\n\ntoil options for limiting the number of service\
  \ jobs and detecting service deadlocks:\n  Allows the specification of the maximum\
  \ number of service jobs in a\n  cluster. By keeping this limited we can avoid all\
  \ the nodes being occupied\n  with services, so causing a deadlock\n\n  --maxServiceJobs\
  \ MAXSERVICEJOBS\n                        The maximum number of service jobs that\
  \ can be run\n                        concurrently, excluding service jobs running\
  \ on\n                        preemptable nodes. default=9223372036854775807\n \
  \ --maxPreemptableServiceJobs MAXPREEMPTABLESERVICEJOBS\n                      \
  \  The maximum number of service jobs that can run\n                        concurrently\
  \ on preemptable nodes.\n                        default=9223372036854775807\n \
  \ --deadlockWait DEADLOCKWAIT\n                        The minimum number of seconds\
  \ to observe the cluster\n                        stuck running only the same service\
  \ jobs before\n                        throwing a deadlock exception. default=60\n\
  \  --statePollingWait STATEPOLLINGWAIT\n                        Time, in seconds,\
  \ to wait before doing a scheduler\n                        query for job state.\
  \ Return cached results if within\n                        the waiting period.\n\
  \ntoil options for cores/memory requirements:\n  The options to specify default\
  \ cores/memory requirements (if not specified\n  by the jobs themselves), and to\
  \ limit the total amount of memory/cores\n  requested from the batch system.\n\n\
  \  --defaultMemory INT   The default amount of memory to request for a job.\n  \
  \                      Only applicable to jobs that do not specify an\n        \
  \                explicit value for this requirement. Standard suffixes\n      \
  \                  like K, Ki, M, Mi, G or Gi are supported. Default is\n      \
  \                  2.0 Gi\n  --defaultCores FLOAT  The default number of CPU cores\
  \ to dedicate a job.\n                        Only applicable to jobs that do not\
  \ specify an\n                        explicit value for this requirement. Fractions\
  \ of a\n                        core (for example 0.1) are supported on some batch\n\
  \                        systems, namely Mesos and singleMachine. Default is\n \
  \                       1.0\n  --defaultDisk INT     The default amount of disk\
  \ space to dedicate a job.\n                        Only applicable to jobs that\
  \ do not specify an\n                        explicit value for this requirement.\
  \ Standard suffixes\n                        like K, Ki, M, Mi, G or Gi are supported.\
  \ Default is\n                        2.0 Gi\n  --defaultPreemptable\n  --maxCores\
  \ INT        The maximum number of CPU cores to request from the\n             \
  \           batch system at any one time. Standard suffixes like\n             \
  \           K, Ki, M, Mi, G or Gi are supported. Default is 8.0 Ei\n  --maxMemory\
  \ INT       The maximum amount of memory to request from the batch\n           \
  \             system at any one time. Standard suffixes like K, Ki,\n          \
  \              M, Mi, G or Gi are supported. Default is 8.0 Ei\n  --maxDisk INT\
  \         The maximum amount of disk space to request from the\n               \
  \         batch system at any one time. Standard suffixes like\n               \
  \         K, Ki, M, Mi, G or Gi are supported. Default is 8.0 Ei\n\ntoil options\
  \ for rescuing/killing/restarting jobs:\n  The options for jobs that either run\
  \ too long/fail or get lost (some batch\n  systems have issues!)\n\n  --retryCount\
  \ RETRYCOUNT\n                        Number of times to retry a failing job before\
  \ giving\n                        up and labeling job failed. default=1\n  --maxJobDuration\
  \ MAXJOBDURATION\n                        Maximum runtime of a job (in seconds)\
  \ before we kill\n                        it (this is a lower bound, and the actual\
  \ time before\n                        killing the job may be longer).\n       \
  \                 default=9223372036854775807\n  --rescueJobsFrequency RESCUEJOBSFREQUENCY\n\
  \                        Period of time to wait (in seconds) between checking\n\
  \                        for missing/overlong jobs, that is jobs which get lost\n\
  \                        by the batch system. Expert parameter. default=3600\n\n\
  toil miscellaneous options:\n  Miscellaneous options\n\n  --disableCaching     \
  \ Disables caching in the file store. This flag must be\n                      \
  \  set to use a batch system that does not support\n                        caching\
  \ such as Grid Engine, Parasol, LSF, or Slurm\n  --maxLogFileSize MAXLOGFILESIZE\n\
  \                        The maximum size of a job log file to keep (in bytes),\n\
  \                        log files larger than this will be truncated to the\n \
  \                       last X bytes. Setting this option to zero will prevent\n\
  \                        any truncation. Setting this option to a negative\n   \
  \                     value will truncate from the beginning.Default=62.5 K\n  --writeLogs\
  \ [WRITELOGS]\n                        Write worker logs received by the leader\
  \ into their\n                        own files at the specified path. The current\
  \ working\n                        directory will be used if a path is not specified\n\
  \                        explicitly. Note: By default only the logs of failed\n\
  \                        jobs are returned to leader. Set log level to 'debug'\n\
  \                        to get logs back from successful jobs, and adjust\n   \
  \                     'maxLogFileSize' to control the truncation limit for\n   \
  \                     worker logs.\n  --writeLogsGzip [WRITELOGSGZIP]\n        \
  \                Identical to --writeLogs except the logs files are\n          \
  \              gzipped on the leader.\n  --realTimeLogging     Enable real-time\
  \ logging from workers to masters\n  --sseKey SSEKEY       Path to file containing\
  \ 32 character key to be used\n                        for server-side encryption\
  \ on awsJobStore or\n                        googleJobStore. SSE will not be used\
  \ if this flag is\n                        not passed.\n  --cseKey CSEKEY      \
  \ Path to file containing 256-bit key to be used for\n                        client-side\
  \ encryption on azureJobStore. By default,\n                        no encryption\
  \ is used.\n  --setEnv NAME=VALUE or NAME, -e NAME=VALUE or NAME\n             \
  \           Set an environment variable early on in the worker. If\n           \
  \             VALUE is omitted, it will be looked up in the current\n          \
  \              environment. Independently of this option, the worker\n         \
  \               will try to emulate the leader's environment before\n          \
  \              running a job. Using this option, a variable can be\n           \
  \             injected into the worker process itself before it is\n           \
  \             started.\n  --servicePollingInterval SERVICEPOLLINGINTERVAL\n    \
  \                    Interval of time service jobs wait between polling for\n  \
  \                      the existence of the keep-alive flag (defailt=60)\n\ntoil\
  \ debug options:\n  Debug options\n\n  --debugWorker         Experimental no forking\
  \ mode for local debugging.\n                        Specifically, workers are not\
  \ forked and stderr/stdout\n                        are not redirected to the log.\n\
  \  --badWorker BADWORKER\n                        For testing purposes randomly\
  \ kill 'badWorker'\n                        proportion of jobs using SIGKILL, default=0.0\n\
  \  --badWorkerFailInterval BADWORKERFAILINTERVAL\n                        When killing\
  \ the job pick uniformly within the\n                        interval from 0.0 to\
  \ 'badWorkerFailInterval' seconds\n                        after the worker starts,\
  \ default=0.01\n"
generated_using:
- --help
