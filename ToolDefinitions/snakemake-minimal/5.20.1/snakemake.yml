!Command
command:
- snakemake
positional:
- !Positional
  description: "Targets to build. May be rules or files. (default:\nNone)"
  position: 0
  name: target
  optional: false
named:
- !Flag
  description: "Do not execute anything, and display what would be\ndone. If you have\
    \ a very large workflow, use --dry-run\n--quiet to just print a summary of the\
    \ DAG of jobs.\n(default: False)"
  synonyms:
  - --dry-run
  - --dryrun
  - -n
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Name of profile to use for configuring Snakemake.\nSnakemake will\
    \ search for a corresponding folder in\n/etc/xdg/snakemake and /root/.config/snakemake.\n\
    Alternatively, this can be an absolute or relative\npath. The profile folder has\
    \ to contain a file\n'config.yaml'. This file can be used to set default\nvalues\
    \ for command line options in YAML format. For\nexample, '--cluster qsub' becomes\
    \ 'cluster: qsub' in\nthe YAML file. Profiles can be obtained from\nhttps://github.com/snakemake-profiles.\
    \ (default: None)"
  synonyms:
  - --profile
  args: !SimpleFlagArg
    name: PROFILE
  optional: true
- !Flag
  description: "[RULE [RULE ...]]\nStore output files of given rules in a central\
    \ cache\ngiven by the environment variable\n$SNAKEMAKE_OUTPUT_CACHE. Likewise,\
    \ retrieve output\nfiles of the given rules from this cache if they have\nbeen\
    \ created before (by anybody writing to the same\ncache), instead of actually\
    \ executing the rules.\nOutput files are identified by hashing all steps,\nparameters\
    \ and software stack (conda envs or\ncontainers) needed to create them. (default:\
    \ None)"
  synonyms:
  - --cache
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "The workflow definition in form of a\nsnakefile.Usually, you should\
    \ not need to specify\nthis. By default, Snakemake will search for\n'Snakefile',\
    \ 'snakefile', 'workflow/Snakefile',\n'workflow/snakefile' beneath the current\
    \ working\ndirectory, in this order. Only if you definitely want\na different\
    \ layout, you need to use this parameter.\n(default: None)"
  synonyms:
  - --snakefile
  - -s
  args: !SimpleFlagArg
    name: FILE
  optional: true
- !Flag
  description: "[N], --jobs [N], -j [N]\nUse at most N CPU cores/jobs in parallel.\
    \ If N is\nomitted or 'all', the limit is set to the number of\navailable CPU\
    \ cores. (default: None)"
  synonyms:
  - --cores
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "In cluster mode, use at most N cores of the host\nmachine in parallel\
    \ (default: number of CPU cores of\nthe host). The cores are used to execute local\
    \ rules.\nThis option is ignored when not in cluster mode.\n(default: 8)"
  synonyms:
  - --local-cores
  args: !SimpleFlagArg
    name: N
  optional: true
- !Flag
  description: "[NAME=INT [NAME=INT ...]], --res [NAME=INT [NAME=INT ...]]\nDefine\
    \ additional resources that shall constrain the\nscheduling analogously to threads\
    \ (see above). A\nresource is defined as a name and an integer value.\nE.g. --resources\
    \ mem_mb=1000. Rules can use resources\nby defining the resource keyword, e.g.\
    \ resources:\nmem_mb=600. If now two rules require 600 of the\nresource 'mem_mb'\
    \ they won't be run in parallel by the\nscheduler. (default: None)"
  synonyms:
  - --resources
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "[RULE=THREADS [RULE=THREADS ...]]\nOverwrite thread usage of rules.\
    \ This allows to fine-\ntune workflow parallelization. In particular, this is\n\
    helpful to target certain cluster nodes by e.g.\nshifting a rule to use more,\
    \ or less threads than\ndefined in the workflow. Thereby, THREADS has to be a\n\
    positive integer, and RULE has to be the name of the\nrule. (default: None)"
  synonyms:
  - --set-threads
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "[NAME=INT [NAME=INT ...]], --default-res [NAME=INT [NAME=INT ...]]\n\
    Define default values of resources for rules that do\nnot define their own values.\
    \ In addition to plain\nintegers, python expressions over inputsize are\nallowed\
    \ (e.g. '2*input.size_mb').When specifying this\nwithout any arguments (--default-resources),\
    \ it\ndefines 'mem_mb=max(2*input.size_mb, 1000)'\n'disk_mb=max(2*input.size_mb,\
    \ 1000)', i.e., default\ndisk and mem usage is twice the input file size but at\n\
    least 1GB. (default: None)"
  synonyms:
  - --default-resources
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "[KEY=VALUE [KEY=VALUE ...]], -C [KEY=VALUE [KEY=VALUE ...]]\nSet or\
    \ overwrite values in the workflow config object.\nThe workflow config object\
    \ is accessible as variable\nconfig inside the workflow. Default values can be\
    \ set\nby providing a JSON file (see Documentation).\n(default: None)"
  synonyms:
  - --config
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Specify or overwrite the config file of the workflow\n(see the docs).\
    \ Values specified in JSON or YAML\nformat are available in the global config\
    \ dictionary\ninside the workflow. Multiple files overwrite each\nother in the\
    \ given order. (default: None)"
  synonyms:
  - --configfile
  - --configfiles
  args: !RepeatFlagArg
    name: FILE
  optional: true
- !Flag
  description: "Environment variables to pass to cloud jobs. (default:\nNone)"
  synonyms:
  - --envvars
  args: !RepeatFlagArg
    name: VARNAME
  optional: true
- !Flag
  description: "Specify working directory (relative paths in the\nsnakefile will use\
    \ this as their origin). (default:\nNone)"
  synonyms:
  - --directory
  - -d
  args: !SimpleFlagArg
    name: DIR
  optional: true
- !Flag
  description: "Touch output files (mark them up to date without\nreally changing\
    \ them) instead of running their\ncommands. This is used to pretend that the rules\
    \ were\nexecuted, in order to fool future invocations of\nsnakemake. Fails if\
    \ a file does not yet exist. Note\nthat this will only touch files that would\
    \ otherwise\nbe recreated by Snakemake (e.g. because their input\nfiles are newer).\
    \ For enforcing a touch, combine this\nwith --force, --forceall, or --forcerun.\
    \ Note however\nthat you loose the provenance information when the\nfiles have\
    \ been created in realitiy. Hence, this\nshould be used only as a last resort.\
    \ (default: False)"
  synonyms:
  - --touch
  - -t
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Go on with independent jobs if a job fails. (default:\nFalse)"
  synonyms:
  - --keep-going
  - -k
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Force the execution of the selected target or the\nfirst rule regardless\
    \ of already created output.\n(default: False)"
  synonyms:
  - --force
  - -f
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Force the execution of the selected (or the first)\nrule and all rules\
    \ it is dependent on regardless of\nalready created output. (default: False)"
  synonyms:
  - --forceall
  - -F
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "[TARGET [TARGET ...]], -R [TARGET [TARGET ...]]\nForce the re-execution\
    \ or creation of the given rules\nor files. Use this option if you changed a rule\
    \ and\nwant to have all its output in your workflow updated.\n(default: None)"
  synonyms:
  - --forcerun
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Tell the scheduler to assign creation of given targets\n(and all their\
    \ dependencies) highest priority.\n(EXPERIMENTAL) (default: None)"
  synonyms:
  - --prioritize
  - -P
  args: !RepeatFlagArg
    name: TARGET
  optional: true
- !Flag
  description: "=BATCH/BATCHES\nOnly create the given BATCH of the input files of\
    \ the\ngiven RULE. This can be used to iteratively run parts\nof very large workflows.\
    \ Only the execution plan of\nthe relevant part of the workflow has to be\ncalculated,\
    \ thereby speeding up DAG computation. It is\nrecommended to provide the most\
    \ suitable rule for\nbatching when documenting a workflow. It should be\nsome\
    \ aggregating rule that would be executed only\nonce, and has a large number of\
    \ input files. For\nexample, it can be a rule that aggregates over\nsamples. (default:\
    \ None)"
  synonyms:
  - --batch
  args: !SimpleFlagArg
    name: RULE
  optional: true
- !Flag
  description: "Runs the pipeline until it reaches the specified rules\nor files.\
    \ Only runs jobs that are dependencies of the\nspecified rule or files, does not\
    \ run sibling DAGs.\n(default: None)"
  synonyms:
  - --until
  - -U
  args: !RepeatFlagArg
    name: TARGET
  optional: true
- !Flag
  description: "Prevent the execution or creation of the given rules\nor files as\
    \ well as any rules or files that are\ndownstream of these targets in the DAG.\
    \ Also runs jobs\nin sibling DAGs that are independent of the rules or\nfiles\
    \ specified here. (default: None)"
  synonyms:
  - --omit-from
  - -O
  args: !RepeatFlagArg
    name: TARGET
  optional: true
- !Flag
  description: "Re-run all jobs the output of which is recognized as\nincomplete.\
    \ (default: False)"
  synonyms:
  - --rerun-incomplete
  - --ri
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Specify a directory in which the 'shadow' directory is\ncreated. If\
    \ not supplied, the value is set to the\n'.snakemake' directory relative to the\
    \ working\ndirectory. (default: None)"
  synonyms:
  - --shadow-prefix
  args: !SimpleFlagArg
    name: DIR
  optional: true
- !Flag
  description: "[FILE]       Create an HTML report with results and statistics.\n\
    This can be either a .html file or a .zip file. In the\nformer case, all results\
    \ are embedded into the .html\n(this only works for small data). In the latter\
    \ case,\nresults are stored along with a file report.html in\nthe zip archive.\
    \ If no filename is given, an embedded\nreport.html is the default. (default:\
    \ None)"
  synonyms:
  - --report
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Custom stylesheet to use for report. In particular,\nthis can be used\
    \ for branding the report with e.g. a\ncustom logo, see docs. (default: None)"
  synonyms:
  - --report-stylesheet
  args: !SimpleFlagArg
    name: CSSFILE
  optional: true
- !Flag
  description: "Interactively edit the notebook associated with the\nrule used to\
    \ generate the given target file. This will\nstart a local jupyter notebook server.\
    \ Any changes to\nthe notebook should be saved, and the server has to be\nstopped\
    \ by closing the notebook and hitting the 'Quit'\nbutton on the jupyter dashboard.\
    \ Afterwards, the\nupdated notebook will be automatically stored in the\npath\
    \ defined in the rule. If the notebook is not yet\npresent, this will create an\
    \ empty draft. (default:\nNone)"
  synonyms:
  - --edit-notebook
  args: !SimpleFlagArg
    name: TARGET
  optional: true
- !Flag
  description: ":PORT\nThe IP address and PORT the notebook server used for\nediting\
    \ the notebook (--edit-notebook) will listen on.\n(default: localhost:8888)"
  synonyms:
  - --notebook-listen
  args: !SimpleFlagArg
    name: IP
  optional: true
- !Flag
  description: "[{text,json}]  Perform linting on the given workflow. This will print\n\
    snakemake specific suggestions to improve code quality\n(work in progress, more\
    \ lints to be added in the\nfuture). If no argument is provided, plain text output\n\
    is used. (default: None)"
  synonyms:
  - --lint
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Compile workflow to CWL and store it in given FILE.\n(default: None)"
  synonyms:
  - --export-cwl
  args: !SimpleFlagArg
    name: FILE
  optional: true
- !Flag
  description: "Show available rules in given Snakefile. (default:\nFalse)"
  synonyms:
  - --list
  - -l
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Show available target rules in given Snakefile.\n(default: False)"
  synonyms:
  - --list-target-rules
  - --lt
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Do not execute anything and print the directed acyclic\ngraph of jobs\
    \ in the dot language. Recommended use on\nUnix systems: snakemake --dag | dot\
    \ | displayNote\nprint statements in your Snakefile may interfere with\nvisualization.\
    \ (default: False)"
  synonyms:
  - --dag
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Do not execute anything and print the dependency graph\nof rules in\
    \ the dot language. This will be less\ncrowded than above DAG of jobs, but also\
    \ show less\ninformation. Note that each rule is displayed once,\nhence the displayed\
    \ graph will be cyclic if a rule\nappears in several steps of the workflow. Use\
    \ this if\nabove option leads to a DAG that is too large.\nRecommended use on\
    \ Unix systems: snakemake --rulegraph\n| dot | displayNote print statements in\
    \ your Snakefile\nmay interfere with visualization. (default: False)"
  synonyms:
  - --rulegraph
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Do not execute anything and print the dependency graph\nof rules with\
    \ their input and output files in the dot\nlanguage. This is an intermediate solution\
    \ between\nabove DAG of jobs and the rule graph. Note that each\nrule is displayed\
    \ once, hence the displayed graph will\nbe cyclic if a rule appears in several\
    \ steps of the\nworkflow. Use this if above option leads to a DAG that\nis too\
    \ large. Recommended use on Unix systems:\nsnakemake --filegraph | dot | displayNote\
    \ print\nstatements in your Snakefile may interfere with\nvisualization. (default:\
    \ False)"
  synonyms:
  - --filegraph
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Print the DAG in D3.js compatible JSON format.\n(default: False)"
  synonyms:
  - --d3dag
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Print a summary of all files created by the workflow.\nThe has the\
    \ following columns: filename, modification\ntime, rule version, status, plan.\
    \ Thereby rule version\ncontains the versionthe file was created with (see the\n\
    version keyword of rules), and status denotes whether\nthe file is missing, its\
    \ input files are newer or if\nversion or implementation of the rule changed since\n\
    file creation. Finally the last column denotes whether\nthe file will be updated\
    \ or created during the next\nworkflow execution. (default: False)"
  synonyms:
  - --summary
  - -S
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Print a summary of all files created by the workflow.\nThe has the\
    \ following columns: filename, modification\ntime, rule version, input file(s),\
    \ shell command,\nstatus, plan. Thereby rule version contains the\nversion the\
    \ file was created with (see the version\nkeyword of rules), and status denotes\
    \ whether the file\nis missing, its input files are newer or if version or\nimplementation\
    \ of the rule changed since file\ncreation. The input file and shell command columns\
    \ are\nself explanatory. Finally the last column denotes\nwhether the file will\
    \ be updated or created during the\nnext workflow execution. (default: False)"
  synonyms:
  - --detailed-summary
  - -D
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Archive the workflow into the given tar archive FILE.\nThe archive\
    \ will be created such that the workflow can\nbe re-executed on a vanilla system.\
    \ The function needs\nconda and git to be installed. It will archive every\nfile\
    \ that is under git version control. Note that it\nis best practice to have the\
    \ Snakefile, config files,\nand scripts under version control. Hence, they will\
    \ be\nincluded in the archive. Further, it will add input\nfiles that are not\
    \ generated by by the workflow itself\nand conda environments. Note that symlinks\
    \ are\ndereferenced. Supported formats are .tar, .tar.gz,\n.tar.bz2 and .tar.xz.\
    \ (default: None)"
  synonyms:
  - --archive
  args: !SimpleFlagArg
    name: FILE
  optional: true
- !Flag
  description: "Cleanup the metadata of given files. That means that\nsnakemake removes\
    \ any tracked version info, and any\nmarks that files are incomplete. (default:\
    \ None)"
  synonyms:
  - --cleanup-metadata
  - --cm
  args: !RepeatFlagArg
    name: FILE
  optional: true
- !Flag
  description: "Cleanup old shadow directories which have not been\ndeleted due to\
    \ failures or power loss. (default:\nFalse)"
  synonyms:
  - --cleanup-shadow
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Don't delete wrapper scripts used for execution\n(default: False)"
  synonyms:
  - --skip-script-cleanup
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Remove a lock on the working directory. (default:\nFalse)"
  synonyms:
  - --unlock
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "List all output files that have been created with a\ndifferent version\
    \ (as determined by the version\nkeyword). (default: False)"
  synonyms:
  - --list-version-changes
  - --lv
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "List all output files for which the rule body (run or\nshell) have\
    \ changed in the Snakefile. (default: False)"
  synonyms:
  - --list-code-changes
  - --lc
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "List all output files for which the defined input\nfiles have changed\
    \ in the Snakefile (e.g. new input\nfiles were added in the rule definition or\
    \ files were\nrenamed). For listing input file modification in the\nfilesystem,\
    \ use --summary. (default: False)"
  synonyms:
  - --list-input-changes
  - --li
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "List all output files for which the defined params\nhave changed in\
    \ the Snakefile. (default: False)"
  synonyms:
  - --list-params-changes
  - --lp
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "List all files in the working directory that are not\nused in the\
    \ workflow. This can be used e.g. for\nidentifying leftover files. Hidden files\
    \ and\ndirectories are ignored. (default: False)"
  synonyms:
  - --list-untracked
  - --lu
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Remove all files generated by the workflow. Use\ntogether with --dry-run\
    \ to list files without actually\ndeleting anything. Note that this will not recurse\n\
    into subworkflows. Write-protected files are not\nremoved. Nevertheless, use with\
    \ care! (default: False)"
  synonyms:
  - --delete-all-output
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Remove all temporary files generated by the workflow.\nUse together\
    \ with --dry-run to list files without\nactually deleting anything. Note that\
    \ this will not\nrecurse into subworkflows. (default: False)"
  synonyms:
  - --delete-temp-output
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Output code to register bash completion for snakemake.\nPut the following\
    \ in your .bashrc (including the\naccents): `snakemake --bash-completion` or issue\
    \ it in\nan open terminal session. (default: False)"
  synonyms:
  - --bash-completion
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Do not remove incomplete output files by failed jobs.\n(default: False)"
  synonyms:
  - --keep-incomplete
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Print the reason for each executed rule. (default:\nFalse)"
  synonyms:
  - --reason
  - -r
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "[PORT]          Serve an HTML based user interface to the given\n\
    network and port e.g. 168.129.10.15:8000. By default\nSnakemake is only available\
    \ in the local network\n(default port: 8000). To make Snakemake listen to all\n\
    ip addresses add the special host address 0.0.0.0 to\nthe url (0.0.0.0:8000).\
    \ This is important if Snakemake\nis used in a virtualised environment like Docker.\
    \ If\npossible, a browser window is opened. (default: None)"
  synonyms:
  - --gui
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Print out the shell commands that will be executed.\n(default: False)"
  synonyms:
  - --printshellcmds
  - -p
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Print candidate and selected jobs (including their\nwildcards) while\
    \ inferring DAG. This can help to debug\nunexpected DAG topology or errors. (default:\
    \ False)"
  synonyms:
  - --debug-dag
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Write stats about Snakefile execution in JSON format\nto the given\
    \ file. (default: None)"
  synonyms:
  - --stats
  args: !SimpleFlagArg
    name: FILE
  optional: true
- !Flag
  description: 'Do not use a colored output. (default: False)'
  synonyms:
  - --nocolor
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Do not output any progress or rule information.\n(default: False)"
  synonyms:
  - --quiet
  - -q
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Print the python representation of the workflow.\n(default: False)"
  synonyms:
  - --print-compilation
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: 'Print debugging output. (default: False)'
  synonyms:
  - --verbose
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Force threads rather than processes. Helpful if shared\nmemory (/dev/shm)\
    \ is full or unavailable. (default:\nFalse)"
  synonyms:
  - --force-use-threads
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Don't check for ambiguous rules and simply use the\nfirst if several\
    \ can produce the same file. This\nallows the user to prioritize rules by their\
    \ order in\nthe snakefile. (default: False)"
  synonyms:
  - --allow-ambiguity
  - -a
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: 'Do not lock the working directory (default: False)'
  synonyms:
  - --nolock
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Do not check for incomplete output files. (default:\nFalse)"
  synonyms:
  - --ignore-incomplete
  - --ii
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Wait given seconds if an output file of a job is not\npresent after\
    \ the job finished. This helps if your\nfilesystem suffers from latency (default\
    \ 5). (default:\n5)"
  synonyms:
  - --latency-wait
  - --output-wait
  - -w
  args: !SimpleFlagArg
    name: SECONDS
  optional: true
- !Flag
  description: "[FILE [FILE ...]]\nWait --latency-wait seconds for these files to\
    \ be\npresent before executing the workflow. This option is\nused internally to\
    \ handle filesystem latency in\ncluster environments. (default: None)"
  synonyms:
  - --wait-for-files
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Ignore temp() declarations. This is useful when\nrunning only a part\
    \ of the workflow, since temp()\nwould lead to deletion of probably needed files\
    \ by\nother parts of the workflow. (default: False)"
  synonyms:
  - --notemp
  - --nt
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Keep local copies of remote input files. (default:\nFalse)"
  synonyms:
  - --keep-remote
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Do not adjust the paths of given target files relative\nto the working\
    \ directory. (default: False)"
  synonyms:
  - --keep-target-files
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Only consider given rules. If omitted, all rules in\nSnakefile are\
    \ used. Note that this is intended\nprimarily for internal use and may lead to\
    \ unexpected\nresults otherwise. (default: None)"
  synonyms:
  - --allowed-rules
  args: !RepeatFlagArg
    name: ALLOWED_RULES
  optional: true
- !Flag
  description: "Maximal number of cluster/drmaa jobs per second,\ndefault is 10, fractions\
    \ allowed. (default: 10)"
  synonyms:
  - --max-jobs-per-second
  args: !SimpleFlagArg
    name: MAX_JOBS_PER_SECOND
  optional: true
- !Flag
  description: "Maximal number of job status checks per second,\ndefault is 10, fractions\
    \ allowed. (default: 10)"
  synonyms:
  - --max-status-checks-per-second
  args: !SimpleFlagArg
    name: MAX_STATUS_CHECKS_PER_SECOND
  optional: true
- !Flag
  description: "Number of times to restart failing jobs (defaults to\n0). (default:\
    \ 0)"
  synonyms:
  - --restart-times
  args: !SimpleFlagArg
    name: RESTART_TIMES
  optional: true
- !Flag
  description: "Internal use only: define the initial value of the\nattempt parameter\
    \ (default: 1). (default: 1)"
  synonyms:
  - --attempt
  args: !SimpleFlagArg
    name: ATTEMPT
  optional: true
- !Flag
  description: "Prefix for URL created from wrapper directive\n(default: https://github.com/snakemake/snakemake-\n\
    wrappers/raw/). Set this to a different URL to use\nyour fork or a local clone\
    \ of the repository, e.g.,\nuse a git URL like\n'git+file://path/to/your/local/clone@'.\
    \ (default:\nhttps://github.com/snakemake/snakemake-wrappers/raw/)"
  synonyms:
  - --wrapper-prefix
  args: !SimpleFlagArg
    name: WRAPPER_PREFIX
  optional: true
- !Flag
  description: "Specify default remote provider to be used for all\ninput and output\
    \ files that don't yet specify one.\n(default: None)"
  synonyms:
  - --default-remote-provider
  args: !ChoiceFlagArg
    choices: !!set
      gridftp:
      S3:
      S3Mocked:
      GS:
      SFTP:
      gfal:
      FTP:
      iRODS:
  optional: true
- !Flag
  description: "Specify prefix for default remote provider. E.g. a\nbucket name. (default:\
    \ )"
  synonyms:
  - --default-remote-prefix
  args: !SimpleFlagArg
    name: DEFAULT_REMOTE_PREFIX
  optional: true
- !Flag
  description: "Do not assume that jobs share a common file system.\nWhen this flag\
    \ is activated, Snakemake will assume\nthat the filesystem on a cluster node is\
    \ not shared\nwith other nodes. For example, this will lead to\ndownloading remote\
    \ files on each cluster node\nseparately. Further, it won't take special measures\
    \ to\ndeal with filesystem latency issues. This option will\nin most cases only\
    \ make sense in combination with\n--default-remote-provider. Further, when using\n\
    --cluster you will have to also provide --cluster-\nstatus. Only activate this\
    \ if you know what you are\ndoing. (default: False)"
  synonyms:
  - --no-shared-fs
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Set the greediness of scheduling. This value between 0\nand 1 determines\
    \ how careful jobs are selected for\nexecution. The default value (1.0) provides\
    \ the best\nspeed and still acceptable scheduling quality.\n(default: None)"
  synonyms:
  - --greediness
  args: !SimpleFlagArg
    name: GREEDINESS
  optional: true
- !Flag
  description: "Do not invoke onstart, onsuccess or onerror hooks\nafter execution.\
    \ (default: False)"
  synonyms:
  - --no-hooks
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Provide a shell command that shall be executed instead\nof those given\
    \ in the workflow. This is for debugging\npurposes only. (default: None)"
  synonyms:
  - --overwrite-shellcmd
  args: !SimpleFlagArg
    name: OVERWRITE_SHELLCMD
  optional: true
- !Flag
  description: "Allow to debug rules with e.g. PDB. This flag allows\nto set breakpoints\
    \ in run blocks. (default: False)"
  synonyms:
  - --debug
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Profile Snakemake and write the output to FILE. This\nrequires yappi\
    \ to be installed. (default: None)"
  synonyms:
  - --runtime-profile
  args: !SimpleFlagArg
    name: FILE
  optional: true
- !Flag
  description: "Set execution mode of Snakemake (internal use only).\n(default: 0)"
  synonyms:
  - --mode
  args: !ChoiceFlagArg
    choices: !!set
      '2':
      '0':
      '1':
  optional: true
- !Flag
  description: "Automatically display logs of failed jobs. (default:\nFalse)"
  synonyms:
  - --show-failed-logs
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Provide a custom script containing a function 'def\nlog_handler(msg):'.\
    \ Snakemake will call this function\nfor every logging output (given as a dictionary\n\
    msg)allowing to e.g. send notifications in the form of\ne.g. slack messages or\
    \ emails. (default: None)"
  synonyms:
  - --log-handler-script
  args: !SimpleFlagArg
    name: FILE
  optional: true
- !Flag
  description: "Set a specific messaging service for logging\noutput.Snakemake will\
    \ notify the service on errors and\ncompleted execution.Currently only slack is\
    \ supported.\n(default: None)"
  synonyms:
  - --log-service
  args: !ChoiceFlagArg
    choices: !!set
      slack:
      none:
  optional: true
- !Flag
  description: "Execute snakemake rules with the given submit command,\ne.g. qsub.\
    \ Snakemake compiles jobs into scripts that\nare submitted to the cluster with\
    \ the given command,\nonce all input files for a particular job are present.\n\
    The submit command can be decorated to make it aware\nof certain job properties\
    \ (name, rulename, input,\noutput, params, wildcards, log, threads and\ndependencies\
    \ (see the argument below)), e.g.: $\nsnakemake --cluster 'qsub -pe threaded {threads}'.\n\
    (default: None)"
  synonyms:
  - --cluster
  - -c
  args: !SimpleFlagArg
    name: CMD
  optional: true
- !Flag
  description: "cluster submission command will block, returning the\nremote exitstatus\
    \ upon remote termination (for\nexample, this should be usedif the cluster command\
    \ is\n'qsub -sync y' (SGE) (default: None)"
  synonyms:
  - --cluster-sync
  args: !SimpleFlagArg
    name: CMD
  optional: true
- !Flag
  description: "[ARGS]        Execute snakemake on a cluster accessed via DRMAA,\n\
    Snakemake compiles jobs into scripts that are\nsubmitted to the cluster with the\
    \ given command, once\nall input files for a particular job are present. ARGS\n\
    can be used to specify options of the underlying\ncluster system, thereby using\
    \ the job properties name,\nrulename, input, output, params, wildcards, log,\n\
    threads and dependencies, e.g.: --drmaa ' -pe threaded\n{threads}'. Note that\
    \ ARGS must be given in quotes and\nwith a leading whitespace. (default: None)"
  synonyms:
  - --drmaa
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "A JSON or YAML file that defines the wildcards used in\n'cluster'for\
    \ specific rules, instead of having them\nspecified in the Snakefile. For example,\
    \ for rule\n'job' you may define: { 'job' : { 'time' : '24:00:00'\n} } to specify\
    \ the time for rule 'job'. You can\nspecify more than one file. The configuration\
    \ files\nare merged with later values overriding earlier ones.\nThis option is\
    \ deprecated in favor of using --profile,\nsee docs. (default: [])"
  synonyms:
  - --cluster-config
  - -u
  args: !SimpleFlagArg
    name: FILE
  optional: true
- !Flag
  description: "Immediately submit all jobs to the cluster instead of\nwaiting for\
    \ present input files. This will fail,\nunless you make the cluster aware of job\
    \ dependencies,\ne.g. via: $ snakemake --cluster 'sbatch --dependency\n{dependencies}.\
    \ Assuming that your submit script (here\nsbatch) outputs the generated job id\
    \ to the first\nstdout line, {dependencies} will be filled with space\nseparated\
    \ job ids this job depends on. (default:\nFalse)"
  synonyms:
  - --immediate-submit
  - --is
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Provide a custom job script for submission to the\ncluster. The default\
    \ script resides as 'jobscript.sh'\nin the installation directory. (default: None)"
  synonyms:
  - --jobscript
  - --js
  args: !SimpleFlagArg
    name: SCRIPT
  optional: true
- !Flag
  description: "Provide a custom name for the jobscript that is\nsubmitted to the\
    \ cluster (see --cluster). NAME is\n\"snakejob.{name}.{jobid}.sh\" per default.\
    \ The wildcard\n{jobid} has to be present in the name. (default:\nsnakejob.{name}.{jobid}.sh)"
  synonyms:
  - --jobname
  - --jn
  args: !SimpleFlagArg
    name: NAME
  optional: true
- !Flag
  description: "Status command for cluster execution. This is only\nconsidered in\
    \ combination with the --cluster flag. If\nprovided, Snakemake will use the status\
    \ command to\ndetermine if a job has finished successfully or\nfailed. For this\
    \ it is necessary that the submit\ncommand provided to --cluster returns the cluster\
    \ job\nid. Then, the status command will be invoked with the\njob id. Snakemake\
    \ expects it to return 'success' if\nthe job was successfull, 'failed' if the\
    \ job failed\nand 'running' if the job still runs. (default: None)"
  synonyms:
  - --cluster-status
  args: !SimpleFlagArg
    name: CLUSTER_STATUS
  optional: true
- !Flag
  description: "Specify a directory in which stdout and stderr files\nof DRMAA jobs\
    \ will be written. The value may be given\nas a relative path, in which case Snakemake\
    \ will use\nthe current invocation directory as the origin. If\ngiven, this will\
    \ override any given '-o' and/or '-e'\nnative specification. If not given, all\
    \ DRMAA stdout\nand stderr files are written to the current working\ndirectory.\
    \ (default: None)"
  synonyms:
  - --drmaa-log-dir
  args: !SimpleFlagArg
    name: DIR
  optional: true
- !Flag
  description: "[NAMESPACE]\nExecute workflow in a kubernetes cluster (in the\ncloud).\
    \ NAMESPACE is the namespace you want to use for\nyour job (if nothing specified:\
    \ 'default'). Usually,\nthis requires --default-remote-provider and --default-\n\
    remote-prefix to be set to a S3 or GS bucket where\nyour . data shall be stored.\
    \ It is further advisable\nto activate conda integration via --use-conda.\n(default:\
    \ None)"
  synonyms:
  - --kubernetes
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Docker image to use, e.g., when submitting jobs to\nkubernetes Defaults\
    \ to\n'https://hub.docker.com/r/snakemake/snakemake', tagged\nwith the same version\
    \ as the currently running\nSnakemake instance. Note that overwriting this value\n\
    is up to your responsibility. Any used image has to\ncontain a working snakemake\
    \ installation that is\ncompatible with (or ideally the same as) the currently\n\
    running version. (default: None)"
  synonyms:
  - --container-image
  args: !SimpleFlagArg
    name: IMAGE
  optional: true
- !Flag
  description: "Execute workflow on AWS cloud using Tibanna. This\nrequires --default-remote-prefix\
    \ to be set to S3\nbucket name and prefix (e.g.\n'bucketname/subdirectory') where\
    \ input is already\nstored and output will be sent to. Using --tibanna\nimplies\
    \ --default-resources is set as default.\nOptionally, use --precommand to specify\
    \ any\npreparation command to run before snakemake command on\nthe cloud (inside\
    \ snakemake container on Tibanna VM).\nAlso, --use-conda, --use-singularity, --config,\n\
    --configfile are supported and will be carried over.\n(default: False)"
  synonyms:
  - --tibanna
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Name of Tibanna Unicorn step function (e.g.\ntibanna_unicorn_monty).This\
    \ works as serverless\nscheduler/resource allocator and must be deployed\nfirst\
    \ using tibanna cli. (e.g. tibanna deploy_unicorn\n--usergroup=monty --buckets=bucketname)\
    \ (default:\nNone)"
  synonyms:
  - --tibanna-sfn
  args: !SimpleFlagArg
    name: TIBANNA_SFN
  optional: true
- !Flag
  description: "Any command to execute before snakemake command on AWS\ncloud such\
    \ as wget, git clone, unzip, etc. This is\nused with --tibanna.Do not include\
    \ input/output\ndownload/upload commands - file transfer between S3\nbucket and\
    \ the run environment (container) is\nautomatically handled by Tibanna. (default:\
    \ None)"
  synonyms:
  - --precommand
  args: !SimpleFlagArg
    name: PRECOMMAND
  optional: true
- !Flag
  description: "Additional tibanan config e.g. --tibanna-config\nspot_instance=true\
    \ subnet=<subnet_id> security\ngroup=<security_group_id> (default: None)"
  synonyms:
  - --tibanna-config
  args: !RepeatFlagArg
    name: TIBANNA_CONFIG
  optional: true
- !Flag
  description: "Execute workflow on Google Cloud cloud using the\nGoogle Life. Science\
    \ API. This requires default\napplication credentials (json) to be created and\n\
    export to the environment to use Google Cloud Storage,\nCompute Engine, and Life\
    \ Sciences. The credential file\nshould be exported as GOOGLE_APPLICATION_CREDENTIALS\n\
    for snakemake to discover. Also, --use-conda, --use-\nsingularity, --config, --configfile\
    \ are supported and\nwill be carried over. (default: False)"
  synonyms:
  - --google-lifesciences
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Specify one or more valid instance regions (defaults\nto US) (default:\
    \ ['us-east1', 'us-west1', 'us-\ncentral1'])"
  synonyms:
  - --google-lifesciences-regions
  args: !RepeatFlagArg
    name: GOOGLE_LIFESCIENCES_REGIONS
  optional: true
- !Flag
  description: "The Life Sciences API service used to schedule the\njobs. E.g., us-centra1\
    \ (Iowa) and europe-west2\n(London) Watch the terminal output to see all options\n\
    found to be available. If not specified, defaults to\nthe first found with a matching\
    \ prefix from regions\nspecified with --google-lifesciences-regions.\n(default:\
    \ None)"
  synonyms:
  - --google-lifesciences-location
  args: !SimpleFlagArg
    name: GOOGLE_LIFESCIENCES_LOCATION
  optional: true
- !Flag
  description: "Cache workflows in your Google Cloud Storage Bucket\nspecified by\
    \ --default-remote-prefix/{source}/{cache}.\nEach workflow working directory is\
    \ compressed to a\n.tar.gz, named by the hash of the contents, and kept\nin Google\
    \ Cloud Storage. By default, the caches are\ndeleted at the shutdown step of the\
    \ workflow.\n(default: False)"
  synonyms:
  - --google-lifesciences-keep-cache
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "If defined in the rule, run job in a conda\nenvironment. If this flag\
    \ is not set, the conda\ndirective is ignored. (default: False)"
  synonyms:
  - --use-conda
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "List all conda environments and their location on\ndisk. (default:\
    \ False)"
  synonyms:
  - --list-conda-envs
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Specify a directory in which the 'conda' and 'conda-\narchive' directories\
    \ are created. These are used to\nstore conda environments and their archives,\n\
    respectively. If not supplied, the value is set to the\n'.snakemake' directory\
    \ relative to the invocation\ndirectory. If supplied, the `--use-conda` flag must\n\
    also be set. The value may be given as a relative\npath, which will be extrapolated\
    \ to the invocation\ndirectory, or as an absolute path. (default: None)"
  synonyms:
  - --conda-prefix
  args: !SimpleFlagArg
    name: DIR
  optional: true
- !Flag
  description: 'Cleanup unused conda environments. (default: False)'
  synonyms:
  - --conda-cleanup-envs
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "[{tarballs,cache}]\nCleanup conda packages after creating environments.\
    \ In\ncase of 'tarballs' mode, will clean up all downloaded\npackage tarballs.\
    \ In case of 'cache' mode, will\nadditionally clean up unused package caches.\
    \ If mode\nis omitted, will default to only cleaning up the\ntarballs. (default:\
    \ None)"
  synonyms:
  - --conda-cleanup-pkgs
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "If specified, only creates the job-specific conda\nenvironments then\
    \ exits. The `--use-conda` flag must\nalso be set. (default: False)"
  synonyms:
  - --conda-create-envs-only
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Choose the conda frontend for installing environments.\nCaution: mamba\
    \ is much faster, but still in beta test.\n(default: conda)"
  synonyms:
  - --conda-frontend
  args: !ChoiceFlagArg
    choices: !!set
      mamba:
      conda:
  optional: true
- !Flag
  description: "If defined in the rule, run job within a singularity\ncontainer. If\
    \ this flag is not set, the singularity\ndirective is ignored. (default: False)"
  synonyms:
  - --use-singularity
  args: !EmptyFlagArg {}
  optional: true
- !Flag
  description: "Specify a directory in which singularity images will\nbe stored.If\
    \ not supplied, the value is set to the\n'.snakemake' directory relative to the\
    \ invocation\ndirectory. If supplied, the `--use-singularity` flag\nmust also\
    \ be set. The value may be given as a relative\npath, which will be extrapolated\
    \ to the invocation\ndirectory, or as an absolute path. (default: None)"
  synonyms:
  - --singularity-prefix
  args: !SimpleFlagArg
    name: DIR
  optional: true
- !Flag
  description: 'Pass additional args to singularity. (default: )'
  synonyms:
  - --singularity-args
  args: !SimpleFlagArg
    name: ARGS
  optional: true
- !Flag
  description: "If defined in the rule, run job within the given\nenvironment modules,\
    \ loaded in the given order. This\ncan be combined with --use-conda and --use-\n\
    singularity, which will then be only used as a\nfallback for rules which don't\
    \ define environment\nmodules. (default: False)\n"
  synonyms:
  - --use-envmodules
  args: !EmptyFlagArg {}
  optional: true
parent:
subcommands: []
usage: []
help_flag: !Flag
  description: show this help message and exit
  synonyms:
  - -h
  - --help
  args: !EmptyFlagArg {}
  optional: true
usage_flag:
version_flag: !Flag
  description: show program's version number and exit
  synonyms:
  - --version
  - -v
  args: !EmptyFlagArg {}
  optional: true
help_text: "usage: snakemake [-h] [--dry-run] [--profile PROFILE]\n              \
  \   [--cache [RULE [RULE ...]]] [--snakefile FILE] [--cores [N]]\n             \
  \    [--local-cores N] [--resources [NAME=INT [NAME=INT ...]]]\n               \
  \  [--set-threads [RULE=THREADS [RULE=THREADS ...]]]\n                 [--default-resources\
  \ [NAME=INT [NAME=INT ...]]]\n                 [--config [KEY=VALUE [KEY=VALUE ...]]]\n\
  \                 [--configfile FILE [FILE ...]]\n                 [--envvars VARNAME\
  \ [VARNAME ...]] [--directory DIR] [--touch]\n                 [--keep-going] [--force]\
  \ [--forceall]\n                 [--forcerun [TARGET [TARGET ...]]]\n          \
  \       [--prioritize TARGET [TARGET ...]]\n                 [--batch RULE=BATCH/BATCHES]\
  \ [--until TARGET [TARGET ...]]\n                 [--omit-from TARGET [TARGET ...]]\
  \ [--rerun-incomplete]\n                 [--shadow-prefix DIR] [--report [FILE]]\n\
  \                 [--report-stylesheet CSSFILE] [--edit-notebook TARGET]\n     \
  \            [--notebook-listen IP:PORT] [--lint [{text,json}]]\n              \
  \   [--export-cwl FILE] [--list] [--list-target-rules] [--dag]\n               \
  \  [--rulegraph] [--filegraph] [--d3dag] [--summary]\n                 [--detailed-summary]\
  \ [--archive FILE]\n                 [--cleanup-metadata FILE [FILE ...]] [--cleanup-shadow]\n\
  \                 [--skip-script-cleanup] [--unlock] [--list-version-changes]\n\
  \                 [--list-code-changes] [--list-input-changes]\n               \
  \  [--list-params-changes] [--list-untracked]\n                 [--delete-all-output]\
  \ [--delete-temp-output]\n                 [--bash-completion] [--keep-incomplete]\
  \ [--version]\n                 [--reason] [--gui [PORT]] [--printshellcmds] [--debug-dag]\n\
  \                 [--stats FILE] [--nocolor] [--quiet] [--print-compilation]\n \
  \                [--verbose] [--force-use-threads] [--allow-ambiguity]\n       \
  \          [--nolock] [--ignore-incomplete] [--latency-wait SECONDS]\n         \
  \        [--wait-for-files [FILE [FILE ...]]] [--notemp]\n                 [--keep-remote]\
  \ [--keep-target-files]\n                 [--allowed-rules ALLOWED_RULES [ALLOWED_RULES\
  \ ...]]\n                 [--max-jobs-per-second MAX_JOBS_PER_SECOND]\n        \
  \         [--max-status-checks-per-second MAX_STATUS_CHECKS_PER_SECOND]\n      \
  \           [--restart-times RESTART_TIMES] [--attempt ATTEMPT]\n              \
  \   [--wrapper-prefix WRAPPER_PREFIX]\n                 [--default-remote-provider\
  \ {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp,iRODS}]\n                 [--default-remote-prefix\
  \ DEFAULT_REMOTE_PREFIX]\n                 [--no-shared-fs] [--greediness GREEDINESS]\
  \ [--no-hooks]\n                 [--overwrite-shellcmd OVERWRITE_SHELLCMD] [--debug]\n\
  \                 [--runtime-profile FILE] [--mode {0,1,2}]\n                 [--show-failed-logs]\
  \ [--log-handler-script FILE]\n                 [--log-service {none,slack}]\n \
  \                [--cluster CMD | --cluster-sync CMD | --drmaa [ARGS]]\n       \
  \          [--cluster-config FILE] [--immediate-submit]\n                 [--jobscript\
  \ SCRIPT] [--jobname NAME]\n                 [--cluster-status CLUSTER_STATUS] [--drmaa-log-dir\
  \ DIR]\n                 [--kubernetes [NAMESPACE]] [--container-image IMAGE]\n\
  \                 [--tibanna] [--tibanna-sfn TIBANNA_SFN]\n                 [--precommand\
  \ PRECOMMAND]\n                 [--tibanna-config TIBANNA_CONFIG [TIBANNA_CONFIG\
  \ ...]]\n                 [--google-lifesciences]\n                 [--google-lifesciences-regions\
  \ GOOGLE_LIFESCIENCES_REGIONS [GOOGLE_LIFESCIENCES_REGIONS ...]]\n             \
  \    [--google-lifesciences-location GOOGLE_LIFESCIENCES_LOCATION]\n           \
  \      [--google-lifesciences-keep-cache] [--use-conda]\n                 [--list-conda-envs]\
  \ [--conda-prefix DIR]\n                 [--conda-cleanup-envs]\n              \
  \   [--conda-cleanup-pkgs [{tarballs,cache}]]\n                 [--conda-create-envs-only]\
  \ [--conda-frontend {conda,mamba}]\n                 [--use-singularity] [--singularity-prefix\
  \ DIR]\n                 [--singularity-args ARGS] [--use-envmodules]\n        \
  \         [target [target ...]]\n\nSnakemake is a Python based language and execution\
  \ environment for GNU Make-\nlike workflows.\n\noptional arguments:\n  -h, --help\
  \            show this help message and exit\n\nEXECUTION:\n  target           \
  \     Targets to build. May be rules or files. (default:\n                     \
  \   None)\n  --dry-run, --dryrun, -n\n                        Do not execute anything,\
  \ and display what would be\n                        done. If you have a very large\
  \ workflow, use --dry-run\n                        --quiet to just print a summary\
  \ of the DAG of jobs.\n                        (default: False)\n  --profile PROFILE\
  \     Name of profile to use for configuring Snakemake.\n                      \
  \  Snakemake will search for a corresponding folder in\n                       \
  \ /etc/xdg/snakemake and /root/.config/snakemake.\n                        Alternatively,\
  \ this can be an absolute or relative\n                        path. The profile\
  \ folder has to contain a file\n                        'config.yaml'. This file\
  \ can be used to set default\n                        values for command line options\
  \ in YAML format. For\n                        example, '--cluster qsub' becomes\
  \ 'cluster: qsub' in\n                        the YAML file. Profiles can be obtained\
  \ from\n                        https://github.com/snakemake-profiles. (default:\
  \ None)\n  --cache [RULE [RULE ...]]\n                        Store output files\
  \ of given rules in a central cache\n                        given by the environment\
  \ variable\n                        $SNAKEMAKE_OUTPUT_CACHE. Likewise, retrieve\
  \ output\n                        files of the given rules from this cache if they\
  \ have\n                        been created before (by anybody writing to the same\n\
  \                        cache), instead of actually executing the rules.\n    \
  \                    Output files are identified by hashing all steps,\n       \
  \                 parameters and software stack (conda envs or\n               \
  \         containers) needed to create them. (default: None)\n  --snakefile FILE,\
  \ -s FILE\n                        The workflow definition in form of a\n      \
  \                  snakefile.Usually, you should not need to specify\n         \
  \               this. By default, Snakemake will search for\n                  \
  \      'Snakefile', 'snakefile', 'workflow/Snakefile',\n                       \
  \ 'workflow/snakefile' beneath the current working\n                        directory,\
  \ in this order. Only if you definitely want\n                        a different\
  \ layout, you need to use this parameter.\n                        (default: None)\n\
  \  --cores [N], --jobs [N], -j [N]\n                        Use at most N CPU cores/jobs\
  \ in parallel. If N is\n                        omitted or 'all', the limit is set\
  \ to the number of\n                        available CPU cores. (default: None)\n\
  \  --local-cores N       In cluster mode, use at most N cores of the host\n    \
  \                    machine in parallel (default: number of CPU cores of\n    \
  \                    the host). The cores are used to execute local rules.\n   \
  \                     This option is ignored when not in cluster mode.\n       \
  \                 (default: 8)\n  --resources [NAME=INT [NAME=INT ...]], --res [NAME=INT\
  \ [NAME=INT ...]]\n                        Define additional resources that shall\
  \ constrain the\n                        scheduling analogously to threads (see\
  \ above). A\n                        resource is defined as a name and an integer\
  \ value.\n                        E.g. --resources mem_mb=1000. Rules can use resources\n\
  \                        by defining the resource keyword, e.g. resources:\n   \
  \                     mem_mb=600. If now two rules require 600 of the\n        \
  \                resource 'mem_mb' they won't be run in parallel by the\n      \
  \                  scheduler. (default: None)\n  --set-threads [RULE=THREADS [RULE=THREADS\
  \ ...]]\n                        Overwrite thread usage of rules. This allows to\
  \ fine-\n                        tune workflow parallelization. In particular, this\
  \ is\n                        helpful to target certain cluster nodes by e.g.\n\
  \                        shifting a rule to use more, or less threads than\n   \
  \                     defined in the workflow. Thereby, THREADS has to be a\n  \
  \                      positive integer, and RULE has to be the name of the\n  \
  \                      rule. (default: None)\n  --default-resources [NAME=INT [NAME=INT\
  \ ...]], --default-res [NAME=INT [NAME=INT ...]]\n                        Define\
  \ default values of resources for rules that do\n                        not define\
  \ their own values. In addition to plain\n                        integers, python\
  \ expressions over inputsize are\n                        allowed (e.g. '2*input.size_mb').When\
  \ specifying this\n                        without any arguments (--default-resources),\
  \ it\n                        defines 'mem_mb=max(2*input.size_mb, 1000)'\n    \
  \                    'disk_mb=max(2*input.size_mb, 1000)', i.e., default\n     \
  \                   disk and mem usage is twice the input file size but at\n   \
  \                     least 1GB. (default: None)\n  --config [KEY=VALUE [KEY=VALUE\
  \ ...]], -C [KEY=VALUE [KEY=VALUE ...]]\n                        Set or overwrite\
  \ values in the workflow config object.\n                        The workflow config\
  \ object is accessible as variable\n                        config inside the workflow.\
  \ Default values can be set\n                        by providing a JSON file (see\
  \ Documentation).\n                        (default: None)\n  --configfile FILE\
  \ [FILE ...], --configfiles FILE [FILE ...]\n                        Specify or\
  \ overwrite the config file of the workflow\n                        (see the docs).\
  \ Values specified in JSON or YAML\n                        format are available\
  \ in the global config dictionary\n                        inside the workflow.\
  \ Multiple files overwrite each\n                        other in the given order.\
  \ (default: None)\n  --envvars VARNAME [VARNAME ...]\n                        Environment\
  \ variables to pass to cloud jobs. (default:\n                        None)\n  --directory\
  \ DIR, -d DIR\n                        Specify working directory (relative paths\
  \ in the\n                        snakefile will use this as their origin). (default:\n\
  \                        None)\n  --touch, -t           Touch output files (mark\
  \ them up to date without\n                        really changing them) instead\
  \ of running their\n                        commands. This is used to pretend that\
  \ the rules were\n                        executed, in order to fool future invocations\
  \ of\n                        snakemake. Fails if a file does not yet exist. Note\n\
  \                        that this will only touch files that would otherwise\n\
  \                        be recreated by Snakemake (e.g. because their input\n \
  \                       files are newer). For enforcing a touch, combine this\n\
  \                        with --force, --forceall, or --forcerun. Note however\n\
  \                        that you loose the provenance information when the\n  \
  \                      files have been created in realitiy. Hence, this\n      \
  \                  should be used only as a last resort. (default: False)\n  --keep-going,\
  \ -k      Go on with independent jobs if a job fails. (default:\n              \
  \          False)\n  --force, -f           Force the execution of the selected target\
  \ or the\n                        first rule regardless of already created output.\n\
  \                        (default: False)\n  --forceall, -F        Force the execution\
  \ of the selected (or the first)\n                        rule and all rules it\
  \ is dependent on regardless of\n                        already created output.\
  \ (default: False)\n  --forcerun [TARGET [TARGET ...]], -R [TARGET [TARGET ...]]\n\
  \                        Force the re-execution or creation of the given rules\n\
  \                        or files. Use this option if you changed a rule and\n \
  \                       want to have all its output in your workflow updated.\n\
  \                        (default: None)\n  --prioritize TARGET [TARGET ...], -P\
  \ TARGET [TARGET ...]\n                        Tell the scheduler to assign creation\
  \ of given targets\n                        (and all their dependencies) highest\
  \ priority.\n                        (EXPERIMENTAL) (default: None)\n  --batch RULE=BATCH/BATCHES\n\
  \                        Only create the given BATCH of the input files of the\n\
  \                        given RULE. This can be used to iteratively run parts\n\
  \                        of very large workflows. Only the execution plan of\n \
  \                       the relevant part of the workflow has to be\n          \
  \              calculated, thereby speeding up DAG computation. It is\n        \
  \                recommended to provide the most suitable rule for\n           \
  \             batching when documenting a workflow. It should be\n             \
  \           some aggregating rule that would be executed only\n                \
  \        once, and has a large number of input files. For\n                    \
  \    example, it can be a rule that aggregates over\n                        samples.\
  \ (default: None)\n  --until TARGET [TARGET ...], -U TARGET [TARGET ...]\n     \
  \                   Runs the pipeline until it reaches the specified rules\n   \
  \                     or files. Only runs jobs that are dependencies of the\n  \
  \                      specified rule or files, does not run sibling DAGs.\n   \
  \                     (default: None)\n  --omit-from TARGET [TARGET ...], -O TARGET\
  \ [TARGET ...]\n                        Prevent the execution or creation of the\
  \ given rules\n                        or files as well as any rules or files that\
  \ are\n                        downstream of these targets in the DAG. Also runs\
  \ jobs\n                        in sibling DAGs that are independent of the rules\
  \ or\n                        files specified here. (default: None)\n  --rerun-incomplete,\
  \ --ri\n                        Re-run all jobs the output of which is recognized\
  \ as\n                        incomplete. (default: False)\n  --shadow-prefix DIR\
  \   Specify a directory in which the 'shadow' directory is\n                   \
  \     created. If not supplied, the value is set to the\n                      \
  \  '.snakemake' directory relative to the working\n                        directory.\
  \ (default: None)\n\nREPORTS:\n  --report [FILE]       Create an HTML report with\
  \ results and statistics.\n                        This can be either a .html file\
  \ or a .zip file. In the\n                        former case, all results are embedded\
  \ into the .html\n                        (this only works for small data). In the\
  \ latter case,\n                        results are stored along with a file report.html\
  \ in\n                        the zip archive. If no filename is given, an embedded\n\
  \                        report.html is the default. (default: None)\n  --report-stylesheet\
  \ CSSFILE\n                        Custom stylesheet to use for report. In particular,\n\
  \                        this can be used for branding the report with e.g. a\n\
  \                        custom logo, see docs. (default: None)\n\nNOTEBOOKS:\n\
  \  --edit-notebook TARGET\n                        Interactively edit the notebook\
  \ associated with the\n                        rule used to generate the given target\
  \ file. This will\n                        start a local jupyter notebook server.\
  \ Any changes to\n                        the notebook should be saved, and the\
  \ server has to be\n                        stopped by closing the notebook and\
  \ hitting the 'Quit'\n                        button on the jupyter dashboard. Afterwards,\
  \ the\n                        updated notebook will be automatically stored in\
  \ the\n                        path defined in the rule. If the notebook is not\
  \ yet\n                        present, this will create an empty draft. (default:\n\
  \                        None)\n  --notebook-listen IP:PORT\n                  \
  \      The IP address and PORT the notebook server used for\n                  \
  \      editing the notebook (--edit-notebook) will listen on.\n                \
  \        (default: localhost:8888)\n\nUTILITIES:\n  --lint [{text,json}]  Perform\
  \ linting on the given workflow. This will print\n                        snakemake\
  \ specific suggestions to improve code quality\n                        (work in\
  \ progress, more lints to be added in the\n                        future). If no\
  \ argument is provided, plain text output\n                        is used. (default:\
  \ None)\n  --export-cwl FILE     Compile workflow to CWL and store it in given FILE.\n\
  \                        (default: None)\n  --list, -l            Show available\
  \ rules in given Snakefile. (default:\n                        False)\n  --list-target-rules,\
  \ --lt\n                        Show available target rules in given Snakefile.\n\
  \                        (default: False)\n  --dag                 Do not execute\
  \ anything and print the directed acyclic\n                        graph of jobs\
  \ in the dot language. Recommended use on\n                        Unix systems:\
  \ snakemake --dag | dot | displayNote\n                        print statements\
  \ in your Snakefile may interfere with\n                        visualization. (default:\
  \ False)\n  --rulegraph           Do not execute anything and print the dependency\
  \ graph\n                        of rules in the dot language. This will be less\n\
  \                        crowded than above DAG of jobs, but also show less\n  \
  \                      information. Note that each rule is displayed once,\n   \
  \                     hence the displayed graph will be cyclic if a rule\n     \
  \                   appears in several steps of the workflow. Use this if\n    \
  \                    above option leads to a DAG that is too large.\n          \
  \              Recommended use on Unix systems: snakemake --rulegraph\n        \
  \                | dot | displayNote print statements in your Snakefile\n      \
  \                  may interfere with visualization. (default: False)\n  --filegraph\
  \           Do not execute anything and print the dependency graph\n           \
  \             of rules with their input and output files in the dot\n          \
  \              language. This is an intermediate solution between\n            \
  \            above DAG of jobs and the rule graph. Note that each\n            \
  \            rule is displayed once, hence the displayed graph will\n          \
  \              be cyclic if a rule appears in several steps of the\n           \
  \             workflow. Use this if above option leads to a DAG that\n         \
  \               is too large. Recommended use on Unix systems:\n               \
  \         snakemake --filegraph | dot | displayNote print\n                    \
  \    statements in your Snakefile may interfere with\n                        visualization.\
  \ (default: False)\n  --d3dag               Print the DAG in D3.js compatible JSON\
  \ format.\n                        (default: False)\n  --summary, -S         Print\
  \ a summary of all files created by the workflow.\n                        The has\
  \ the following columns: filename, modification\n                        time, rule\
  \ version, status, plan. Thereby rule version\n                        contains\
  \ the versionthe file was created with (see the\n                        version\
  \ keyword of rules), and status denotes whether\n                        the file\
  \ is missing, its input files are newer or if\n                        version or\
  \ implementation of the rule changed since\n                        file creation.\
  \ Finally the last column denotes whether\n                        the file will\
  \ be updated or created during the next\n                        workflow execution.\
  \ (default: False)\n  --detailed-summary, -D\n                        Print a summary\
  \ of all files created by the workflow.\n                        The has the following\
  \ columns: filename, modification\n                        time, rule version, input\
  \ file(s), shell command,\n                        status, plan. Thereby rule version\
  \ contains the\n                        version the file was created with (see the\
  \ version\n                        keyword of rules), and status denotes whether\
  \ the file\n                        is missing, its input files are newer or if\
  \ version or\n                        implementation of the rule changed since file\n\
  \                        creation. The input file and shell command columns are\n\
  \                        self explanatory. Finally the last column denotes\n   \
  \                     whether the file will be updated or created during the\n \
  \                       next workflow execution. (default: False)\n  --archive FILE\
  \        Archive the workflow into the given tar archive FILE.\n               \
  \         The archive will be created such that the workflow can\n             \
  \           be re-executed on a vanilla system. The function needs\n           \
  \             conda and git to be installed. It will archive every\n           \
  \             file that is under git version control. Note that it\n           \
  \             is best practice to have the Snakefile, config files,\n          \
  \              and scripts under version control. Hence, they will be\n        \
  \                included in the archive. Further, it will add input\n         \
  \               files that are not generated by by the workflow itself\n       \
  \                 and conda environments. Note that symlinks are\n             \
  \           dereferenced. Supported formats are .tar, .tar.gz,\n               \
  \         .tar.bz2 and .tar.xz. (default: None)\n  --cleanup-metadata FILE [FILE\
  \ ...], --cm FILE [FILE ...]\n                        Cleanup the metadata of given\
  \ files. That means that\n                        snakemake removes any tracked\
  \ version info, and any\n                        marks that files are incomplete.\
  \ (default: None)\n  --cleanup-shadow      Cleanup old shadow directories which\
  \ have not been\n                        deleted due to failures or power loss.\
  \ (default:\n                        False)\n  --skip-script-cleanup\n         \
  \               Don't delete wrapper scripts used for execution\n              \
  \          (default: False)\n  --unlock              Remove a lock on the working\
  \ directory. (default:\n                        False)\n  --list-version-changes,\
  \ --lv\n                        List all output files that have been created with\
  \ a\n                        different version (as determined by the version\n \
  \                       keyword). (default: False)\n  --list-code-changes, --lc\n\
  \                        List all output files for which the rule body (run or\n\
  \                        shell) have changed in the Snakefile. (default: False)\n\
  \  --list-input-changes, --li\n                        List all output files for\
  \ which the defined input\n                        files have changed in the Snakefile\
  \ (e.g. new input\n                        files were added in the rule definition\
  \ or files were\n                        renamed). For listing input file modification\
  \ in the\n                        filesystem, use --summary. (default: False)\n\
  \  --list-params-changes, --lp\n                        List all output files for\
  \ which the defined params\n                        have changed in the Snakefile.\
  \ (default: False)\n  --list-untracked, --lu\n                        List all files\
  \ in the working directory that are not\n                        used in the workflow.\
  \ This can be used e.g. for\n                        identifying leftover files.\
  \ Hidden files and\n                        directories are ignored. (default: False)\n\
  \  --delete-all-output   Remove all files generated by the workflow. Use\n     \
  \                   together with --dry-run to list files without actually\n   \
  \                     deleting anything. Note that this will not recurse\n     \
  \                   into subworkflows. Write-protected files are not\n         \
  \               removed. Nevertheless, use with care! (default: False)\n  --delete-temp-output\
  \  Remove all temporary files generated by the workflow.\n                     \
  \   Use together with --dry-run to list files without\n                        actually\
  \ deleting anything. Note that this will not\n                        recurse into\
  \ subworkflows. (default: False)\n  --bash-completion     Output code to register\
  \ bash completion for snakemake.\n                        Put the following in your\
  \ .bashrc (including the\n                        accents): `snakemake --bash-completion`\
  \ or issue it in\n                        an open terminal session. (default: False)\n\
  \  --keep-incomplete     Do not remove incomplete output files by failed jobs.\n\
  \                        (default: False)\n  --version, -v         show program's\
  \ version number and exit\n\nOUTPUT:\n  --reason, -r          Print the reason for\
  \ each executed rule. (default:\n                        False)\n  --gui [PORT]\
  \          Serve an HTML based user interface to the given\n                   \
  \     network and port e.g. 168.129.10.15:8000. By default\n                   \
  \     Snakemake is only available in the local network\n                       \
  \ (default port: 8000). To make Snakemake listen to all\n                      \
  \  ip addresses add the special host address 0.0.0.0 to\n                      \
  \  the url (0.0.0.0:8000). This is important if Snakemake\n                    \
  \    is used in a virtualised environment like Docker. If\n                    \
  \    possible, a browser window is opened. (default: None)\n  --printshellcmds,\
  \ -p  Print out the shell commands that will be executed.\n                    \
  \    (default: False)\n  --debug-dag           Print candidate and selected jobs\
  \ (including their\n                        wildcards) while inferring DAG. This\
  \ can help to debug\n                        unexpected DAG topology or errors.\
  \ (default: False)\n  --stats FILE          Write stats about Snakefile execution\
  \ in JSON format\n                        to the given file. (default: None)\n \
  \ --nocolor             Do not use a colored output. (default: False)\n  --quiet,\
  \ -q           Do not output any progress or rule information.\n               \
  \         (default: False)\n  --print-compilation   Print the python representation\
  \ of the workflow.\n                        (default: False)\n  --verbose      \
  \       Print debugging output. (default: False)\n\nBEHAVIOR:\n  --force-use-threads\
  \   Force threads rather than processes. Helpful if shared\n                   \
  \     memory (/dev/shm) is full or unavailable. (default:\n                    \
  \    False)\n  --allow-ambiguity, -a\n                        Don't check for ambiguous\
  \ rules and simply use the\n                        first if several can produce\
  \ the same file. This\n                        allows the user to prioritize rules\
  \ by their order in\n                        the snakefile. (default: False)\n \
  \ --nolock              Do not lock the working directory (default: False)\n  --ignore-incomplete,\
  \ --ii\n                        Do not check for incomplete output files. (default:\n\
  \                        False)\n  --latency-wait SECONDS, --output-wait SECONDS,\
  \ -w SECONDS\n                        Wait given seconds if an output file of a\
  \ job is not\n                        present after the job finished. This helps\
  \ if your\n                        filesystem suffers from latency (default 5).\
  \ (default:\n                        5)\n  --wait-for-files [FILE [FILE ...]]\n\
  \                        Wait --latency-wait seconds for these files to be\n   \
  \                     present before executing the workflow. This option is\n  \
  \                      used internally to handle filesystem latency in\n       \
  \                 cluster environments. (default: None)\n  --notemp, --nt      \
  \  Ignore temp() declarations. This is useful when\n                        running\
  \ only a part of the workflow, since temp()\n                        would lead\
  \ to deletion of probably needed files by\n                        other parts of\
  \ the workflow. (default: False)\n  --keep-remote         Keep local copies of remote\
  \ input files. (default:\n                        False)\n  --keep-target-files\
  \   Do not adjust the paths of given target files relative\n                   \
  \     to the working directory. (default: False)\n  --allowed-rules ALLOWED_RULES\
  \ [ALLOWED_RULES ...]\n                        Only consider given rules. If omitted,\
  \ all rules in\n                        Snakefile are used. Note that this is intended\n\
  \                        primarily for internal use and may lead to unexpected\n\
  \                        results otherwise. (default: None)\n  --max-jobs-per-second\
  \ MAX_JOBS_PER_SECOND\n                        Maximal number of cluster/drmaa jobs\
  \ per second,\n                        default is 10, fractions allowed. (default:\
  \ 10)\n  --max-status-checks-per-second MAX_STATUS_CHECKS_PER_SECOND\n         \
  \               Maximal number of job status checks per second,\n              \
  \          default is 10, fractions allowed. (default: 10)\n  --restart-times RESTART_TIMES\n\
  \                        Number of times to restart failing jobs (defaults to\n\
  \                        0). (default: 0)\n  --attempt ATTEMPT     Internal use\
  \ only: define the initial value of the\n                        attempt parameter\
  \ (default: 1). (default: 1)\n  --wrapper-prefix WRAPPER_PREFIX\n              \
  \          Prefix for URL created from wrapper directive\n                     \
  \   (default: https://github.com/snakemake/snakemake-\n                        wrappers/raw/).\
  \ Set this to a different URL to use\n                        your fork or a local\
  \ clone of the repository, e.g.,\n                        use a git URL like\n \
  \                       'git+file://path/to/your/local/clone@'. (default:\n    \
  \                    https://github.com/snakemake/snakemake-wrappers/raw/)\n  --default-remote-provider\
  \ {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp,iRODS}\n                        Specify\
  \ default remote provider to be used for all\n                        input and\
  \ output files that don't yet specify one.\n                        (default: None)\n\
  \  --default-remote-prefix DEFAULT_REMOTE_PREFIX\n                        Specify\
  \ prefix for default remote provider. E.g. a\n                        bucket name.\
  \ (default: )\n  --no-shared-fs        Do not assume that jobs share a common file\
  \ system.\n                        When this flag is activated, Snakemake will assume\n\
  \                        that the filesystem on a cluster node is not shared\n \
  \                       with other nodes. For example, this will lead to\n     \
  \                   downloading remote files on each cluster node\n            \
  \            separately. Further, it won't take special measures to\n          \
  \              deal with filesystem latency issues. This option will\n         \
  \               in most cases only make sense in combination with\n            \
  \            --default-remote-provider. Further, when using\n                  \
  \      --cluster you will have to also provide --cluster-\n                    \
  \    status. Only activate this if you know what you are\n                     \
  \   doing. (default: False)\n  --greediness GREEDINESS\n                       \
  \ Set the greediness of scheduling. This value between 0\n                     \
  \   and 1 determines how careful jobs are selected for\n                       \
  \ execution. The default value (1.0) provides the best\n                       \
  \ speed and still acceptable scheduling quality.\n                        (default:\
  \ None)\n  --no-hooks            Do not invoke onstart, onsuccess or onerror hooks\n\
  \                        after execution. (default: False)\n  --overwrite-shellcmd\
  \ OVERWRITE_SHELLCMD\n                        Provide a shell command that shall\
  \ be executed instead\n                        of those given in the workflow. This\
  \ is for debugging\n                        purposes only. (default: None)\n  --debug\
  \               Allow to debug rules with e.g. PDB. This flag allows\n         \
  \               to set breakpoints in run blocks. (default: False)\n  --runtime-profile\
  \ FILE\n                        Profile Snakemake and write the output to FILE.\
  \ This\n                        requires yappi to be installed. (default: None)\n\
  \  --mode {0,1,2}        Set execution mode of Snakemake (internal use only).\n\
  \                        (default: 0)\n  --show-failed-logs    Automatically display\
  \ logs of failed jobs. (default:\n                        False)\n  --log-handler-script\
  \ FILE\n                        Provide a custom script containing a function 'def\n\
  \                        log_handler(msg):'. Snakemake will call this function\n\
  \                        for every logging output (given as a dictionary\n     \
  \                   msg)allowing to e.g. send notifications in the form of\n   \
  \                     e.g. slack messages or emails. (default: None)\n  --log-service\
  \ {none,slack}\n                        Set a specific messaging service for logging\n\
  \                        output.Snakemake will notify the service on errors and\n\
  \                        completed execution.Currently only slack is supported.\n\
  \                        (default: None)\n\nCLUSTER:\n  --cluster CMD, -c CMD\n\
  \                        Execute snakemake rules with the given submit command,\n\
  \                        e.g. qsub. Snakemake compiles jobs into scripts that\n\
  \                        are submitted to the cluster with the given command,\n\
  \                        once all input files for a particular job are present.\n\
  \                        The submit command can be decorated to make it aware\n\
  \                        of certain job properties (name, rulename, input,\n   \
  \                     output, params, wildcards, log, threads and\n            \
  \            dependencies (see the argument below)), e.g.: $\n                 \
  \       snakemake --cluster 'qsub -pe threaded {threads}'.\n                   \
  \     (default: None)\n  --cluster-sync CMD    cluster submission command will block,\
  \ returning the\n                        remote exitstatus upon remote termination\
  \ (for\n                        example, this should be usedif the cluster command\
  \ is\n                        'qsub -sync y' (SGE) (default: None)\n  --drmaa [ARGS]\
  \        Execute snakemake on a cluster accessed via DRMAA,\n                  \
  \      Snakemake compiles jobs into scripts that are\n                        submitted\
  \ to the cluster with the given command, once\n                        all input\
  \ files for a particular job are present. ARGS\n                        can be used\
  \ to specify options of the underlying\n                        cluster system,\
  \ thereby using the job properties name,\n                        rulename, input,\
  \ output, params, wildcards, log,\n                        threads and dependencies,\
  \ e.g.: --drmaa ' -pe threaded\n                        {threads}'. Note that ARGS\
  \ must be given in quotes and\n                        with a leading whitespace.\
  \ (default: None)\n  --cluster-config FILE, -u FILE\n                        A JSON\
  \ or YAML file that defines the wildcards used in\n                        'cluster'for\
  \ specific rules, instead of having them\n                        specified in the\
  \ Snakefile. For example, for rule\n                        'job' you may define:\
  \ { 'job' : { 'time' : '24:00:00'\n                        } } to specify the time\
  \ for rule 'job'. You can\n                        specify more than one file. The\
  \ configuration files\n                        are merged with later values overriding\
  \ earlier ones.\n                        This option is deprecated in favor of using\
  \ --profile,\n                        see docs. (default: [])\n  --immediate-submit,\
  \ --is\n                        Immediately submit all jobs to the cluster instead\
  \ of\n                        waiting for present input files. This will fail,\n\
  \                        unless you make the cluster aware of job dependencies,\n\
  \                        e.g. via: $ snakemake --cluster 'sbatch --dependency\n\
  \                        {dependencies}. Assuming that your submit script (here\n\
  \                        sbatch) outputs the generated job id to the first\n   \
  \                     stdout line, {dependencies} will be filled with space\n  \
  \                      separated job ids this job depends on. (default:\n      \
  \                  False)\n  --jobscript SCRIPT, --js SCRIPT\n                 \
  \       Provide a custom job script for submission to the\n                    \
  \    cluster. The default script resides as 'jobscript.sh'\n                   \
  \     in the installation directory. (default: None)\n  --jobname NAME, --jn NAME\n\
  \                        Provide a custom name for the jobscript that is\n     \
  \                   submitted to the cluster (see --cluster). NAME is\n        \
  \                \"snakejob.{name}.{jobid}.sh\" per default. The wildcard\n    \
  \                    {jobid} has to be present in the name. (default:\n        \
  \                snakejob.{name}.{jobid}.sh)\n  --cluster-status CLUSTER_STATUS\n\
  \                        Status command for cluster execution. This is only\n  \
  \                      considered in combination with the --cluster flag. If\n \
  \                       provided, Snakemake will use the status command to\n   \
  \                     determine if a job has finished successfully or\n        \
  \                failed. For this it is necessary that the submit\n            \
  \            command provided to --cluster returns the cluster job\n           \
  \             id. Then, the status command will be invoked with the\n          \
  \              job id. Snakemake expects it to return 'success' if\n           \
  \             the job was successfull, 'failed' if the job failed\n            \
  \            and 'running' if the job still runs. (default: None)\n  --drmaa-log-dir\
  \ DIR   Specify a directory in which stdout and stderr files\n                 \
  \       of DRMAA jobs will be written. The value may be given\n                \
  \        as a relative path, in which case Snakemake will use\n                \
  \        the current invocation directory as the origin. If\n                  \
  \      given, this will override any given '-o' and/or '-e'\n                  \
  \      native specification. If not given, all DRMAA stdout\n                  \
  \      and stderr files are written to the current working\n                   \
  \     directory. (default: None)\n\nKUBERNETES:\n  --kubernetes [NAMESPACE]\n  \
  \                      Execute workflow in a kubernetes cluster (in the\n      \
  \                  cloud). NAMESPACE is the namespace you want to use for\n    \
  \                    your job (if nothing specified: 'default'). Usually,\n    \
  \                    this requires --default-remote-provider and --default-\n  \
  \                      remote-prefix to be set to a S3 or GS bucket where\n    \
  \                    your . data shall be stored. It is further advisable\n    \
  \                    to activate conda integration via --use-conda.\n          \
  \              (default: None)\n  --container-image IMAGE\n                    \
  \    Docker image to use, e.g., when submitting jobs to\n                      \
  \  kubernetes Defaults to\n                        'https://hub.docker.com/r/snakemake/snakemake',\
  \ tagged\n                        with the same version as the currently running\n\
  \                        Snakemake instance. Note that overwriting this value\n\
  \                        is up to your responsibility. Any used image has to\n \
  \                       contain a working snakemake installation that is\n     \
  \                   compatible with (or ideally the same as) the currently\n   \
  \                     running version. (default: None)\n\nTIBANNA:\n  --tibanna\
  \             Execute workflow on AWS cloud using Tibanna. This\n              \
  \          requires --default-remote-prefix to be set to S3\n                  \
  \      bucket name and prefix (e.g.\n                        'bucketname/subdirectory')\
  \ where input is already\n                        stored and output will be sent\
  \ to. Using --tibanna\n                        implies --default-resources is set\
  \ as default.\n                        Optionally, use --precommand to specify any\n\
  \                        preparation command to run before snakemake command on\n\
  \                        the cloud (inside snakemake container on Tibanna VM).\n\
  \                        Also, --use-conda, --use-singularity, --config,\n     \
  \                   --configfile are supported and will be carried over.\n     \
  \                   (default: False)\n  --tibanna-sfn TIBANNA_SFN\n            \
  \            Name of Tibanna Unicorn step function (e.g.\n                     \
  \   tibanna_unicorn_monty).This works as serverless\n                        scheduler/resource\
  \ allocator and must be deployed\n                        first using tibanna cli.\
  \ (e.g. tibanna deploy_unicorn\n                        --usergroup=monty --buckets=bucketname)\
  \ (default:\n                        None)\n  --precommand PRECOMMAND\n        \
  \                Any command to execute before snakemake command on AWS\n      \
  \                  cloud such as wget, git clone, unzip, etc. This is\n        \
  \                used with --tibanna.Do not include input/output\n             \
  \           download/upload commands - file transfer between S3\n              \
  \          bucket and the run environment (container) is\n                     \
  \   automatically handled by Tibanna. (default: None)\n  --tibanna-config TIBANNA_CONFIG\
  \ [TIBANNA_CONFIG ...]\n                        Additional tibanan config e.g. --tibanna-config\n\
  \                        spot_instance=true subnet=<subnet_id> security\n      \
  \                  group=<security_group_id> (default: None)\n\nGOOGLE_LIFE_SCIENCE:\n\
  \  --google-lifesciences\n                        Execute workflow on Google Cloud\
  \ cloud using the\n                        Google Life. Science API. This requires\
  \ default\n                        application credentials (json) to be created\
  \ and\n                        export to the environment to use Google Cloud Storage,\n\
  \                        Compute Engine, and Life Sciences. The credential file\n\
  \                        should be exported as GOOGLE_APPLICATION_CREDENTIALS\n\
  \                        for snakemake to discover. Also, --use-conda, --use-\n\
  \                        singularity, --config, --configfile are supported and\n\
  \                        will be carried over. (default: False)\n  --google-lifesciences-regions\
  \ GOOGLE_LIFESCIENCES_REGIONS [GOOGLE_LIFESCIENCES_REGIONS ...]\n              \
  \          Specify one or more valid instance regions (defaults\n              \
  \          to US) (default: ['us-east1', 'us-west1', 'us-\n                    \
  \    central1'])\n  --google-lifesciences-location GOOGLE_LIFESCIENCES_LOCATION\n\
  \                        The Life Sciences API service used to schedule the\n  \
  \                      jobs. E.g., us-centra1 (Iowa) and europe-west2\n        \
  \                (London) Watch the terminal output to see all options\n       \
  \                 found to be available. If not specified, defaults to\n       \
  \                 the first found with a matching prefix from regions\n        \
  \                specified with --google-lifesciences-regions.\n               \
  \         (default: None)\n  --google-lifesciences-keep-cache\n                \
  \        Cache workflows in your Google Cloud Storage Bucket\n                 \
  \       specified by --default-remote-prefix/{source}/{cache}.\n               \
  \         Each workflow working directory is compressed to a\n                 \
  \       .tar.gz, named by the hash of the contents, and kept\n                 \
  \       in Google Cloud Storage. By default, the caches are\n                  \
  \      deleted at the shutdown step of the workflow.\n                        (default:\
  \ False)\n\nCONDA:\n  --use-conda           If defined in the rule, run job in a\
  \ conda\n                        environment. If this flag is not set, the conda\n\
  \                        directive is ignored. (default: False)\n  --list-conda-envs\
  \     List all conda environments and their location on\n                      \
  \  disk. (default: False)\n  --conda-prefix DIR    Specify a directory in which\
  \ the 'conda' and 'conda-\n                        archive' directories are created.\
  \ These are used to\n                        store conda environments and their\
  \ archives,\n                        respectively. If not supplied, the value is\
  \ set to the\n                        '.snakemake' directory relative to the invocation\n\
  \                        directory. If supplied, the `--use-conda` flag must\n \
  \                       also be set. The value may be given as a relative\n    \
  \                    path, which will be extrapolated to the invocation\n      \
  \                  directory, or as an absolute path. (default: None)\n  --conda-cleanup-envs\
  \  Cleanup unused conda environments. (default: False)\n  --conda-cleanup-pkgs [{tarballs,cache}]\n\
  \                        Cleanup conda packages after creating environments. In\n\
  \                        case of 'tarballs' mode, will clean up all downloaded\n\
  \                        package tarballs. In case of 'cache' mode, will\n     \
  \                   additionally clean up unused package caches. If mode\n     \
  \                   is omitted, will default to only cleaning up the\n         \
  \               tarballs. (default: None)\n  --conda-create-envs-only\n        \
  \                If specified, only creates the job-specific conda\n           \
  \             environments then exits. The `--use-conda` flag must\n           \
  \             also be set. (default: False)\n  --conda-frontend {conda,mamba}\n\
  \                        Choose the conda frontend for installing environments.\n\
  \                        Caution: mamba is much faster, but still in beta test.\n\
  \                        (default: conda)\n\nSINGULARITY:\n  --use-singularity \
  \    If defined in the rule, run job within a singularity\n                    \
  \    container. If this flag is not set, the singularity\n                     \
  \   directive is ignored. (default: False)\n  --singularity-prefix DIR\n       \
  \                 Specify a directory in which singularity images will\n       \
  \                 be stored.If not supplied, the value is set to the\n         \
  \               '.snakemake' directory relative to the invocation\n            \
  \            directory. If supplied, the `--use-singularity` flag\n            \
  \            must also be set. The value may be given as a relative\n          \
  \              path, which will be extrapolated to the invocation\n            \
  \            directory, or as an absolute path. (default: None)\n  --singularity-args\
  \ ARGS\n                        Pass additional args to singularity. (default: )\n\
  \nENVIRONMENT MODULES:\n  --use-envmodules      If defined in the rule, run job\
  \ within the given\n                        environment modules, loaded in the given\
  \ order. This\n                        can be combined with --use-conda and --use-\n\
  \                        singularity, which will then be only used as a\n      \
  \                  fallback for rules which don't define environment\n         \
  \               modules. (default: False)\n"
generated_using:
- --help
