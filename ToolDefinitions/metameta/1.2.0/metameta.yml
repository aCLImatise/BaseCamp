!Command
command: &id001
- metameta
positional: []
named: []
parent:
subcommands:
- !Command
  command: &id002
  - metameta
  - target
  positional: []
  named:
  - !Flag
    description: use conda to automatically install pre-configured packages
    synonyms:
    - --use-conda
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: number of cores
    synonyms:
    - -j
    - --cores
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: go on with independent jobs if a job fails
    synonyms:
    - -k
    - --keep-going
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: do not execute anything
    synonyms:
    - -n
    - --dryrun
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: print out the shell commands that will be executed
    synonyms:
    - -p
    - --printshellcmds
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: add a timestamp to all logging output
    synonyms:
    - -t
    - --timestamp
    args: !EmptyFlagArg {}
    optional: true
  parent: &id007 !Command
    command: *id001
    positional: []
    named: []
    parent:
    subcommands:
    - !Command
      command: *id002
      positional: []
      named:
      - !Flag
        description: use conda to automatically install pre-configured packages
        synonyms:
        - --use-conda
        args: !EmptyFlagArg {}
        optional: true
      - !Flag
        description: number of cores
        synonyms:
        - -j
        - --cores
        args: !EmptyFlagArg {}
        optional: true
      - !Flag
        description: go on with independent jobs if a job fails
        synonyms:
        - -k
        - --keep-going
        args: !EmptyFlagArg {}
        optional: true
      - !Flag
        description: do not execute anything
        synonyms:
        - -n
        - --dryrun
        args: !EmptyFlagArg {}
        optional: true
      - !Flag
        description: print out the shell commands that will be executed
        synonyms:
        - -p
        - --printshellcmds
        args: !EmptyFlagArg {}
        optional: true
      - !Flag
        description: add a timestamp to all logging output
        synonyms:
        - -t
        - --timestamp
        args: !EmptyFlagArg {}
        optional: true
      parent: &id006 !Command
        command: *id001
        positional: []
        named: []
        parent:
        subcommands:
        - !Command
          command: *id002
          positional: []
          named:
          - !Flag
            description: use conda to automatically install pre-configured packages
            synonyms:
            - --use-conda
            args: !EmptyFlagArg {}
            optional: true
          - !Flag
            description: number of cores
            synonyms:
            - -j
            - --cores
            args: !EmptyFlagArg {}
            optional: true
          - !Flag
            description: go on with independent jobs if a job fails
            synonyms:
            - -k
            - --keep-going
            args: !EmptyFlagArg {}
            optional: true
          - !Flag
            description: do not execute anything
            synonyms:
            - -n
            - --dryrun
            args: !EmptyFlagArg {}
            optional: true
          - !Flag
            description: print out the shell commands that will be executed
            synonyms:
            - -p
            - --printshellcmds
            args: !EmptyFlagArg {}
            optional: true
          - !Flag
            description: add a timestamp to all logging output
            synonyms:
            - -t
            - --timestamp
            args: !EmptyFlagArg {}
            optional: true
          parent: &id003 !Command
            command: *id001
            positional: []
            named:
            - !Flag
              description: "Name of profile to use for configuring Snakemake.\nSnakemake\
                \ will search for a corresponding folder in\n/etc/xdg/snakemake and\
                \ /root/.config/snakemake.\nAlternatively, this can be an absolute\
                \ or relative\npath. The profile folder has to contain a file\n'config.yaml'.\
                \ This file can be used to set default\nvalues for command line options\
                \ in YAML format. For\nexample, '--cluster qsub' becomes 'cluster:\
                \ qsub' in\nthe YAML file. Profiles can be obtained from\nhttps://github.com/snakemake-profiles."
              synonyms:
              - --profile
              args: !SimpleFlagArg
                name: PROFILE
              optional: true
            - !Flag
              description: The workflow definition in a snakefile.
              synonyms:
              - --snakefile
              - -s
              args: !SimpleFlagArg
                name: FILE
              optional: true
            - !Flag
              description: "[PORT]          Serve an HTML based user interface to\
                \ the given\nnetwork and port e.g. 168.129.10.15:8000. By default\n\
                Snakemake is only available in the local network\n(default port: 8000).\
                \ To make Snakemake listen to all\nip addresses add the special host\
                \ address 0.0.0.0 to\nthe url (0.0.0.0:8000). This is important if\
                \ Snakemake\nis used in a virtualised environment like Docker. If\n\
                possible, a browser window is opened."
              synonyms:
              - --gui
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "[N], --jobs [N], -j [N]\nUse at most N cores in parallel\
                \ (default: 1). If N is\nomitted, the limit is set to the number of\
                \ available\ncores."
              synonyms:
              - --cores
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "In cluster mode, use at most N cores of the host\nmachine\
                \ in parallel (default: number of CPU cores of\nthe host). The cores\
                \ are used to execute local rules.\nThis option is ignored when not\
                \ in cluster mode."
              synonyms:
              - --local-cores
              args: !SimpleFlagArg
                name: N
              optional: true
            - !Flag
              description: "[NAME=INT [NAME=INT ...]], --res [NAME=INT [NAME=INT ...]]\n\
                Define additional resources that shall constrain the\nscheduling analogously\
                \ to threads (see above). A\nresource is defined as a name and an\
                \ integer value.\nE.g. --resources gpu=1. Rules can use resources\
                \ by\ndefining the resource keyword, e.g. resources: gpu=1.\nIf now\
                \ two rules require 1 of the resource 'gpu' they\nwon't be run in\
                \ parallel by the scheduler."
              synonyms:
              - --resources
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "[KEY=VALUE [KEY=VALUE ...]], -C [KEY=VALUE [KEY=VALUE\
                \ ...]]\nSet or overwrite values in the workflow config object.\n\
                The workflow config object is accessible as variable\nconfig inside\
                \ the workflow. Default values can be set\nby providing a JSON file\
                \ (see Documentation)."
              synonyms:
              - --config
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Specify or overwrite the config file of the workflow\n\
                (see the docs). Values specified in JSON or YAML\nformat are available\
                \ in the global config dictionary\ninside the workflow."
              synonyms:
              - --configfile
              args: !SimpleFlagArg
                name: FILE
              optional: true
            - !Flag
              description: Show availiable rules in given Snakefile.
              synonyms:
              - --list
              - -l
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: Show available target rules in given Snakefile.
              synonyms:
              - --list-target-rules
              - --lt
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Specify working directory (relative paths in the\nsnakefile\
                \ will use this as their origin)."
              synonyms:
              - --directory
              - -d
              args: !SimpleFlagArg
                name: DIR
              optional: true
            - !Flag
              description: Do not execute anything.
              synonyms:
              - --dryrun
              - -n
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: Print out the shell commands that will be executed.
              synonyms:
              - --printshellcmds
              - -p
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Print candidate and selected jobs (including their\nwildcards)\
                \ while inferring DAG. This can help to debug\nunexpected DAG topology\
                \ or errors."
              synonyms:
              - --debug-dag
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Do not execute anything and print the directed acyclic\n\
                graph of jobs in the dot language. Recommended use on\nUnix systems:\
                \ snakemake --dag | dot | display"
              synonyms:
              - --dag
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Force threads rather than processes. Helpful if shared\n\
                memory (/dev/shm) is full or unavailable."
              synonyms:
              - --force-use-threads
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Do not execute anything and print the dependency graph\n\
                of rules in the dot language. This will be less\ncrowded than above\
                \ DAG of jobs, but also show less\ninformation. Note that each rule\
                \ is displayed once,\nhence the displayed graph will be cyclic if\
                \ a rule\nappears in several steps of the workflow. Use this if\n\
                above option leads to a DAG that is too large.\nRecommended use on\
                \ Unix systems: snakemake --rulegraph\n| dot | display"
              synonyms:
              - --rulegraph
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: Print the DAG in D3.js compatible JSON format.
              synonyms:
              - --d3dag
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Print a summary of all files created by the workflow.\n\
                The has the following columns: filename, modification\ntime, rule\
                \ version, status, plan. Thereby rule version\ncontains the versionthe\
                \ file was created with (see the\nversion keyword of rules), and status\
                \ denotes whether\nthe file is missing, its input files are newer\
                \ or if\nversion or implementation of the rule changed since\nfile\
                \ creation. Finally the last column denotes whether\nthe file will\
                \ be updated or created during the next\nworkflow execution."
              synonyms:
              - --summary
              - -S
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Print a summary of all files created by the workflow.\n\
                The has the following columns: filename, modification\ntime, rule\
                \ version, input file(s), shell command,\nstatus, plan. Thereby rule\
                \ version contains the\nversionthe file was created with (see the\
                \ version\nkeyword of rules), and status denotes whether the file\n\
                is missing, its input files are newer or if version or\nimplementation\
                \ of the rule changed since file\ncreation. The input file and shell\
                \ command columns are\nselfexplanatory. Finally the last column denotes\n\
                whether the file will be updated or created during the\nnext workflow\
                \ execution."
              synonyms:
              - --detailed-summary
              - -D
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Archive the workflow into the given tar archive FILE.\n\
                The archive will be created such that the workflow can\nbe re-executed\
                \ on a vanilla system. The function needs\nconda and git to be installed.\
                \ It will archive every\nfile that is under git version control. Note\
                \ that it\nis best practice to have the Snakefile, config files,\n\
                and scripts under version control. Hence, they will be\nincluded in\
                \ the archive. Further, it will add input\nfiles that are not generated\
                \ by by the workflow itself\nand conda environments. Note that symlinks\
                \ are\ndereferenced. Supported formats are .tar, .tar.gz,\n.tar.bz2\
                \ and .tar.xz."
              synonyms:
              - --archive
              args: !SimpleFlagArg
                name: FILE
              optional: true
            - !Flag
              description: "Touch output files (mark them up to date without\nreally\
                \ changing them) instead of running their\ncommands. This is used\
                \ to pretend that the rules were\nexecuted, in order to fool future\
                \ invocations of\nsnakemake. Fails if a file does not yet exist."
              synonyms:
              - --touch
              - -t
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: Go on with independent jobs if a job fails.
              synonyms:
              - --keep-going
              - -k
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Force the execution of the selected target or the\nfirst\
                \ rule regardless of already created output."
              synonyms:
              - --force
              - -f
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Force the execution of the selected (or the first)\nrule\
                \ and all rules it is dependent on regardless of\nalready created\
                \ output."
              synonyms:
              - --forceall
              - -F
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "[TARGET [TARGET ...]], -R [TARGET [TARGET ...]]\nForce\
                \ the re-execution or creation of the given rules\nor files. Use this\
                \ option if you changed a rule and\nwant to have all its output in\
                \ your workflow updated."
              synonyms:
              - --forcerun
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Tell the scheduler to assign creation of given targets\n\
                (and all their dependencies) highest priority.\n(EXPERIMENTAL)"
              synonyms:
              - --prioritize
              - -P
              args: !RepeatFlagArg
                name: TARGET
              optional: true
            - !Flag
              description: "Runs the pipeline until it reaches the specified rules\n\
                or files. Only runs jobs that are dependencies of the\nspecified rule\
                \ or files, does not run sibling DAGs."
              synonyms:
              - --until
              - -U
              args: !RepeatFlagArg
                name: TARGET
              optional: true
            - !Flag
              description: "Prevent the execution or creation of the given rules\n\
                or files as well as any rules or files that are\ndownstream of these\
                \ targets in the DAG. Also runs jobs\nin sibling DAGs that are independent\
                \ of the rules or\nfiles specified here."
              synonyms:
              - --omit-from
              - -O
              args: !RepeatFlagArg
                name: TARGET
              optional: true
            - !Flag
              description: "Don't check for ambiguous rules and simply use the\nfirst\
                \ if several can produce the same file. This\nallows the user to prioritize\
                \ rules by their order in\nthe snakefile."
              synonyms:
              - --allow-ambiguity
              - -a
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Execute snakemake rules with the given submit command,\n\
                e.g. qsub. Snakemake compiles jobs into scripts that\nare submitted\
                \ to the cluster with the given command,\nonce all input files for\
                \ a particular job are present.\nThe submit command can be decorated\
                \ to make it aware\nof certain job properties (input, output, params,\n\
                wildcards, log, threads and dependencies (see the\nargument below)),\
                \ e.g.: $ snakemake --cluster 'qsub\n-pe threaded {threads}'."
              synonyms:
              - --cluster
              - -c
              args: !SimpleFlagArg
                name: CMD
              optional: true
            - !Flag
              description: "cluster submission command will block, returning the\n\
                remote exitstatus upon remote termination (for\nexample, this should\
                \ be usedif the cluster command is\n'qsub -sync y' (SGE)"
              synonyms:
              - --cluster-sync
              args: !SimpleFlagArg
                name: CMD
              optional: true
            - !Flag
              description: "[ARGS]        Execute snakemake on a cluster accessed\
                \ via DRMAA,\nSnakemake compiles jobs into scripts that are\nsubmitted\
                \ to the cluster with the given command, once\nall input files for\
                \ a particular job are present. ARGS\ncan be used to specify options\
                \ of the underlying\ncluster system, thereby using the job properties\n\
                input, output, params, wildcards, log, threads and\ndependencies,\
                \ e.g.: --drmaa ' -pe threaded {threads}'.\nNote that ARGS must be\
                \ given in quotes and with a\nleading whitespace."
              synonyms:
              - --drmaa
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Specify a directory in which stdout and stderr files\n\
                of DRMAA jobs will be written. The value may be given\nas a relative\
                \ path, in which case Snakemake will use\nthe current invocation directory\
                \ as the origin. If\ngiven, this will override any given '-o' and/or\
                \ '-e'\nnative specification. If not given, all DRMAA stdout\nand\
                \ stderr files are written to the current working\ndirectory."
              synonyms:
              - --drmaa-log-dir
              args: !SimpleFlagArg
                name: DIR
              optional: true
            - !Flag
              description: "A JSON or YAML file that defines the wildcards used in\n\
                'cluster'for specific rules, instead of having them\nspecified in\
                \ the Snakefile. For example, for rule\n'job' you may define: { 'job'\
                \ : { 'time' : '24:00:00'\n} } to specify the time for rule 'job'.\
                \ You can\nspecify more than one file. The configuration files\nare\
                \ merged with later values overriding earlier ones."
              synonyms:
              - --cluster-config
              - -u
              args: !SimpleFlagArg
                name: FILE
              optional: true
            - !Flag
              description: "Immediately submit all jobs to the cluster instead of\n\
                waiting for present input files. This will fail,\nunless you make\
                \ the cluster aware of job dependencies,\ne.g. via: $ snakemake --cluster\
                \ 'sbatch --dependency\n{dependencies}. Assuming that your submit\
                \ script (here\nsbatch) outputs the generated job id to the first\n\
                stdout line, {dependencies} will be filled with space\nseparated job\
                \ ids this job depends on."
              synonyms:
              - --immediate-submit
              - --is
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Provide a custom job script for submission to the\ncluster.\
                \ The default script resides as 'jobscript.sh'\nin the installation\
                \ directory."
              synonyms:
              - --jobscript
              - --js
              args: !SimpleFlagArg
                name: SCRIPT
              optional: true
            - !Flag
              description: "Provide a custom name for the jobscript that is\nsubmitted\
                \ to the cluster (see --cluster). NAME is\n\"snakejob.{rulename}.{jobid}.sh\"\
                \ per default. The\nwildcard {jobid} has to be present in the name."
              synonyms:
              - --jobname
              - --jn
              args: !SimpleFlagArg
                name: NAME
              optional: true
            - !Flag
              description: "Status command for cluster execution. This is only\nconsidered\
                \ in combination with the --cluster flag. If\nprovided, Snakemake\
                \ will use the status command to\ndetermine if a job has finished\
                \ successfully or\nfailed. For this it is necessary that the submit\n\
                command provided to --cluster returns the cluster job\nid. Then, the\
                \ status command will be invoked with the\njob id. Snakemake expects\
                \ it to return 'success' if\nthe job was successfull, 'failed' if\
                \ the job failed\nand 'running' if the job still runs."
              synonyms:
              - --cluster-status
              args: !SimpleFlagArg
                name: CLUSTER_STATUS
              optional: true
            - !Flag
              description: "[NAMESPACE]\nExecute workflow in a kubernetes cluster\
                \ (in the\ncloud). NAMESPACE is the namespace you want to use for\n\
                your job (if nothing specified: 'default'). Usually,\nthis requires\
                \ --default-remote-provider and --default-\nremote-prefix to be set\
                \ to a S3 or GS bucket where\nyour . data shall be stored. It is further\
                \ advisable\nto activate conda integration via --use-conda."
              synonyms:
              - --kubernetes
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Specify environment variables to pass to the\nkubernetes\
                \ job."
              synonyms:
              - --kubernetes-env
              args: !RepeatFlagArg
                name: ENVVAR
              optional: true
            - !Flag
              description: "Docker image to use, e.g., when submitting jobs to\nkubernetes.\
                \ By default, this is\n'quay.io/snakemake/snakemake', tagged with\
                \ the same\nversion as the currently running Snakemake instance.\n\
                Note that overwriting this value is up to your\nresponsibility. Any\
                \ used image has to contain a\nworking snakemake installation that\
                \ is compatible with\n(or ideally the same as) the currently running\n\
                version."
              synonyms:
              - --container-image
              args: !SimpleFlagArg
                name: IMAGE
              optional: true
            - !Flag
              description: Print the reason for each executed rule.
              synonyms:
              - --reason
              - -r
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Write stats about Snakefile execution in JSON format\n\
                to the given file."
              synonyms:
              - --stats
              args: !SimpleFlagArg
                name: FILE
              optional: true
            - !Flag
              description: Do not use a colored output.
              synonyms:
              - --nocolor
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: Do not output any progress or rule information.
              synonyms:
              - --quiet
              - -q
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: Do not lock the working directory
              synonyms:
              - --nolock
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: Remove a lock on the working directory.
              synonyms:
              - --unlock
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Cleanup the metadata of given files. That means that\n\
                snakemake removes any tracked version info, and any\nmarks that files\
                \ are incomplete."
              synonyms:
              - --cleanup-metadata
              - --cm
              args: !RepeatFlagArg
                name: FILE
              optional: true
            - !Flag
              description: "Re-run all jobs the output of which is recognized as\n\
                incomplete."
              synonyms:
              - --rerun-incomplete
              - --ri
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: Do not check for incomplete output files.
              synonyms:
              - --ignore-incomplete
              - --ii
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "List all output files that have been created with a\n\
                different version (as determined by the version\nkeyword)."
              synonyms:
              - --list-version-changes
              - --lv
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "List all output files for which the rule body (run or\n\
                shell) have changed in the Snakefile."
              synonyms:
              - --list-code-changes
              - --lc
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "List all output files for which the defined input\nfiles\
                \ have changed in the Snakefile (e.g. new input\nfiles were added\
                \ in the rule definition or files were\nrenamed). For listing input\
                \ file modification in the\nfilesystem, use --summary."
              synonyms:
              - --list-input-changes
              - --li
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "List all output files for which the defined params\nhave\
                \ changed in the Snakefile."
              synonyms:
              - --list-params-changes
              - --lp
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Wait given seconds if an output file of a job is not\n\
                present after the job finished. This helps if your\nfilesystem suffers\
                \ from latency (default 5)."
              synonyms:
              - --latency-wait
              - --output-wait
              - -w
              args: !SimpleFlagArg
                name: SECONDS
              optional: true
            - !Flag
              description: "[FILE [FILE ...]]\nWait --latency-wait seconds for these\
                \ files to be\npresent before executing the workflow. This option\
                \ is\nused internally to handle filesystem latency in\ncluster environments."
              synonyms:
              - --wait-for-files
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Repeat a job N times if marked for benchmarking\n(default\
                \ 1)."
              synonyms:
              - --benchmark-repeats
              args: !SimpleFlagArg
                name: N
              optional: true
            - !Flag
              description: "Ignore temp() declarations. This is useful when\nrunning\
                \ only a part of the workflow, since temp()\nwould lead to deletion\
                \ of probably needed files by\nother parts of the workflow."
              synonyms:
              - --notemp
              - --nt
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: Keep local copies of remote input files.
              synonyms:
              - --keep-remote
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Do not adjust the paths of given target files relative\n\
                to the working directory."
              synonyms:
              - --keep-target-files
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: Do not delete the shadow directory on snakemake
              synonyms:
              - --keep-shadow
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Maximal number of cluster/drmaa jobs per second,\ndefault\
                \ is 10, fractions allowed."
              synonyms:
              - --max-jobs-per-second
              args: !SimpleFlagArg
                name: MAX_JOBS_PER_SECOND
              optional: true
            - !Flag
              description: "Maximal number of job status checks per second,\ndefault\
                \ is 10, fractions allowed."
              synonyms:
              - --max-status-checks-per-second
              args: !SimpleFlagArg
                name: MAX_STATUS_CHECKS_PER_SECOND
              optional: true
            - !Flag
              description: "Number of times to restart failing jobs (defaults to\n\
                0)."
              synonyms:
              - --restart-times
              args: !SimpleFlagArg
                name: RESTART_TIMES
              optional: true
            - !Flag
              description: "Internal use only: define the initial value of the\nattempt\
                \ parameter (default: 1)."
              synonyms:
              - --attempt
              args: !SimpleFlagArg
                name: ATTEMPT
              optional: true
            - !Flag
              description: Add a timestamp to all logging output
              synonyms:
              - --timestamp
              - -T
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Set the greediness of scheduling. This value between 0\n\
                and 1 determines how careful jobs are selected for\nexecution. The\
                \ default value (1.0) provides the best\nspeed and still acceptable\
                \ scheduling quality."
              synonyms:
              - --greediness
              args: !SimpleFlagArg
                name: GREEDINESS
              optional: true
            - !Flag
              description: "Do not invoke onstart, onsuccess or onerror hooks\nafter\
                \ execution."
              synonyms:
              - --no-hooks
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: Print the python representation of the workflow.
              synonyms:
              - --print-compilation
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Provide a shell command that shall be executed instead\n\
                of those given in the workflow. This is for debugging\npurposes only."
              synonyms:
              - --overwrite-shellcmd
              args: !SimpleFlagArg
                name: OVERWRITE_SHELLCMD
              optional: true
            - !Flag
              description: Print debugging output.
              synonyms:
              - --verbose
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Allow to debug rules with e.g. PDB. This flag allows\n\
                to set breakpoints in run blocks."
              synonyms:
              - --debug
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Profile Snakemake and write the output to FILE. This\n\
                requires yappi to be installed."
              synonyms:
              - --runtime-profile
              args: !SimpleFlagArg
                name: FILE
              optional: true
            - !Flag
              description: Set execution mode of Snakemake (internal use only).
              synonyms:
              - --mode
              args: !ChoiceFlagArg
                choices: !!set
                  ? "1"
                  ? "2"
                  ? "0"
              optional: true
            - !Flag
              description: "Output code to register bash completion for snakemake.\n\
                Put the following in your .bashrc (including the\naccents): `snakemake\
                \ --bash-completion` or issue it in\nan open terminal session."
              synonyms:
              - --bash-completion
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "If defined in the rule, run job in a conda\nenvironment.\
                \ If this flag is not set, the conda\ndirective is ignored."
              synonyms:
              - --use-conda
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Specify a directory in which the 'conda' and 'conda-\n\
                archive' directories are created. These are used to\nstore conda environments\
                \ and their archives,\nrespectively. If not supplied, the value is\
                \ set to the\n'.snakemake' directory relative to the invocation\n\
                directory. If supplied, the `--use-conda` flag must\nalso be set.\
                \ The value may be given as a relative\npath, which will be extrapolated\
                \ to the invocation\ndirectory, or as an absolute path."
              synonyms:
              - --conda-prefix
              args: !SimpleFlagArg
                name: DIR
              optional: true
            - !Flag
              description: "If specified, only creates the job-specific conda\nenvironments\
                \ then exits. The `--use-conda` flag must\nalso be set."
              synonyms:
              - --create-envs-only
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "If defined in the rule, run job within a singularity\n\
                container. If this flag is not set, the singularity\ndirective is\
                \ ignored."
              synonyms:
              - --use-singularity
              args: !EmptyFlagArg {}
              optional: true
            - !Flag
              description: "Specify a directory in which singularity images will\n\
                be stored.If not supplied, the value is set to the\n'.snakemake' directory\
                \ relative to the invocation\ndirectory. If supplied, the `--use-singularity`\
                \ flag\nmust also be set. The value may be given as a relative\npath,\
                \ which will be extrapolated to the invocation\ndirectory, or as an\
                \ absolute path."
              synonyms:
              - --singularity-prefix
              args: !SimpleFlagArg
                name: DIR
              optional: true
            - !Flag
              description: Pass additional args to singularity.
              synonyms:
              - --singularity-args
              args: !SimpleFlagArg
                name: ARGS
              optional: true
            - !Flag
              description: "Prefix for URL created from wrapper directive\n(default:\
                \ https://bitbucket.org/snakemake/snakemake-\nwrappers/raw/). Set\
                \ this to a different URL to use\nyour fork or a local clone of the\
                \ repository."
              synonyms:
              - --wrapper-prefix
              args: !SimpleFlagArg
                name: WRAPPER_PREFIX
              optional: true
            - !Flag
              description: "Specify default remote provider to be used for all\ninput\
                \ and output files that don't yet specify one."
              synonyms:
              - --default-remote-provider
              args: !ChoiceFlagArg
                choices: !!set
                  ? gridftp
                  ? FTP
                  ? GS
                  ? SFTP
                  ? S3
                  ? gfal
                  ? S3Mocked
              optional: true
            - !Flag
              description: "Specify prefix for default remote provider. E.g. a\nbucket\
                \ name."
              synonyms:
              - --default-remote-prefix
              args: !SimpleFlagArg
                name: DEFAULT_REMOTE_PREFIX
              optional: true
            - !Flag
              description: "Do not assume that jobs share a common file system.\n\
                When this flag is activated, Snakemake will assume\nthat the filesystem\
                \ on a cluster node is not shared\nwith other nodes. For example,\
                \ this will lead to\ndownloading remote files on each cluster node\n\
                separately. Further, it won't take special measures to\ndeal with\
                \ filesystem latency issues. This option will\nin most cases only\
                \ make sense in combination with\n--default-remote-provider. Further,\
                \ when using\n--cluster you will have to also provide --cluster-\n\
                status. Only activate this if you know what you are\ndoing."
              synonyms:
              - --no-shared-fs
              args: !EmptyFlagArg {}
              optional: true
            parent:
            subcommands:
            - !Command
              command: *id002
              positional: []
              named:
              - !Flag
                description: use conda to automatically install pre-configured packages
                synonyms:
                - --use-conda
                args: !EmptyFlagArg {}
                optional: true
              - !Flag
                description: number of cores
                synonyms:
                - -j
                - --cores
                args: !EmptyFlagArg {}
                optional: true
              - !Flag
                description: go on with independent jobs if a job fails
                synonyms:
                - -k
                - --keep-going
                args: !EmptyFlagArg {}
                optional: true
              - !Flag
                description: do not execute anything
                synonyms:
                - -n
                - --dryrun
                args: !EmptyFlagArg {}
                optional: true
              - !Flag
                description: print out the shell commands that will be executed
                synonyms:
                - -p
                - --printshellcmds
                args: !EmptyFlagArg {}
                optional: true
              - !Flag
                description: add a timestamp to all logging output
                synonyms:
                - -t
                - --timestamp
                args: !EmptyFlagArg {}
                optional: true
              parent: *id003
              subcommands: []
              usage: []
              help_flag: !Flag
                description: show Snakemake help (or snakemake -h)
                synonyms:
                - --help
                args: !EmptyFlagArg {}
                optional: true
              usage_flag:
              version_flag:
              help_text: "MetaMeta Pipeline (powered by Snakemake)\n\nUsage: /usr/local/bin/metameta\
                \ --configfile FILE [Snakemake options]\n\n Useful Snakemake parameters:\n\
                \   --use-conda            use conda to automatically install pre-configured\
                \ packages\n   -j, --cores            number of cores\n   -k, --keep-going\
                \       go on with independent jobs if a job fails\n   -n, --dryrun\
                \           do not execute anything\n   -p, --printshellcmds   print\
                \ out the shell commands that will be executed\n   -t, --timestamp\
                \  \t\tadd a timestamp to all logging output\n\n Full list of parameters:\n\
                \   --help                 show Snakemake help (or snakemake -h)\n\
                \n"
              generated_using: &id004
              - --help
            - !Command
              command: &id005
              - metameta
              - startup.
              positional: []
              named:
              - !Flag
                description: use conda to automatically install pre-configured packages
                synonyms:
                - --use-conda
                args: !EmptyFlagArg {}
                optional: true
              - !Flag
                description: number of cores
                synonyms:
                - -j
                - --cores
                args: !EmptyFlagArg {}
                optional: true
              - !Flag
                description: go on with independent jobs if a job fails
                synonyms:
                - -k
                - --keep-going
                args: !EmptyFlagArg {}
                optional: true
              - !Flag
                description: do not execute anything
                synonyms:
                - -n
                - --dryrun
                args: !EmptyFlagArg {}
                optional: true
              - !Flag
                description: print out the shell commands that will be executed
                synonyms:
                - -p
                - --printshellcmds
                args: !EmptyFlagArg {}
                optional: true
              - !Flag
                description: add a timestamp to all logging output
                synonyms:
                - -t
                - --timestamp
                args: !EmptyFlagArg {}
                optional: true
              parent: *id003
              subcommands: []
              usage: []
              help_flag: !Flag
                description: show Snakemake help (or snakemake -h)
                synonyms:
                - --help
                args: !EmptyFlagArg {}
                optional: true
              usage_flag:
              version_flag:
              help_text: "MetaMeta Pipeline (powered by Snakemake)\n\nUsage: /usr/local/bin/metameta\
                \ --configfile FILE [Snakemake options]\n\n Useful Snakemake parameters:\n\
                \   --use-conda            use conda to automatically install pre-configured\
                \ packages\n   -j, --cores            number of cores\n   -k, --keep-going\
                \       go on with independent jobs if a job fails\n   -n, --dryrun\
                \           do not execute anything\n   -p, --printshellcmds   print\
                \ out the shell commands that will be executed\n   -t, --timestamp\
                \  \t\tadd a timestamp to all logging output\n\n Full list of parameters:\n\
                \   --help                 show Snakemake help (or snakemake -h)\n\
                \n"
              generated_using: *id004
            usage: []
            help_flag: !Flag
              description: show this help message and exit
              synonyms:
              - -h
              - --help
              args: !EmptyFlagArg {}
              optional: true
            usage_flag:
            version_flag: !Flag
              description: show program's version number and exit
              synonyms:
              - --version
              - -v
              args: !EmptyFlagArg {}
              optional: true
            help_text: "usage: snakemake [-h] [--profile PROFILE] [--snakefile FILE]\
              \ [--gui [PORT]]\n                 [--cores [N]] [--local-cores N]\n\
              \                 [--resources [NAME=INT [NAME=INT ...]]]\n        \
              \         [--config [KEY=VALUE [KEY=VALUE ...]]] [--configfile FILE]\n\
              \                 [--list] [--list-target-rules] [--directory DIR] [--dryrun]\n\
              \                 [--printshellcmds] [--debug-dag] [--dag]\n       \
              \          [--force-use-threads] [--rulegraph] [--d3dag] [--summary]\n\
              \                 [--detailed-summary] [--archive FILE] [--touch]\n\
              \                 [--keep-going] [--force] [--forceall]\n          \
              \       [--forcerun [TARGET [TARGET ...]]]\n                 [--prioritize\
              \ TARGET [TARGET ...]]\n                 [--until TARGET [TARGET ...]]\n\
              \                 [--omit-from TARGET [TARGET ...]] [--allow-ambiguity]\n\
              \                 [--cluster CMD | --cluster-sync CMD | --drmaa [ARGS]]\n\
              \                 [--drmaa-log-dir DIR] [--cluster-config FILE]\n  \
              \               [--immediate-submit] [--jobscript SCRIPT] [--jobname\
              \ NAME]\n                 [--cluster-status CLUSTER_STATUS] [--kubernetes\
              \ [NAMESPACE]]\n                 [--kubernetes-env ENVVAR [ENVVAR ...]]\n\
              \                 [--container-image IMAGE] [--reason] [--stats FILE]\n\
              \                 [--nocolor] [--quiet] [--nolock] [--unlock]\n    \
              \             [--cleanup-metadata FILE [FILE ...]] [--rerun-incomplete]\n\
              \                 [--ignore-incomplete] [--list-version-changes]\n \
              \                [--list-code-changes] [--list-input-changes]\n    \
              \             [--list-params-changes] [--latency-wait SECONDS]\n   \
              \              [--wait-for-files [FILE [FILE ...]]] [--benchmark-repeats\
              \ N]\n                 [--notemp] [--keep-remote] [--keep-target-files]\n\
              \                 [--keep-shadow]\n                 [--allowed-rules\
              \ ALLOWED_RULES [ALLOWED_RULES ...]]\n                 [--max-jobs-per-second\
              \ MAX_JOBS_PER_SECOND]\n                 [--max-status-checks-per-second\
              \ MAX_STATUS_CHECKS_PER_SECOND]\n                 [--restart-times RESTART_TIMES]\
              \ [--attempt ATTEMPT]\n                 [--timestamp] [--greediness\
              \ GREEDINESS] [--no-hooks]\n                 [--print-compilation]\n\
              \                 [--overwrite-shellcmd OVERWRITE_SHELLCMD] [--verbose]\n\
              \                 [--debug] [--runtime-profile FILE] [--mode {0,1,2}]\n\
              \                 [--bash-completion] [--use-conda] [--conda-prefix\
              \ DIR]\n                 [--create-envs-only] [--use-singularity]\n\
              \                 [--singularity-prefix DIR] [--singularity-args ARGS]\n\
              \                 [--wrapper-prefix WRAPPER_PREFIX]\n              \
              \   [--default-remote-provider {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp}]\n\
              \                 [--default-remote-prefix DEFAULT_REMOTE_PREFIX]\n\
              \                 [--no-shared-fs] [--version]\n                 [target\
              \ [target ...]]\n\nSnakemake is a Python based language and execution\
              \ environment for GNU Make-\nlike workflows.\n\npositional arguments:\n\
              \  target                Targets to build. May be rules or files.\n\n\
              optional arguments:\n  -h, --help            show this help message\
              \ and exit\n  --profile PROFILE     Name of profile to use for configuring\
              \ Snakemake.\n                        Snakemake will search for a corresponding\
              \ folder in\n                        /etc/xdg/snakemake and /root/.config/snakemake.\n\
              \                        Alternatively, this can be an absolute or relative\n\
              \                        path. The profile folder has to contain a file\n\
              \                        'config.yaml'. This file can be used to set\
              \ default\n                        values for command line options in\
              \ YAML format. For\n                        example, '--cluster qsub'\
              \ becomes 'cluster: qsub' in\n                        the YAML file.\
              \ Profiles can be obtained from\n                        https://github.com/snakemake-profiles.\n\
              \  --snakefile FILE, -s FILE\n                        The workflow definition\
              \ in a snakefile.\n  --gui [PORT]          Serve an HTML based user\
              \ interface to the given\n                        network and port e.g.\
              \ 168.129.10.15:8000. By default\n                        Snakemake\
              \ is only available in the local network\n                        (default\
              \ port: 8000). To make Snakemake listen to all\n                   \
              \     ip addresses add the special host address 0.0.0.0 to\n       \
              \                 the url (0.0.0.0:8000). This is important if Snakemake\n\
              \                        is used in a virtualised environment like Docker.\
              \ If\n                        possible, a browser window is opened.\n\
              \  --cores [N], --jobs [N], -j [N]\n                        Use at most\
              \ N cores in parallel (default: 1). If N is\n                      \
              \  omitted, the limit is set to the number of available\n          \
              \              cores.\n  --local-cores N       In cluster mode, use\
              \ at most N cores of the host\n                        machine in parallel\
              \ (default: number of CPU cores of\n                        the host).\
              \ The cores are used to execute local rules.\n                     \
              \   This option is ignored when not in cluster mode.\n  --resources\
              \ [NAME=INT [NAME=INT ...]], --res [NAME=INT [NAME=INT ...]]\n     \
              \                   Define additional resources that shall constrain\
              \ the\n                        scheduling analogously to threads (see\
              \ above). A\n                        resource is defined as a name and\
              \ an integer value.\n                        E.g. --resources gpu=1.\
              \ Rules can use resources by\n                        defining the resource\
              \ keyword, e.g. resources: gpu=1.\n                        If now two\
              \ rules require 1 of the resource 'gpu' they\n                     \
              \   won't be run in parallel by the scheduler.\n  --config [KEY=VALUE\
              \ [KEY=VALUE ...]], -C [KEY=VALUE [KEY=VALUE ...]]\n               \
              \         Set or overwrite values in the workflow config object.\n \
              \                       The workflow config object is accessible as\
              \ variable\n                        config inside the workflow. Default\
              \ values can be set\n                        by providing a JSON file\
              \ (see Documentation).\n  --configfile FILE     Specify or overwrite\
              \ the config file of the workflow\n                        (see the\
              \ docs). Values specified in JSON or YAML\n                        format\
              \ are available in the global config dictionary\n                  \
              \      inside the workflow.\n  --list, -l            Show availiable\
              \ rules in given Snakefile.\n  --list-target-rules, --lt\n         \
              \               Show available target rules in given Snakefile.\n  --directory\
              \ DIR, -d DIR\n                        Specify working directory (relative\
              \ paths in the\n                        snakefile will use this as their\
              \ origin).\n  --dryrun, -n          Do not execute anything.\n  --printshellcmds,\
              \ -p  Print out the shell commands that will be executed.\n  --debug-dag\
              \           Print candidate and selected jobs (including their\n   \
              \                     wildcards) while inferring DAG. This can help\
              \ to debug\n                        unexpected DAG topology or errors.\n\
              \  --dag                 Do not execute anything and print the directed\
              \ acyclic\n                        graph of jobs in the dot language.\
              \ Recommended use on\n                        Unix systems: snakemake\
              \ --dag | dot | display\n  --force-use-threads   Force threads rather\
              \ than processes. Helpful if shared\n                        memory\
              \ (/dev/shm) is full or unavailable.\n  --rulegraph           Do not\
              \ execute anything and print the dependency graph\n                \
              \        of rules in the dot language. This will be less\n         \
              \               crowded than above DAG of jobs, but also show less\n\
              \                        information. Note that each rule is displayed\
              \ once,\n                        hence the displayed graph will be cyclic\
              \ if a rule\n                        appears in several steps of the\
              \ workflow. Use this if\n                        above option leads\
              \ to a DAG that is too large.\n                        Recommended use\
              \ on Unix systems: snakemake --rulegraph\n                        |\
              \ dot | display\n  --d3dag               Print the DAG in D3.js compatible\
              \ JSON format.\n  --summary, -S         Print a summary of all files\
              \ created by the workflow.\n                        The has the following\
              \ columns: filename, modification\n                        time, rule\
              \ version, status, plan. Thereby rule version\n                    \
              \    contains the versionthe file was created with (see the\n      \
              \                  version keyword of rules), and status denotes whether\n\
              \                        the file is missing, its input files are newer\
              \ or if\n                        version or implementation of the rule\
              \ changed since\n                        file creation. Finally the\
              \ last column denotes whether\n                        the file will\
              \ be updated or created during the next\n                        workflow\
              \ execution.\n  --detailed-summary, -D\n                        Print\
              \ a summary of all files created by the workflow.\n                \
              \        The has the following columns: filename, modification\n   \
              \                     time, rule version, input file(s), shell command,\n\
              \                        status, plan. Thereby rule version contains\
              \ the\n                        versionthe file was created with (see\
              \ the version\n                        keyword of rules), and status\
              \ denotes whether the file\n                        is missing, its\
              \ input files are newer or if version or\n                        implementation\
              \ of the rule changed since file\n                        creation.\
              \ The input file and shell command columns are\n                   \
              \     selfexplanatory. Finally the last column denotes\n           \
              \             whether the file will be updated or created during the\n\
              \                        next workflow execution.\n  --archive FILE\
              \        Archive the workflow into the given tar archive FILE.\n   \
              \                     The archive will be created such that the workflow\
              \ can\n                        be re-executed on a vanilla system. The\
              \ function needs\n                        conda and git to be installed.\
              \ It will archive every\n                        file that is under\
              \ git version control. Note that it\n                        is best\
              \ practice to have the Snakefile, config files,\n                  \
              \      and scripts under version control. Hence, they will be\n    \
              \                    included in the archive. Further, it will add input\n\
              \                        files that are not generated by by the workflow\
              \ itself\n                        and conda environments. Note that\
              \ symlinks are\n                        dereferenced. Supported formats\
              \ are .tar, .tar.gz,\n                        .tar.bz2 and .tar.xz.\n\
              \  --touch, -t           Touch output files (mark them up to date without\n\
              \                        really changing them) instead of running their\n\
              \                        commands. This is used to pretend that the\
              \ rules were\n                        executed, in order to fool future\
              \ invocations of\n                        snakemake. Fails if a file\
              \ does not yet exist.\n  --keep-going, -k      Go on with independent\
              \ jobs if a job fails.\n  --force, -f           Force the execution\
              \ of the selected target or the\n                        first rule\
              \ regardless of already created output.\n  --forceall, -F        Force\
              \ the execution of the selected (or the first)\n                   \
              \     rule and all rules it is dependent on regardless of\n        \
              \                already created output.\n  --forcerun [TARGET [TARGET\
              \ ...]], -R [TARGET [TARGET ...]]\n                        Force the\
              \ re-execution or creation of the given rules\n                    \
              \    or files. Use this option if you changed a rule and\n         \
              \               want to have all its output in your workflow updated.\n\
              \  --prioritize TARGET [TARGET ...], -P TARGET [TARGET ...]\n      \
              \                  Tell the scheduler to assign creation of given targets\n\
              \                        (and all their dependencies) highest priority.\n\
              \                        (EXPERIMENTAL)\n  --until TARGET [TARGET ...],\
              \ -U TARGET [TARGET ...]\n                        Runs the pipeline\
              \ until it reaches the specified rules\n                        or files.\
              \ Only runs jobs that are dependencies of the\n                    \
              \    specified rule or files, does not run sibling DAGs.\n  --omit-from\
              \ TARGET [TARGET ...], -O TARGET [TARGET ...]\n                    \
              \    Prevent the execution or creation of the given rules\n        \
              \                or files as well as any rules or files that are\n \
              \                       downstream of these targets in the DAG. Also\
              \ runs jobs\n                        in sibling DAGs that are independent\
              \ of the rules or\n                        files specified here.\n \
              \ --allow-ambiguity, -a\n                        Don't check for ambiguous\
              \ rules and simply use the\n                        first if several\
              \ can produce the same file. This\n                        allows the\
              \ user to prioritize rules by their order in\n                     \
              \   the snakefile.\n  --cluster CMD, -c CMD\n                      \
              \  Execute snakemake rules with the given submit command,\n        \
              \                e.g. qsub. Snakemake compiles jobs into scripts that\n\
              \                        are submitted to the cluster with the given\
              \ command,\n                        once all input files for a particular\
              \ job are present.\n                        The submit command can be\
              \ decorated to make it aware\n                        of certain job\
              \ properties (input, output, params,\n                        wildcards,\
              \ log, threads and dependencies (see the\n                        argument\
              \ below)), e.g.: $ snakemake --cluster 'qsub\n                     \
              \   -pe threaded {threads}'.\n  --cluster-sync CMD    cluster submission\
              \ command will block, returning the\n                        remote\
              \ exitstatus upon remote termination (for\n                        example,\
              \ this should be usedif the cluster command is\n                   \
              \     'qsub -sync y' (SGE)\n  --drmaa [ARGS]        Execute snakemake\
              \ on a cluster accessed via DRMAA,\n                        Snakemake\
              \ compiles jobs into scripts that are\n                        submitted\
              \ to the cluster with the given command, once\n                    \
              \    all input files for a particular job are present. ARGS\n      \
              \                  can be used to specify options of the underlying\n\
              \                        cluster system, thereby using the job properties\n\
              \                        input, output, params, wildcards, log, threads\
              \ and\n                        dependencies, e.g.: --drmaa ' -pe threaded\
              \ {threads}'.\n                        Note that ARGS must be given\
              \ in quotes and with a\n                        leading whitespace.\n\
              \  --drmaa-log-dir DIR   Specify a directory in which stdout and stderr\
              \ files\n                        of DRMAA jobs will be written. The\
              \ value may be given\n                        as a relative path, in\
              \ which case Snakemake will use\n                        the current\
              \ invocation directory as the origin. If\n                        given,\
              \ this will override any given '-o' and/or '-e'\n                  \
              \      native specification. If not given, all DRMAA stdout\n      \
              \                  and stderr files are written to the current working\n\
              \                        directory.\n  --cluster-config FILE, -u FILE\n\
              \                        A JSON or YAML file that defines the wildcards\
              \ used in\n                        'cluster'for specific rules, instead\
              \ of having them\n                        specified in the Snakefile.\
              \ For example, for rule\n                        'job' you may define:\
              \ { 'job' : { 'time' : '24:00:00'\n                        } } to specify\
              \ the time for rule 'job'. You can\n                        specify\
              \ more than one file. The configuration files\n                    \
              \    are merged with later values overriding earlier ones.\n  --immediate-submit,\
              \ --is\n                        Immediately submit all jobs to the cluster\
              \ instead of\n                        waiting for present input files.\
              \ This will fail,\n                        unless you make the cluster\
              \ aware of job dependencies,\n                        e.g. via: $ snakemake\
              \ --cluster 'sbatch --dependency\n                        {dependencies}.\
              \ Assuming that your submit script (here\n                        sbatch)\
              \ outputs the generated job id to the first\n                      \
              \  stdout line, {dependencies} will be filled with space\n         \
              \               separated job ids this job depends on.\n  --jobscript\
              \ SCRIPT, --js SCRIPT\n                        Provide a custom job\
              \ script for submission to the\n                        cluster. The\
              \ default script resides as 'jobscript.sh'\n                       \
              \ in the installation directory.\n  --jobname NAME, --jn NAME\n    \
              \                    Provide a custom name for the jobscript that is\n\
              \                        submitted to the cluster (see --cluster). NAME\
              \ is\n                        \"snakejob.{rulename}.{jobid}.sh\" per\
              \ default. The\n                        wildcard {jobid} has to be present\
              \ in the name.\n  --cluster-status CLUSTER_STATUS\n                \
              \        Status command for cluster execution. This is only\n      \
              \                  considered in combination with the --cluster flag.\
              \ If\n                        provided, Snakemake will use the status\
              \ command to\n                        determine if a job has finished\
              \ successfully or\n                        failed. For this it is necessary\
              \ that the submit\n                        command provided to --cluster\
              \ returns the cluster job\n                        id. Then, the status\
              \ command will be invoked with the\n                        job id.\
              \ Snakemake expects it to return 'success' if\n                    \
              \    the job was successfull, 'failed' if the job failed\n         \
              \               and 'running' if the job still runs.\n  --kubernetes\
              \ [NAMESPACE]\n                        Execute workflow in a kubernetes\
              \ cluster (in the\n                        cloud). NAMESPACE is the\
              \ namespace you want to use for\n                        your job (if\
              \ nothing specified: 'default'). Usually,\n                        this\
              \ requires --default-remote-provider and --default-\n              \
              \          remote-prefix to be set to a S3 or GS bucket where\n    \
              \                    your . data shall be stored. It is further advisable\n\
              \                        to activate conda integration via --use-conda.\n\
              \  --kubernetes-env ENVVAR [ENVVAR ...]\n                        Specify\
              \ environment variables to pass to the\n                        kubernetes\
              \ job.\n  --container-image IMAGE\n                        Docker image\
              \ to use, e.g., when submitting jobs to\n                        kubernetes.\
              \ By default, this is\n                        'quay.io/snakemake/snakemake',\
              \ tagged with the same\n                        version as the currently\
              \ running Snakemake instance.\n                        Note that overwriting\
              \ this value is up to your\n                        responsibility.\
              \ Any used image has to contain a\n                        working snakemake\
              \ installation that is compatible with\n                        (or\
              \ ideally the same as) the currently running\n                     \
              \   version.\n  --reason, -r          Print the reason for each executed\
              \ rule.\n  --stats FILE          Write stats about Snakefile execution\
              \ in JSON format\n                        to the given file.\n  --nocolor\
              \             Do not use a colored output.\n  --quiet, -q          \
              \ Do not output any progress or rule information.\n  --nolock      \
              \        Do not lock the working directory\n  --unlock             \
              \ Remove a lock on the working directory.\n  --cleanup-metadata FILE\
              \ [FILE ...], --cm FILE [FILE ...]\n                        Cleanup\
              \ the metadata of given files. That means that\n                   \
              \     snakemake removes any tracked version info, and any\n        \
              \                marks that files are incomplete.\n  --rerun-incomplete,\
              \ --ri\n                        Re-run all jobs the output of which\
              \ is recognized as\n                        incomplete.\n  --ignore-incomplete,\
              \ --ii\n                        Do not check for incomplete output files.\n\
              \  --list-version-changes, --lv\n                        List all output\
              \ files that have been created with a\n                        different\
              \ version (as determined by the version\n                        keyword).\n\
              \  --list-code-changes, --lc\n                        List all output\
              \ files for which the rule body (run or\n                        shell)\
              \ have changed in the Snakefile.\n  --list-input-changes, --li\n   \
              \                     List all output files for which the defined input\n\
              \                        files have changed in the Snakefile (e.g. new\
              \ input\n                        files were added in the rule definition\
              \ or files were\n                        renamed). For listing input\
              \ file modification in the\n                        filesystem, use\
              \ --summary.\n  --list-params-changes, --lp\n                      \
              \  List all output files for which the defined params\n            \
              \            have changed in the Snakefile.\n  --latency-wait SECONDS,\
              \ --output-wait SECONDS, -w SECONDS\n                        Wait given\
              \ seconds if an output file of a job is not\n                      \
              \  present after the job finished. This helps if your\n            \
              \            filesystem suffers from latency (default 5).\n  --wait-for-files\
              \ [FILE [FILE ...]]\n                        Wait --latency-wait seconds\
              \ for these files to be\n                        present before executing\
              \ the workflow. This option is\n                        used internally\
              \ to handle filesystem latency in\n                        cluster environments.\n\
              \  --benchmark-repeats N\n                        Repeat a job N times\
              \ if marked for benchmarking\n                        (default 1).\n\
              \  --notemp, --nt        Ignore temp() declarations. This is useful\
              \ when\n                        running only a part of the workflow,\
              \ since temp()\n                        would lead to deletion of probably\
              \ needed files by\n                        other parts of the workflow.\n\
              \  --keep-remote         Keep local copies of remote input files.\n\
              \  --keep-target-files   Do not adjust the paths of given target files\
              \ relative\n                        to the working directory.\n  --keep-shadow\
              \         Do not delete the shadow directory on snakemake\n        \
              \                startup.\n  --allowed-rules ALLOWED_RULES [ALLOWED_RULES\
              \ ...]\n                        Only consider given rules. If omitted,\
              \ all rules in\n                        Snakefile are used. Note that\
              \ this is intended\n                        primarily for internal use\
              \ and may lead to unexpected\n                        results otherwise.\n\
              \  --max-jobs-per-second MAX_JOBS_PER_SECOND\n                     \
              \   Maximal number of cluster/drmaa jobs per second,\n             \
              \           default is 10, fractions allowed.\n  --max-status-checks-per-second\
              \ MAX_STATUS_CHECKS_PER_SECOND\n                        Maximal number\
              \ of job status checks per second,\n                        default\
              \ is 10, fractions allowed.\n  --restart-times RESTART_TIMES\n     \
              \                   Number of times to restart failing jobs (defaults\
              \ to\n                        0).\n  --attempt ATTEMPT     Internal\
              \ use only: define the initial value of the\n                      \
              \  attempt parameter (default: 1).\n  --timestamp, -T       Add a timestamp\
              \ to all logging output\n  --greediness GREEDINESS\n               \
              \         Set the greediness of scheduling. This value between 0\n \
              \                       and 1 determines how careful jobs are selected\
              \ for\n                        execution. The default value (1.0) provides\
              \ the best\n                        speed and still acceptable scheduling\
              \ quality.\n  --no-hooks            Do not invoke onstart, onsuccess\
              \ or onerror hooks\n                        after execution.\n  --print-compilation\
              \   Print the python representation of the workflow.\n  --overwrite-shellcmd\
              \ OVERWRITE_SHELLCMD\n                        Provide a shell command\
              \ that shall be executed instead\n                        of those given\
              \ in the workflow. This is for debugging\n                        purposes\
              \ only.\n  --verbose             Print debugging output.\n  --debug\
              \               Allow to debug rules with e.g. PDB. This flag allows\n\
              \                        to set breakpoints in run blocks.\n  --runtime-profile\
              \ FILE\n                        Profile Snakemake and write the output\
              \ to FILE. This\n                        requires yappi to be installed.\n\
              \  --mode {0,1,2}        Set execution mode of Snakemake (internal use\
              \ only).\n  --bash-completion     Output code to register bash completion\
              \ for snakemake.\n                        Put the following in your\
              \ .bashrc (including the\n                        accents): `snakemake\
              \ --bash-completion` or issue it in\n                        an open\
              \ terminal session.\n  --use-conda           If defined in the rule,\
              \ run job in a conda\n                        environment. If this flag\
              \ is not set, the conda\n                        directive is ignored.\n\
              \  --conda-prefix DIR    Specify a directory in which the 'conda' and\
              \ 'conda-\n                        archive' directories are created.\
              \ These are used to\n                        store conda environments\
              \ and their archives,\n                        respectively. If not\
              \ supplied, the value is set to the\n                        '.snakemake'\
              \ directory relative to the invocation\n                        directory.\
              \ If supplied, the `--use-conda` flag must\n                       \
              \ also be set. The value may be given as a relative\n              \
              \          path, which will be extrapolated to the invocation\n    \
              \                    directory, or as an absolute path.\n  --create-envs-only\
              \    If specified, only creates the job-specific conda\n           \
              \             environments then exits. The `--use-conda` flag must\n\
              \                        also be set.\n  --use-singularity     If defined\
              \ in the rule, run job within a singularity\n                      \
              \  container. If this flag is not set, the singularity\n           \
              \             directive is ignored.\n  --singularity-prefix DIR\n  \
              \                      Specify a directory in which singularity images\
              \ will\n                        be stored.If not supplied, the value\
              \ is set to the\n                        '.snakemake' directory relative\
              \ to the invocation\n                        directory. If supplied,\
              \ the `--use-singularity` flag\n                        must also be\
              \ set. The value may be given as a relative\n                      \
              \  path, which will be extrapolated to the invocation\n            \
              \            directory, or as an absolute path.\n  --singularity-args\
              \ ARGS\n                        Pass additional args to singularity.\n\
              \  --wrapper-prefix WRAPPER_PREFIX\n                        Prefix for\
              \ URL created from wrapper directive\n                        (default:\
              \ https://bitbucket.org/snakemake/snakemake-\n                     \
              \   wrappers/raw/). Set this to a different URL to use\n           \
              \             your fork or a local clone of the repository.\n  --default-remote-provider\
              \ {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp}\n                        Specify\
              \ default remote provider to be used for all\n                     \
              \   input and output files that don't yet specify one.\n  --default-remote-prefix\
              \ DEFAULT_REMOTE_PREFIX\n                        Specify prefix for\
              \ default remote provider. E.g. a\n                        bucket name.\n\
              \  --no-shared-fs        Do not assume that jobs share a common file\
              \ system.\n                        When this flag is activated, Snakemake\
              \ will assume\n                        that the filesystem on a cluster\
              \ node is not shared\n                        with other nodes. For\
              \ example, this will lead to\n                        downloading remote\
              \ files on each cluster node\n                        separately. Further,\
              \ it won't take special measures to\n                        deal with\
              \ filesystem latency issues. This option will\n                    \
              \    in most cases only make sense in combination with\n           \
              \             --default-remote-provider. Further, when using\n     \
              \                   --cluster you will have to also provide --cluster-\n\
              \                        status. Only activate this if you know what\
              \ you are\n                        doing.\n  --version, -v         show\
              \ program's version number and exit\n"
            generated_using: *id004
          subcommands: []
          usage: []
          help_flag: !Flag
            description: show Snakemake help (or snakemake -h)
            synonyms:
            - --help
            args: !EmptyFlagArg {}
            optional: true
          usage_flag:
          version_flag:
          help_text: "MetaMeta Pipeline (powered by Snakemake)\n\nUsage: /usr/local/bin/metameta\
            \ --configfile FILE [Snakemake options]\n\n Useful Snakemake parameters:\n\
            \   --use-conda            use conda to automatically install pre-configured\
            \ packages\n   -j, --cores            number of cores\n   -k, --keep-going\
            \       go on with independent jobs if a job fails\n   -n, --dryrun  \
            \         do not execute anything\n   -p, --printshellcmds   print out\
            \ the shell commands that will be executed\n   -t, --timestamp  \t\tadd\
            \ a timestamp to all logging output\n\n Full list of parameters:\n   --help\
            \                 show Snakemake help (or snakemake -h)\n\n"
          generated_using: *id004
        - !Command
          command: *id005
          positional: []
          named:
          - !Flag
            description: use conda to automatically install pre-configured packages
            synonyms:
            - --use-conda
            args: !EmptyFlagArg {}
            optional: true
          - !Flag
            description: number of cores
            synonyms:
            - -j
            - --cores
            args: !EmptyFlagArg {}
            optional: true
          - !Flag
            description: go on with independent jobs if a job fails
            synonyms:
            - -k
            - --keep-going
            args: !EmptyFlagArg {}
            optional: true
          - !Flag
            description: do not execute anything
            synonyms:
            - -n
            - --dryrun
            args: !EmptyFlagArg {}
            optional: true
          - !Flag
            description: print out the shell commands that will be executed
            synonyms:
            - -p
            - --printshellcmds
            args: !EmptyFlagArg {}
            optional: true
          - !Flag
            description: add a timestamp to all logging output
            synonyms:
            - -t
            - --timestamp
            args: !EmptyFlagArg {}
            optional: true
          parent: *id003
          subcommands: []
          usage: []
          help_flag: !Flag
            description: show Snakemake help (or snakemake -h)
            synonyms:
            - --help
            args: !EmptyFlagArg {}
            optional: true
          usage_flag:
          version_flag:
          help_text: "MetaMeta Pipeline (powered by Snakemake)\n\nUsage: /usr/local/bin/metameta\
            \ --configfile FILE [Snakemake options]\n\n Useful Snakemake parameters:\n\
            \   --use-conda            use conda to automatically install pre-configured\
            \ packages\n   -j, --cores            number of cores\n   -k, --keep-going\
            \       go on with independent jobs if a job fails\n   -n, --dryrun  \
            \         do not execute anything\n   -p, --printshellcmds   print out\
            \ the shell commands that will be executed\n   -t, --timestamp  \t\tadd\
            \ a timestamp to all logging output\n\n Full list of parameters:\n   --help\
            \                 show Snakemake help (or snakemake -h)\n\n"
          generated_using: *id004
        usage: []
        help_flag:
        usage_flag:
        version_flag:
        help_text: "usage: snakemake [-h] [--profile PROFILE] [--snakefile FILE] [--gui\
          \ [PORT]]\n                 [--cores [N]] [--local-cores N]\n          \
          \       [--resources [NAME=INT [NAME=INT ...]]]\n                 [--config\
          \ [KEY=VALUE [KEY=VALUE ...]]] [--configfile FILE]\n                 [--list]\
          \ [--list-target-rules] [--directory DIR] [--dryrun]\n                 [--printshellcmds]\
          \ [--debug-dag] [--dag]\n                 [--force-use-threads] [--rulegraph]\
          \ [--d3dag] [--summary]\n                 [--detailed-summary] [--archive\
          \ FILE] [--touch]\n                 [--keep-going] [--force] [--forceall]\n\
          \                 [--forcerun [TARGET [TARGET ...]]]\n                 [--prioritize\
          \ TARGET [TARGET ...]]\n                 [--until TARGET [TARGET ...]]\n\
          \                 [--omit-from TARGET [TARGET ...]] [--allow-ambiguity]\n\
          \                 [--cluster CMD | --cluster-sync CMD | --drmaa [ARGS]]\n\
          \                 [--drmaa-log-dir DIR] [--cluster-config FILE]\n      \
          \           [--immediate-submit] [--jobscript SCRIPT] [--jobname NAME]\n\
          \                 [--cluster-status CLUSTER_STATUS] [--kubernetes [NAMESPACE]]\n\
          \                 [--kubernetes-env ENVVAR [ENVVAR ...]]\n             \
          \    [--container-image IMAGE] [--reason] [--stats FILE]\n             \
          \    [--nocolor] [--quiet] [--nolock] [--unlock]\n                 [--cleanup-metadata\
          \ FILE [FILE ...]] [--rerun-incomplete]\n                 [--ignore-incomplete]\
          \ [--list-version-changes]\n                 [--list-code-changes] [--list-input-changes]\n\
          \                 [--list-params-changes] [--latency-wait SECONDS]\n   \
          \              [--wait-for-files [FILE [FILE ...]]] [--benchmark-repeats\
          \ N]\n                 [--notemp] [--keep-remote] [--keep-target-files]\n\
          \                 [--keep-shadow]\n                 [--allowed-rules ALLOWED_RULES\
          \ [ALLOWED_RULES ...]]\n                 [--max-jobs-per-second MAX_JOBS_PER_SECOND]\n\
          \                 [--max-status-checks-per-second MAX_STATUS_CHECKS_PER_SECOND]\n\
          \                 [--restart-times RESTART_TIMES] [--attempt ATTEMPT]\n\
          \                 [--timestamp] [--greediness GREEDINESS] [--no-hooks]\n\
          \                 [--print-compilation]\n                 [--overwrite-shellcmd\
          \ OVERWRITE_SHELLCMD] [--verbose]\n                 [--debug] [--runtime-profile\
          \ FILE] [--mode {0,1,2}]\n                 [--bash-completion] [--use-conda]\
          \ [--conda-prefix DIR]\n                 [--create-envs-only] [--use-singularity]\n\
          \                 [--singularity-prefix DIR] [--singularity-args ARGS]\n\
          \                 [--wrapper-prefix WRAPPER_PREFIX]\n                 [--default-remote-provider\
          \ {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp}]\n                 [--default-remote-prefix\
          \ DEFAULT_REMOTE_PREFIX]\n                 [--no-shared-fs] [--version]\n\
          \                 [target [target ...]]\n\nSnakemake is a Python based language\
          \ and execution environment for GNU Make-\nlike workflows.\n\npositional\
          \ arguments:\n  target                Targets to build. May be rules or\
          \ files.\n\noptional arguments:\n  -h, --help            show this help\
          \ message and exit\n  --profile PROFILE     Name of profile to use for configuring\
          \ Snakemake.\n                        Snakemake will search for a corresponding\
          \ folder in\n                        /etc/xdg/snakemake and /root/.config/snakemake.\n\
          \                        Alternatively, this can be an absolute or relative\n\
          \                        path. The profile folder has to contain a file\n\
          \                        'config.yaml'. This file can be used to set default\n\
          \                        values for command line options in YAML format.\
          \ For\n                        example, '--cluster qsub' becomes 'cluster:\
          \ qsub' in\n                        the YAML file. Profiles can be obtained\
          \ from\n                        https://github.com/snakemake-profiles.\n\
          \  --snakefile FILE, -s FILE\n                        The workflow definition\
          \ in a snakefile.\n  --gui [PORT]          Serve an HTML based user interface\
          \ to the given\n                        network and port e.g. 168.129.10.15:8000.\
          \ By default\n                        Snakemake is only available in the\
          \ local network\n                        (default port: 8000). To make Snakemake\
          \ listen to all\n                        ip addresses add the special host\
          \ address 0.0.0.0 to\n                        the url (0.0.0.0:8000). This\
          \ is important if Snakemake\n                        is used in a virtualised\
          \ environment like Docker. If\n                        possible, a browser\
          \ window is opened.\n  --cores [N], --jobs [N], -j [N]\n               \
          \         Use at most N cores in parallel (default: 1). If N is\n      \
          \                  omitted, the limit is set to the number of available\n\
          \                        cores.\n  --local-cores N       In cluster mode,\
          \ use at most N cores of the host\n                        machine in parallel\
          \ (default: number of CPU cores of\n                        the host). The\
          \ cores are used to execute local rules.\n                        This option\
          \ is ignored when not in cluster mode.\n  --resources [NAME=INT [NAME=INT\
          \ ...]], --res [NAME=INT [NAME=INT ...]]\n                        Define\
          \ additional resources that shall constrain the\n                      \
          \  scheduling analogously to threads (see above). A\n                  \
          \      resource is defined as a name and an integer value.\n           \
          \             E.g. --resources gpu=1. Rules can use resources by\n     \
          \                   defining the resource keyword, e.g. resources: gpu=1.\n\
          \                        If now two rules require 1 of the resource 'gpu'\
          \ they\n                        won't be run in parallel by the scheduler.\n\
          \  --config [KEY=VALUE [KEY=VALUE ...]], -C [KEY=VALUE [KEY=VALUE ...]]\n\
          \                        Set or overwrite values in the workflow config\
          \ object.\n                        The workflow config object is accessible\
          \ as variable\n                        config inside the workflow. Default\
          \ values can be set\n                        by providing a JSON file (see\
          \ Documentation).\n  --configfile FILE     Specify or overwrite the config\
          \ file of the workflow\n                        (see the docs). Values specified\
          \ in JSON or YAML\n                        format are available in the global\
          \ config dictionary\n                        inside the workflow.\n  --list,\
          \ -l            Show availiable rules in given Snakefile.\n  --list-target-rules,\
          \ --lt\n                        Show available target rules in given Snakefile.\n\
          \  --directory DIR, -d DIR\n                        Specify working directory\
          \ (relative paths in the\n                        snakefile will use this\
          \ as their origin).\n  --dryrun, -n          Do not execute anything.\n\
          \  --printshellcmds, -p  Print out the shell commands that will be executed.\n\
          \  --debug-dag           Print candidate and selected jobs (including their\n\
          \                        wildcards) while inferring DAG. This can help to\
          \ debug\n                        unexpected DAG topology or errors.\n  --dag\
          \                 Do not execute anything and print the directed acyclic\n\
          \                        graph of jobs in the dot language. Recommended\
          \ use on\n                        Unix systems: snakemake --dag | dot |\
          \ display\n  --force-use-threads   Force threads rather than processes.\
          \ Helpful if shared\n                        memory (/dev/shm) is full or\
          \ unavailable.\n  --rulegraph           Do not execute anything and print\
          \ the dependency graph\n                        of rules in the dot language.\
          \ This will be less\n                        crowded than above DAG of jobs,\
          \ but also show less\n                        information. Note that each\
          \ rule is displayed once,\n                        hence the displayed graph\
          \ will be cyclic if a rule\n                        appears in several steps\
          \ of the workflow. Use this if\n                        above option leads\
          \ to a DAG that is too large.\n                        Recommended use on\
          \ Unix systems: snakemake --rulegraph\n                        | dot | display\n\
          \  --d3dag               Print the DAG in D3.js compatible JSON format.\n\
          \  --summary, -S         Print a summary of all files created by the workflow.\n\
          \                        The has the following columns: filename, modification\n\
          \                        time, rule version, status, plan. Thereby rule\
          \ version\n                        contains the versionthe file was created\
          \ with (see the\n                        version keyword of rules), and\
          \ status denotes whether\n                        the file is missing, its\
          \ input files are newer or if\n                        version or implementation\
          \ of the rule changed since\n                        file creation. Finally\
          \ the last column denotes whether\n                        the file will\
          \ be updated or created during the next\n                        workflow\
          \ execution.\n  --detailed-summary, -D\n                        Print a\
          \ summary of all files created by the workflow.\n                      \
          \  The has the following columns: filename, modification\n             \
          \           time, rule version, input file(s), shell command,\n        \
          \                status, plan. Thereby rule version contains the\n     \
          \                   versionthe file was created with (see the version\n\
          \                        keyword of rules), and status denotes whether the\
          \ file\n                        is missing, its input files are newer or\
          \ if version or\n                        implementation of the rule changed\
          \ since file\n                        creation. The input file and shell\
          \ command columns are\n                        selfexplanatory. Finally\
          \ the last column denotes\n                        whether the file will\
          \ be updated or created during the\n                        next workflow\
          \ execution.\n  --archive FILE        Archive the workflow into the given\
          \ tar archive FILE.\n                        The archive will be created\
          \ such that the workflow can\n                        be re-executed on\
          \ a vanilla system. The function needs\n                        conda and\
          \ git to be installed. It will archive every\n                        file\
          \ that is under git version control. Note that it\n                    \
          \    is best practice to have the Snakefile, config files,\n           \
          \             and scripts under version control. Hence, they will be\n \
          \                       included in the archive. Further, it will add input\n\
          \                        files that are not generated by by the workflow\
          \ itself\n                        and conda environments. Note that symlinks\
          \ are\n                        dereferenced. Supported formats are .tar,\
          \ .tar.gz,\n                        .tar.bz2 and .tar.xz.\n  --touch, -t\
          \           Touch output files (mark them up to date without\n         \
          \               really changing them) instead of running their\n       \
          \                 commands. This is used to pretend that the rules were\n\
          \                        executed, in order to fool future invocations of\n\
          \                        snakemake. Fails if a file does not yet exist.\n\
          \  --keep-going, -k      Go on with independent jobs if a job fails.\n \
          \ --force, -f           Force the execution of the selected target or the\n\
          \                        first rule regardless of already created output.\n\
          \  --forceall, -F        Force the execution of the selected (or the first)\n\
          \                        rule and all rules it is dependent on regardless\
          \ of\n                        already created output.\n  --forcerun [TARGET\
          \ [TARGET ...]], -R [TARGET [TARGET ...]]\n                        Force\
          \ the re-execution or creation of the given rules\n                    \
          \    or files. Use this option if you changed a rule and\n             \
          \           want to have all its output in your workflow updated.\n  --prioritize\
          \ TARGET [TARGET ...], -P TARGET [TARGET ...]\n                        Tell\
          \ the scheduler to assign creation of given targets\n                  \
          \      (and all their dependencies) highest priority.\n                \
          \        (EXPERIMENTAL)\n  --until TARGET [TARGET ...], -U TARGET [TARGET\
          \ ...]\n                        Runs the pipeline until it reaches the specified\
          \ rules\n                        or files. Only runs jobs that are dependencies\
          \ of the\n                        specified rule or files, does not run\
          \ sibling DAGs.\n  --omit-from TARGET [TARGET ...], -O TARGET [TARGET ...]\n\
          \                        Prevent the execution or creation of the given\
          \ rules\n                        or files as well as any rules or files\
          \ that are\n                        downstream of these targets in the DAG.\
          \ Also runs jobs\n                        in sibling DAGs that are independent\
          \ of the rules or\n                        files specified here.\n  --allow-ambiguity,\
          \ -a\n                        Don't check for ambiguous rules and simply\
          \ use the\n                        first if several can produce the same\
          \ file. This\n                        allows the user to prioritize rules\
          \ by their order in\n                        the snakefile.\n  --cluster\
          \ CMD, -c CMD\n                        Execute snakemake rules with the\
          \ given submit command,\n                        e.g. qsub. Snakemake compiles\
          \ jobs into scripts that\n                        are submitted to the cluster\
          \ with the given command,\n                        once all input files\
          \ for a particular job are present.\n                        The submit\
          \ command can be decorated to make it aware\n                        of\
          \ certain job properties (input, output, params,\n                     \
          \   wildcards, log, threads and dependencies (see the\n                \
          \        argument below)), e.g.: $ snakemake --cluster 'qsub\n         \
          \               -pe threaded {threads}'.\n  --cluster-sync CMD    cluster\
          \ submission command will block, returning the\n                       \
          \ remote exitstatus upon remote termination (for\n                     \
          \   example, this should be usedif the cluster command is\n            \
          \            'qsub -sync y' (SGE)\n  --drmaa [ARGS]        Execute snakemake\
          \ on a cluster accessed via DRMAA,\n                        Snakemake compiles\
          \ jobs into scripts that are\n                        submitted to the cluster\
          \ with the given command, once\n                        all input files\
          \ for a particular job are present. ARGS\n                        can be\
          \ used to specify options of the underlying\n                        cluster\
          \ system, thereby using the job properties\n                        input,\
          \ output, params, wildcards, log, threads and\n                        dependencies,\
          \ e.g.: --drmaa ' -pe threaded {threads}'.\n                        Note\
          \ that ARGS must be given in quotes and with a\n                       \
          \ leading whitespace.\n  --drmaa-log-dir DIR   Specify a directory in which\
          \ stdout and stderr files\n                        of DRMAA jobs will be\
          \ written. The value may be given\n                        as a relative\
          \ path, in which case Snakemake will use\n                        the current\
          \ invocation directory as the origin. If\n                        given,\
          \ this will override any given '-o' and/or '-e'\n                      \
          \  native specification. If not given, all DRMAA stdout\n              \
          \          and stderr files are written to the current working\n       \
          \                 directory.\n  --cluster-config FILE, -u FILE\n       \
          \                 A JSON or YAML file that defines the wildcards used in\n\
          \                        'cluster'for specific rules, instead of having\
          \ them\n                        specified in the Snakefile. For example,\
          \ for rule\n                        'job' you may define: { 'job' : { 'time'\
          \ : '24:00:00'\n                        } } to specify the time for rule\
          \ 'job'. You can\n                        specify more than one file. The\
          \ configuration files\n                        are merged with later values\
          \ overriding earlier ones.\n  --immediate-submit, --is\n               \
          \         Immediately submit all jobs to the cluster instead of\n      \
          \                  waiting for present input files. This will fail,\n  \
          \                      unless you make the cluster aware of job dependencies,\n\
          \                        e.g. via: $ snakemake --cluster 'sbatch --dependency\n\
          \                        {dependencies}. Assuming that your submit script\
          \ (here\n                        sbatch) outputs the generated job id to\
          \ the first\n                        stdout line, {dependencies} will be\
          \ filled with space\n                        separated job ids this job\
          \ depends on.\n  --jobscript SCRIPT, --js SCRIPT\n                     \
          \   Provide a custom job script for submission to the\n                \
          \        cluster. The default script resides as 'jobscript.sh'\n       \
          \                 in the installation directory.\n  --jobname NAME, --jn\
          \ NAME\n                        Provide a custom name for the jobscript\
          \ that is\n                        submitted to the cluster (see --cluster).\
          \ NAME is\n                        \"snakejob.{rulename}.{jobid}.sh\" per\
          \ default. The\n                        wildcard {jobid} has to be present\
          \ in the name.\n  --cluster-status CLUSTER_STATUS\n                    \
          \    Status command for cluster execution. This is only\n              \
          \          considered in combination with the --cluster flag. If\n     \
          \                   provided, Snakemake will use the status command to\n\
          \                        determine if a job has finished successfully or\n\
          \                        failed. For this it is necessary that the submit\n\
          \                        command provided to --cluster returns the cluster\
          \ job\n                        id. Then, the status command will be invoked\
          \ with the\n                        job id. Snakemake expects it to return\
          \ 'success' if\n                        the job was successfull, 'failed'\
          \ if the job failed\n                        and 'running' if the job still\
          \ runs.\n  --kubernetes [NAMESPACE]\n                        Execute workflow\
          \ in a kubernetes cluster (in the\n                        cloud). NAMESPACE\
          \ is the namespace you want to use for\n                        your job\
          \ (if nothing specified: 'default'). Usually,\n                        this\
          \ requires --default-remote-provider and --default-\n                  \
          \      remote-prefix to be set to a S3 or GS bucket where\n            \
          \            your . data shall be stored. It is further advisable\n    \
          \                    to activate conda integration via --use-conda.\n  --kubernetes-env\
          \ ENVVAR [ENVVAR ...]\n                        Specify environment variables\
          \ to pass to the\n                        kubernetes job.\n  --container-image\
          \ IMAGE\n                        Docker image to use, e.g., when submitting\
          \ jobs to\n                        kubernetes. By default, this is\n   \
          \                     'quay.io/snakemake/snakemake', tagged with the same\n\
          \                        version as the currently running Snakemake instance.\n\
          \                        Note that overwriting this value is up to your\n\
          \                        responsibility. Any used image has to contain a\n\
          \                        working snakemake installation that is compatible\
          \ with\n                        (or ideally the same as) the currently running\n\
          \                        version.\n  --reason, -r          Print the reason\
          \ for each executed rule.\n  --stats FILE          Write stats about Snakefile\
          \ execution in JSON format\n                        to the given file.\n\
          \  --nocolor             Do not use a colored output.\n  --quiet, -q   \
          \        Do not output any progress or rule information.\n  --nolock   \
          \           Do not lock the working directory\n  --unlock              Remove\
          \ a lock on the working directory.\n  --cleanup-metadata FILE [FILE ...],\
          \ --cm FILE [FILE ...]\n                        Cleanup the metadata of\
          \ given files. That means that\n                        snakemake removes\
          \ any tracked version info, and any\n                        marks that\
          \ files are incomplete.\n  --rerun-incomplete, --ri\n                  \
          \      Re-run all jobs the output of which is recognized as\n          \
          \              incomplete.\n  --ignore-incomplete, --ii\n              \
          \          Do not check for incomplete output files.\n  --list-version-changes,\
          \ --lv\n                        List all output files that have been created\
          \ with a\n                        different version (as determined by the\
          \ version\n                        keyword).\n  --list-code-changes, --lc\n\
          \                        List all output files for which the rule body (run\
          \ or\n                        shell) have changed in the Snakefile.\n  --list-input-changes,\
          \ --li\n                        List all output files for which the defined\
          \ input\n                        files have changed in the Snakefile (e.g.\
          \ new input\n                        files were added in the rule definition\
          \ or files were\n                        renamed). For listing input file\
          \ modification in the\n                        filesystem, use --summary.\n\
          \  --list-params-changes, --lp\n                        List all output\
          \ files for which the defined params\n                        have changed\
          \ in the Snakefile.\n  --latency-wait SECONDS, --output-wait SECONDS, -w\
          \ SECONDS\n                        Wait given seconds if an output file\
          \ of a job is not\n                        present after the job finished.\
          \ This helps if your\n                        filesystem suffers from latency\
          \ (default 5).\n  --wait-for-files [FILE [FILE ...]]\n                 \
          \       Wait --latency-wait seconds for these files to be\n            \
          \            present before executing the workflow. This option is\n   \
          \                     used internally to handle filesystem latency in\n\
          \                        cluster environments.\n  --benchmark-repeats N\n\
          \                        Repeat a job N times if marked for benchmarking\n\
          \                        (default 1).\n  --notemp, --nt        Ignore temp()\
          \ declarations. This is useful when\n                        running only\
          \ a part of the workflow, since temp()\n                        would lead\
          \ to deletion of probably needed files by\n                        other\
          \ parts of the workflow.\n  --keep-remote         Keep local copies of remote\
          \ input files.\n  --keep-target-files   Do not adjust the paths of given\
          \ target files relative\n                        to the working directory.\n\
          \  --keep-shadow         Do not delete the shadow directory on snakemake\n\
          \                        startup.\n  --allowed-rules ALLOWED_RULES [ALLOWED_RULES\
          \ ...]\n                        Only consider given rules. If omitted, all\
          \ rules in\n                        Snakefile are used. Note that this is\
          \ intended\n                        primarily for internal use and may lead\
          \ to unexpected\n                        results otherwise.\n  --max-jobs-per-second\
          \ MAX_JOBS_PER_SECOND\n                        Maximal number of cluster/drmaa\
          \ jobs per second,\n                        default is 10, fractions allowed.\n\
          \  --max-status-checks-per-second MAX_STATUS_CHECKS_PER_SECOND\n       \
          \                 Maximal number of job status checks per second,\n    \
          \                    default is 10, fractions allowed.\n  --restart-times\
          \ RESTART_TIMES\n                        Number of times to restart failing\
          \ jobs (defaults to\n                        0).\n  --attempt ATTEMPT  \
          \   Internal use only: define the initial value of the\n               \
          \         attempt parameter (default: 1).\n  --timestamp, -T       Add a\
          \ timestamp to all logging output\n  --greediness GREEDINESS\n         \
          \               Set the greediness of scheduling. This value between 0\n\
          \                        and 1 determines how careful jobs are selected\
          \ for\n                        execution. The default value (1.0) provides\
          \ the best\n                        speed and still acceptable scheduling\
          \ quality.\n  --no-hooks            Do not invoke onstart, onsuccess or\
          \ onerror hooks\n                        after execution.\n  --print-compilation\
          \   Print the python representation of the workflow.\n  --overwrite-shellcmd\
          \ OVERWRITE_SHELLCMD\n                        Provide a shell command that\
          \ shall be executed instead\n                        of those given in the\
          \ workflow. This is for debugging\n                        purposes only.\n\
          \  --verbose             Print debugging output.\n  --debug            \
          \   Allow to debug rules with e.g. PDB. This flag allows\n             \
          \           to set breakpoints in run blocks.\n  --runtime-profile FILE\n\
          \                        Profile Snakemake and write the output to FILE.\
          \ This\n                        requires yappi to be installed.\n  --mode\
          \ {0,1,2}        Set execution mode of Snakemake (internal use only).\n\
          \  --bash-completion     Output code to register bash completion for snakemake.\n\
          \                        Put the following in your .bashrc (including the\n\
          \                        accents): `snakemake --bash-completion` or issue\
          \ it in\n                        an open terminal session.\n  --use-conda\
          \           If defined in the rule, run job in a conda\n               \
          \         environment. If this flag is not set, the conda\n            \
          \            directive is ignored.\n  --conda-prefix DIR    Specify a directory\
          \ in which the 'conda' and 'conda-\n                        archive' directories\
          \ are created. These are used to\n                        store conda environments\
          \ and their archives,\n                        respectively. If not supplied,\
          \ the value is set to the\n                        '.snakemake' directory\
          \ relative to the invocation\n                        directory. If supplied,\
          \ the `--use-conda` flag must\n                        also be set. The\
          \ value may be given as a relative\n                        path, which\
          \ will be extrapolated to the invocation\n                        directory,\
          \ or as an absolute path.\n  --create-envs-only    If specified, only creates\
          \ the job-specific conda\n                        environments then exits.\
          \ The `--use-conda` flag must\n                        also be set.\n  --use-singularity\
          \     If defined in the rule, run job within a singularity\n           \
          \             container. If this flag is not set, the singularity\n    \
          \                    directive is ignored.\n  --singularity-prefix DIR\n\
          \                        Specify a directory in which singularity images\
          \ will\n                        be stored.If not supplied, the value is\
          \ set to the\n                        '.snakemake' directory relative to\
          \ the invocation\n                        directory. If supplied, the `--use-singularity`\
          \ flag\n                        must also be set. The value may be given\
          \ as a relative\n                        path, which will be extrapolated\
          \ to the invocation\n                        directory, or as an absolute\
          \ path.\n  --singularity-args ARGS\n                        Pass additional\
          \ args to singularity.\n  --wrapper-prefix WRAPPER_PREFIX\n            \
          \            Prefix for URL created from wrapper directive\n           \
          \             (default: https://bitbucket.org/snakemake/snakemake-\n   \
          \                     wrappers/raw/). Set this to a different URL to use\n\
          \                        your fork or a local clone of the repository.\n\
          \  --default-remote-provider {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp}\n  \
          \                      Specify default remote provider to be used for all\n\
          \                        input and output files that don't yet specify one.\n\
          \  --default-remote-prefix DEFAULT_REMOTE_PREFIX\n                     \
          \   Specify prefix for default remote provider. E.g. a\n               \
          \         bucket name.\n  --no-shared-fs        Do not assume that jobs\
          \ share a common file system.\n                        When this flag is\
          \ activated, Snakemake will assume\n                        that the filesystem\
          \ on a cluster node is not shared\n                        with other nodes.\
          \ For example, this will lead to\n                        downloading remote\
          \ files on each cluster node\n                        separately. Further,\
          \ it won't take special measures to\n                        deal with filesystem\
          \ latency issues. This option will\n                        in most cases\
          \ only make sense in combination with\n                        --default-remote-provider.\
          \ Further, when using\n                        --cluster you will have to\
          \ also provide --cluster-\n                        status. Only activate\
          \ this if you know what you are\n                        doing.\n  --version,\
          \ -v         show program's version number and exit\n"
        generated_using: *id004
      subcommands: []
      usage: []
      help_flag: !Flag
        description: show Snakemake help (or snakemake -h)
        synonyms:
        - --help
        args: !EmptyFlagArg {}
        optional: true
      usage_flag:
      version_flag:
      help_text: "MetaMeta Pipeline (powered by Snakemake)\n\nUsage: /usr/local/bin/metameta\
        \ --configfile FILE [Snakemake options]\n\n Useful Snakemake parameters:\n\
        \   --use-conda            use conda to automatically install pre-configured\
        \ packages\n   -j, --cores            number of cores\n   -k, --keep-going\
        \       go on with independent jobs if a job fails\n   -n, --dryrun      \
        \     do not execute anything\n   -p, --printshellcmds   print out the shell\
        \ commands that will be executed\n   -t, --timestamp  \t\tadd a timestamp\
        \ to all logging output\n\n Full list of parameters:\n   --help          \
        \       show Snakemake help (or snakemake -h)\n\n"
      generated_using: *id004
    - !Command
      command: *id005
      positional: []
      named:
      - !Flag
        description: use conda to automatically install pre-configured packages
        synonyms:
        - --use-conda
        args: !EmptyFlagArg {}
        optional: true
      - !Flag
        description: number of cores
        synonyms:
        - -j
        - --cores
        args: !EmptyFlagArg {}
        optional: true
      - !Flag
        description: go on with independent jobs if a job fails
        synonyms:
        - -k
        - --keep-going
        args: !EmptyFlagArg {}
        optional: true
      - !Flag
        description: do not execute anything
        synonyms:
        - -n
        - --dryrun
        args: !EmptyFlagArg {}
        optional: true
      - !Flag
        description: print out the shell commands that will be executed
        synonyms:
        - -p
        - --printshellcmds
        args: !EmptyFlagArg {}
        optional: true
      - !Flag
        description: add a timestamp to all logging output
        synonyms:
        - -t
        - --timestamp
        args: !EmptyFlagArg {}
        optional: true
      parent: *id006
      subcommands: []
      usage: []
      help_flag: !Flag
        description: show Snakemake help (or snakemake -h)
        synonyms:
        - --help
        args: !EmptyFlagArg {}
        optional: true
      usage_flag:
      version_flag:
      help_text: "MetaMeta Pipeline (powered by Snakemake)\n\nUsage: /usr/local/bin/metameta\
        \ --configfile FILE [Snakemake options]\n\n Useful Snakemake parameters:\n\
        \   --use-conda            use conda to automatically install pre-configured\
        \ packages\n   -j, --cores            number of cores\n   -k, --keep-going\
        \       go on with independent jobs if a job fails\n   -n, --dryrun      \
        \     do not execute anything\n   -p, --printshellcmds   print out the shell\
        \ commands that will be executed\n   -t, --timestamp  \t\tadd a timestamp\
        \ to all logging output\n\n Full list of parameters:\n   --help          \
        \       show Snakemake help (or snakemake -h)\n\n"
      generated_using: *id004
    usage: []
    help_flag:
    usage_flag:
    version_flag:
    help_text: "usage: snakemake [-h] [--profile PROFILE] [--snakefile FILE] [--gui\
      \ [PORT]]\n                 [--cores [N]] [--local-cores N]\n              \
      \   [--resources [NAME=INT [NAME=INT ...]]]\n                 [--config [KEY=VALUE\
      \ [KEY=VALUE ...]]] [--configfile FILE]\n                 [--list] [--list-target-rules]\
      \ [--directory DIR] [--dryrun]\n                 [--printshellcmds] [--debug-dag]\
      \ [--dag]\n                 [--force-use-threads] [--rulegraph] [--d3dag] [--summary]\n\
      \                 [--detailed-summary] [--archive FILE] [--touch]\n        \
      \         [--keep-going] [--force] [--forceall]\n                 [--forcerun\
      \ [TARGET [TARGET ...]]]\n                 [--prioritize TARGET [TARGET ...]]\n\
      \                 [--until TARGET [TARGET ...]]\n                 [--omit-from\
      \ TARGET [TARGET ...]] [--allow-ambiguity]\n                 [--cluster CMD\
      \ | --cluster-sync CMD | --drmaa [ARGS]]\n                 [--drmaa-log-dir\
      \ DIR] [--cluster-config FILE]\n                 [--immediate-submit] [--jobscript\
      \ SCRIPT] [--jobname NAME]\n                 [--cluster-status CLUSTER_STATUS]\
      \ [--kubernetes [NAMESPACE]]\n                 [--kubernetes-env ENVVAR [ENVVAR\
      \ ...]]\n                 [--container-image IMAGE] [--reason] [--stats FILE]\n\
      \                 [--nocolor] [--quiet] [--nolock] [--unlock]\n            \
      \     [--cleanup-metadata FILE [FILE ...]] [--rerun-incomplete]\n          \
      \       [--ignore-incomplete] [--list-version-changes]\n                 [--list-code-changes]\
      \ [--list-input-changes]\n                 [--list-params-changes] [--latency-wait\
      \ SECONDS]\n                 [--wait-for-files [FILE [FILE ...]]] [--benchmark-repeats\
      \ N]\n                 [--notemp] [--keep-remote] [--keep-target-files]\n  \
      \               [--keep-shadow]\n                 [--allowed-rules ALLOWED_RULES\
      \ [ALLOWED_RULES ...]]\n                 [--max-jobs-per-second MAX_JOBS_PER_SECOND]\n\
      \                 [--max-status-checks-per-second MAX_STATUS_CHECKS_PER_SECOND]\n\
      \                 [--restart-times RESTART_TIMES] [--attempt ATTEMPT]\n    \
      \             [--timestamp] [--greediness GREEDINESS] [--no-hooks]\n       \
      \          [--print-compilation]\n                 [--overwrite-shellcmd OVERWRITE_SHELLCMD]\
      \ [--verbose]\n                 [--debug] [--runtime-profile FILE] [--mode {0,1,2}]\n\
      \                 [--bash-completion] [--use-conda] [--conda-prefix DIR]\n \
      \                [--create-envs-only] [--use-singularity]\n                \
      \ [--singularity-prefix DIR] [--singularity-args ARGS]\n                 [--wrapper-prefix\
      \ WRAPPER_PREFIX]\n                 [--default-remote-provider {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp}]\n\
      \                 [--default-remote-prefix DEFAULT_REMOTE_PREFIX]\n        \
      \         [--no-shared-fs] [--version]\n                 [target [target ...]]\n\
      \nSnakemake is a Python based language and execution environment for GNU Make-\n\
      like workflows.\n\npositional arguments:\n  target                Targets to\
      \ build. May be rules or files.\n\noptional arguments:\n  -h, --help       \
      \     show this help message and exit\n  --profile PROFILE     Name of profile\
      \ to use for configuring Snakemake.\n                        Snakemake will\
      \ search for a corresponding folder in\n                        /etc/xdg/snakemake\
      \ and /root/.config/snakemake.\n                        Alternatively, this\
      \ can be an absolute or relative\n                        path. The profile\
      \ folder has to contain a file\n                        'config.yaml'. This\
      \ file can be used to set default\n                        values for command\
      \ line options in YAML format. For\n                        example, '--cluster\
      \ qsub' becomes 'cluster: qsub' in\n                        the YAML file. Profiles\
      \ can be obtained from\n                        https://github.com/snakemake-profiles.\n\
      \  --snakefile FILE, -s FILE\n                        The workflow definition\
      \ in a snakefile.\n  --gui [PORT]          Serve an HTML based user interface\
      \ to the given\n                        network and port e.g. 168.129.10.15:8000.\
      \ By default\n                        Snakemake is only available in the local\
      \ network\n                        (default port: 8000). To make Snakemake listen\
      \ to all\n                        ip addresses add the special host address\
      \ 0.0.0.0 to\n                        the url (0.0.0.0:8000). This is important\
      \ if Snakemake\n                        is used in a virtualised environment\
      \ like Docker. If\n                        possible, a browser window is opened.\n\
      \  --cores [N], --jobs [N], -j [N]\n                        Use at most N cores\
      \ in parallel (default: 1). If N is\n                        omitted, the limit\
      \ is set to the number of available\n                        cores.\n  --local-cores\
      \ N       In cluster mode, use at most N cores of the host\n               \
      \         machine in parallel (default: number of CPU cores of\n           \
      \             the host). The cores are used to execute local rules.\n      \
      \                  This option is ignored when not in cluster mode.\n  --resources\
      \ [NAME=INT [NAME=INT ...]], --res [NAME=INT [NAME=INT ...]]\n             \
      \           Define additional resources that shall constrain the\n         \
      \               scheduling analogously to threads (see above). A\n         \
      \               resource is defined as a name and an integer value.\n      \
      \                  E.g. --resources gpu=1. Rules can use resources by\n    \
      \                    defining the resource keyword, e.g. resources: gpu=1.\n\
      \                        If now two rules require 1 of the resource 'gpu' they\n\
      \                        won't be run in parallel by the scheduler.\n  --config\
      \ [KEY=VALUE [KEY=VALUE ...]], -C [KEY=VALUE [KEY=VALUE ...]]\n            \
      \            Set or overwrite values in the workflow config object.\n      \
      \                  The workflow config object is accessible as variable\n  \
      \                      config inside the workflow. Default values can be set\n\
      \                        by providing a JSON file (see Documentation).\n  --configfile\
      \ FILE     Specify or overwrite the config file of the workflow\n          \
      \              (see the docs). Values specified in JSON or YAML\n          \
      \              format are available in the global config dictionary\n      \
      \                  inside the workflow.\n  --list, -l            Show availiable\
      \ rules in given Snakefile.\n  --list-target-rules, --lt\n                 \
      \       Show available target rules in given Snakefile.\n  --directory DIR,\
      \ -d DIR\n                        Specify working directory (relative paths\
      \ in the\n                        snakefile will use this as their origin).\n\
      \  --dryrun, -n          Do not execute anything.\n  --printshellcmds, -p  Print\
      \ out the shell commands that will be executed.\n  --debug-dag           Print\
      \ candidate and selected jobs (including their\n                        wildcards)\
      \ while inferring DAG. This can help to debug\n                        unexpected\
      \ DAG topology or errors.\n  --dag                 Do not execute anything and\
      \ print the directed acyclic\n                        graph of jobs in the dot\
      \ language. Recommended use on\n                        Unix systems: snakemake\
      \ --dag | dot | display\n  --force-use-threads   Force threads rather than processes.\
      \ Helpful if shared\n                        memory (/dev/shm) is full or unavailable.\n\
      \  --rulegraph           Do not execute anything and print the dependency graph\n\
      \                        of rules in the dot language. This will be less\n \
      \                       crowded than above DAG of jobs, but also show less\n\
      \                        information. Note that each rule is displayed once,\n\
      \                        hence the displayed graph will be cyclic if a rule\n\
      \                        appears in several steps of the workflow. Use this\
      \ if\n                        above option leads to a DAG that is too large.\n\
      \                        Recommended use on Unix systems: snakemake --rulegraph\n\
      \                        | dot | display\n  --d3dag               Print the\
      \ DAG in D3.js compatible JSON format.\n  --summary, -S         Print a summary\
      \ of all files created by the workflow.\n                        The has the\
      \ following columns: filename, modification\n                        time, rule\
      \ version, status, plan. Thereby rule version\n                        contains\
      \ the versionthe file was created with (see the\n                        version\
      \ keyword of rules), and status denotes whether\n                        the\
      \ file is missing, its input files are newer or if\n                       \
      \ version or implementation of the rule changed since\n                    \
      \    file creation. Finally the last column denotes whether\n              \
      \          the file will be updated or created during the next\n           \
      \             workflow execution.\n  --detailed-summary, -D\n              \
      \          Print a summary of all files created by the workflow.\n         \
      \               The has the following columns: filename, modification\n    \
      \                    time, rule version, input file(s), shell command,\n   \
      \                     status, plan. Thereby rule version contains the\n    \
      \                    versionthe file was created with (see the version\n   \
      \                     keyword of rules), and status denotes whether the file\n\
      \                        is missing, its input files are newer or if version\
      \ or\n                        implementation of the rule changed since file\n\
      \                        creation. The input file and shell command columns\
      \ are\n                        selfexplanatory. Finally the last column denotes\n\
      \                        whether the file will be updated or created during\
      \ the\n                        next workflow execution.\n  --archive FILE  \
      \      Archive the workflow into the given tar archive FILE.\n             \
      \           The archive will be created such that the workflow can\n       \
      \                 be re-executed on a vanilla system. The function needs\n \
      \                       conda and git to be installed. It will archive every\n\
      \                        file that is under git version control. Note that it\n\
      \                        is best practice to have the Snakefile, config files,\n\
      \                        and scripts under version control. Hence, they will\
      \ be\n                        included in the archive. Further, it will add\
      \ input\n                        files that are not generated by by the workflow\
      \ itself\n                        and conda environments. Note that symlinks\
      \ are\n                        dereferenced. Supported formats are .tar, .tar.gz,\n\
      \                        .tar.bz2 and .tar.xz.\n  --touch, -t           Touch\
      \ output files (mark them up to date without\n                        really\
      \ changing them) instead of running their\n                        commands.\
      \ This is used to pretend that the rules were\n                        executed,\
      \ in order to fool future invocations of\n                        snakemake.\
      \ Fails if a file does not yet exist.\n  --keep-going, -k      Go on with independent\
      \ jobs if a job fails.\n  --force, -f           Force the execution of the selected\
      \ target or the\n                        first rule regardless of already created\
      \ output.\n  --forceall, -F        Force the execution of the selected (or the\
      \ first)\n                        rule and all rules it is dependent on regardless\
      \ of\n                        already created output.\n  --forcerun [TARGET\
      \ [TARGET ...]], -R [TARGET [TARGET ...]]\n                        Force the\
      \ re-execution or creation of the given rules\n                        or files.\
      \ Use this option if you changed a rule and\n                        want to\
      \ have all its output in your workflow updated.\n  --prioritize TARGET [TARGET\
      \ ...], -P TARGET [TARGET ...]\n                        Tell the scheduler to\
      \ assign creation of given targets\n                        (and all their dependencies)\
      \ highest priority.\n                        (EXPERIMENTAL)\n  --until TARGET\
      \ [TARGET ...], -U TARGET [TARGET ...]\n                        Runs the pipeline\
      \ until it reaches the specified rules\n                        or files. Only\
      \ runs jobs that are dependencies of the\n                        specified\
      \ rule or files, does not run sibling DAGs.\n  --omit-from TARGET [TARGET ...],\
      \ -O TARGET [TARGET ...]\n                        Prevent the execution or creation\
      \ of the given rules\n                        or files as well as any rules\
      \ or files that are\n                        downstream of these targets in\
      \ the DAG. Also runs jobs\n                        in sibling DAGs that are\
      \ independent of the rules or\n                        files specified here.\n\
      \  --allow-ambiguity, -a\n                        Don't check for ambiguous\
      \ rules and simply use the\n                        first if several can produce\
      \ the same file. This\n                        allows the user to prioritize\
      \ rules by their order in\n                        the snakefile.\n  --cluster\
      \ CMD, -c CMD\n                        Execute snakemake rules with the given\
      \ submit command,\n                        e.g. qsub. Snakemake compiles jobs\
      \ into scripts that\n                        are submitted to the cluster with\
      \ the given command,\n                        once all input files for a particular\
      \ job are present.\n                        The submit command can be decorated\
      \ to make it aware\n                        of certain job properties (input,\
      \ output, params,\n                        wildcards, log, threads and dependencies\
      \ (see the\n                        argument below)), e.g.: $ snakemake --cluster\
      \ 'qsub\n                        -pe threaded {threads}'.\n  --cluster-sync\
      \ CMD    cluster submission command will block, returning the\n            \
      \            remote exitstatus upon remote termination (for\n              \
      \          example, this should be usedif the cluster command is\n         \
      \               'qsub -sync y' (SGE)\n  --drmaa [ARGS]        Execute snakemake\
      \ on a cluster accessed via DRMAA,\n                        Snakemake compiles\
      \ jobs into scripts that are\n                        submitted to the cluster\
      \ with the given command, once\n                        all input files for\
      \ a particular job are present. ARGS\n                        can be used to\
      \ specify options of the underlying\n                        cluster system,\
      \ thereby using the job properties\n                        input, output, params,\
      \ wildcards, log, threads and\n                        dependencies, e.g.: --drmaa\
      \ ' -pe threaded {threads}'.\n                        Note that ARGS must be\
      \ given in quotes and with a\n                        leading whitespace.\n\
      \  --drmaa-log-dir DIR   Specify a directory in which stdout and stderr files\n\
      \                        of DRMAA jobs will be written. The value may be given\n\
      \                        as a relative path, in which case Snakemake will use\n\
      \                        the current invocation directory as the origin. If\n\
      \                        given, this will override any given '-o' and/or '-e'\n\
      \                        native specification. If not given, all DRMAA stdout\n\
      \                        and stderr files are written to the current working\n\
      \                        directory.\n  --cluster-config FILE, -u FILE\n    \
      \                    A JSON or YAML file that defines the wildcards used in\n\
      \                        'cluster'for specific rules, instead of having them\n\
      \                        specified in the Snakefile. For example, for rule\n\
      \                        'job' you may define: { 'job' : { 'time' : '24:00:00'\n\
      \                        } } to specify the time for rule 'job'. You can\n \
      \                       specify more than one file. The configuration files\n\
      \                        are merged with later values overriding earlier ones.\n\
      \  --immediate-submit, --is\n                        Immediately submit all\
      \ jobs to the cluster instead of\n                        waiting for present\
      \ input files. This will fail,\n                        unless you make the\
      \ cluster aware of job dependencies,\n                        e.g. via: $ snakemake\
      \ --cluster 'sbatch --dependency\n                        {dependencies}. Assuming\
      \ that your submit script (here\n                        sbatch) outputs the\
      \ generated job id to the first\n                        stdout line, {dependencies}\
      \ will be filled with space\n                        separated job ids this\
      \ job depends on.\n  --jobscript SCRIPT, --js SCRIPT\n                     \
      \   Provide a custom job script for submission to the\n                    \
      \    cluster. The default script resides as 'jobscript.sh'\n               \
      \         in the installation directory.\n  --jobname NAME, --jn NAME\n    \
      \                    Provide a custom name for the jobscript that is\n     \
      \                   submitted to the cluster (see --cluster). NAME is\n    \
      \                    \"snakejob.{rulename}.{jobid}.sh\" per default. The\n \
      \                       wildcard {jobid} has to be present in the name.\n  --cluster-status\
      \ CLUSTER_STATUS\n                        Status command for cluster execution.\
      \ This is only\n                        considered in combination with the --cluster\
      \ flag. If\n                        provided, Snakemake will use the status\
      \ command to\n                        determine if a job has finished successfully\
      \ or\n                        failed. For this it is necessary that the submit\n\
      \                        command provided to --cluster returns the cluster job\n\
      \                        id. Then, the status command will be invoked with the\n\
      \                        job id. Snakemake expects it to return 'success' if\n\
      \                        the job was successfull, 'failed' if the job failed\n\
      \                        and 'running' if the job still runs.\n  --kubernetes\
      \ [NAMESPACE]\n                        Execute workflow in a kubernetes cluster\
      \ (in the\n                        cloud). NAMESPACE is the namespace you want\
      \ to use for\n                        your job (if nothing specified: 'default').\
      \ Usually,\n                        this requires --default-remote-provider\
      \ and --default-\n                        remote-prefix to be set to a S3 or\
      \ GS bucket where\n                        your . data shall be stored. It is\
      \ further advisable\n                        to activate conda integration via\
      \ --use-conda.\n  --kubernetes-env ENVVAR [ENVVAR ...]\n                   \
      \     Specify environment variables to pass to the\n                       \
      \ kubernetes job.\n  --container-image IMAGE\n                        Docker\
      \ image to use, e.g., when submitting jobs to\n                        kubernetes.\
      \ By default, this is\n                        'quay.io/snakemake/snakemake',\
      \ tagged with the same\n                        version as the currently running\
      \ Snakemake instance.\n                        Note that overwriting this value\
      \ is up to your\n                        responsibility. Any used image has\
      \ to contain a\n                        working snakemake installation that\
      \ is compatible with\n                        (or ideally the same as) the currently\
      \ running\n                        version.\n  --reason, -r          Print the\
      \ reason for each executed rule.\n  --stats FILE          Write stats about\
      \ Snakefile execution in JSON format\n                        to the given file.\n\
      \  --nocolor             Do not use a colored output.\n  --quiet, -q       \
      \    Do not output any progress or rule information.\n  --nolock           \
      \   Do not lock the working directory\n  --unlock              Remove a lock\
      \ on the working directory.\n  --cleanup-metadata FILE [FILE ...], --cm FILE\
      \ [FILE ...]\n                        Cleanup the metadata of given files. That\
      \ means that\n                        snakemake removes any tracked version\
      \ info, and any\n                        marks that files are incomplete.\n\
      \  --rerun-incomplete, --ri\n                        Re-run all jobs the output\
      \ of which is recognized as\n                        incomplete.\n  --ignore-incomplete,\
      \ --ii\n                        Do not check for incomplete output files.\n\
      \  --list-version-changes, --lv\n                        List all output files\
      \ that have been created with a\n                        different version (as\
      \ determined by the version\n                        keyword).\n  --list-code-changes,\
      \ --lc\n                        List all output files for which the rule body\
      \ (run or\n                        shell) have changed in the Snakefile.\n \
      \ --list-input-changes, --li\n                        List all output files\
      \ for which the defined input\n                        files have changed in\
      \ the Snakefile (e.g. new input\n                        files were added in\
      \ the rule definition or files were\n                        renamed). For listing\
      \ input file modification in the\n                        filesystem, use --summary.\n\
      \  --list-params-changes, --lp\n                        List all output files\
      \ for which the defined params\n                        have changed in the\
      \ Snakefile.\n  --latency-wait SECONDS, --output-wait SECONDS, -w SECONDS\n\
      \                        Wait given seconds if an output file of a job is not\n\
      \                        present after the job finished. This helps if your\n\
      \                        filesystem suffers from latency (default 5).\n  --wait-for-files\
      \ [FILE [FILE ...]]\n                        Wait --latency-wait seconds for\
      \ these files to be\n                        present before executing the workflow.\
      \ This option is\n                        used internally to handle filesystem\
      \ latency in\n                        cluster environments.\n  --benchmark-repeats\
      \ N\n                        Repeat a job N times if marked for benchmarking\n\
      \                        (default 1).\n  --notemp, --nt        Ignore temp()\
      \ declarations. This is useful when\n                        running only a\
      \ part of the workflow, since temp()\n                        would lead to\
      \ deletion of probably needed files by\n                        other parts\
      \ of the workflow.\n  --keep-remote         Keep local copies of remote input\
      \ files.\n  --keep-target-files   Do not adjust the paths of given target files\
      \ relative\n                        to the working directory.\n  --keep-shadow\
      \         Do not delete the shadow directory on snakemake\n                \
      \        startup.\n  --allowed-rules ALLOWED_RULES [ALLOWED_RULES ...]\n   \
      \                     Only consider given rules. If omitted, all rules in\n\
      \                        Snakefile are used. Note that this is intended\n  \
      \                      primarily for internal use and may lead to unexpected\n\
      \                        results otherwise.\n  --max-jobs-per-second MAX_JOBS_PER_SECOND\n\
      \                        Maximal number of cluster/drmaa jobs per second,\n\
      \                        default is 10, fractions allowed.\n  --max-status-checks-per-second\
      \ MAX_STATUS_CHECKS_PER_SECOND\n                        Maximal number of job\
      \ status checks per second,\n                        default is 10, fractions\
      \ allowed.\n  --restart-times RESTART_TIMES\n                        Number\
      \ of times to restart failing jobs (defaults to\n                        0).\n\
      \  --attempt ATTEMPT     Internal use only: define the initial value of the\n\
      \                        attempt parameter (default: 1).\n  --timestamp, -T\
      \       Add a timestamp to all logging output\n  --greediness GREEDINESS\n \
      \                       Set the greediness of scheduling. This value between\
      \ 0\n                        and 1 determines how careful jobs are selected\
      \ for\n                        execution. The default value (1.0) provides the\
      \ best\n                        speed and still acceptable scheduling quality.\n\
      \  --no-hooks            Do not invoke onstart, onsuccess or onerror hooks\n\
      \                        after execution.\n  --print-compilation   Print the\
      \ python representation of the workflow.\n  --overwrite-shellcmd OVERWRITE_SHELLCMD\n\
      \                        Provide a shell command that shall be executed instead\n\
      \                        of those given in the workflow. This is for debugging\n\
      \                        purposes only.\n  --verbose             Print debugging\
      \ output.\n  --debug               Allow to debug rules with e.g. PDB. This\
      \ flag allows\n                        to set breakpoints in run blocks.\n \
      \ --runtime-profile FILE\n                        Profile Snakemake and write\
      \ the output to FILE. This\n                        requires yappi to be installed.\n\
      \  --mode {0,1,2}        Set execution mode of Snakemake (internal use only).\n\
      \  --bash-completion     Output code to register bash completion for snakemake.\n\
      \                        Put the following in your .bashrc (including the\n\
      \                        accents): `snakemake --bash-completion` or issue it\
      \ in\n                        an open terminal session.\n  --use-conda     \
      \      If defined in the rule, run job in a conda\n                        environment.\
      \ If this flag is not set, the conda\n                        directive is ignored.\n\
      \  --conda-prefix DIR    Specify a directory in which the 'conda' and 'conda-\n\
      \                        archive' directories are created. These are used to\n\
      \                        store conda environments and their archives,\n    \
      \                    respectively. If not supplied, the value is set to the\n\
      \                        '.snakemake' directory relative to the invocation\n\
      \                        directory. If supplied, the `--use-conda` flag must\n\
      \                        also be set. The value may be given as a relative\n\
      \                        path, which will be extrapolated to the invocation\n\
      \                        directory, or as an absolute path.\n  --create-envs-only\
      \    If specified, only creates the job-specific conda\n                   \
      \     environments then exits. The `--use-conda` flag must\n               \
      \         also be set.\n  --use-singularity     If defined in the rule, run\
      \ job within a singularity\n                        container. If this flag\
      \ is not set, the singularity\n                        directive is ignored.\n\
      \  --singularity-prefix DIR\n                        Specify a directory in\
      \ which singularity images will\n                        be stored.If not supplied,\
      \ the value is set to the\n                        '.snakemake' directory relative\
      \ to the invocation\n                        directory. If supplied, the `--use-singularity`\
      \ flag\n                        must also be set. The value may be given as\
      \ a relative\n                        path, which will be extrapolated to the\
      \ invocation\n                        directory, or as an absolute path.\n \
      \ --singularity-args ARGS\n                        Pass additional args to singularity.\n\
      \  --wrapper-prefix WRAPPER_PREFIX\n                        Prefix for URL created\
      \ from wrapper directive\n                        (default: https://bitbucket.org/snakemake/snakemake-\n\
      \                        wrappers/raw/). Set this to a different URL to use\n\
      \                        your fork or a local clone of the repository.\n  --default-remote-provider\
      \ {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp}\n                        Specify default\
      \ remote provider to be used for all\n                        input and output\
      \ files that don't yet specify one.\n  --default-remote-prefix DEFAULT_REMOTE_PREFIX\n\
      \                        Specify prefix for default remote provider. E.g. a\n\
      \                        bucket name.\n  --no-shared-fs        Do not assume\
      \ that jobs share a common file system.\n                        When this flag\
      \ is activated, Snakemake will assume\n                        that the filesystem\
      \ on a cluster node is not shared\n                        with other nodes.\
      \ For example, this will lead to\n                        downloading remote\
      \ files on each cluster node\n                        separately. Further, it\
      \ won't take special measures to\n                        deal with filesystem\
      \ latency issues. This option will\n                        in most cases only\
      \ make sense in combination with\n                        --default-remote-provider.\
      \ Further, when using\n                        --cluster you will have to also\
      \ provide --cluster-\n                        status. Only activate this if\
      \ you know what you are\n                        doing.\n  --version, -v   \
      \      show program's version number and exit\n"
    generated_using: *id004
  subcommands: []
  usage: []
  help_flag: !Flag
    description: show Snakemake help (or snakemake -h)
    synonyms:
    - --help
    args: !EmptyFlagArg {}
    optional: true
  usage_flag:
  version_flag:
  help_text: "MetaMeta Pipeline (powered by Snakemake)\n\nUsage: /usr/local/bin/metameta\
    \ --configfile FILE [Snakemake options]\n\n Useful Snakemake parameters:\n   --use-conda\
    \            use conda to automatically install pre-configured packages\n   -j,\
    \ --cores            number of cores\n   -k, --keep-going       go on with independent\
    \ jobs if a job fails\n   -n, --dryrun           do not execute anything\n   -p,\
    \ --printshellcmds   print out the shell commands that will be executed\n   -t,\
    \ --timestamp  \t\tadd a timestamp to all logging output\n\n Full list of parameters:\n\
    \   --help                 show Snakemake help (or snakemake -h)\n\n"
  generated_using: *id004
- !Command
  command: *id005
  positional: []
  named:
  - !Flag
    description: use conda to automatically install pre-configured packages
    synonyms:
    - --use-conda
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: number of cores
    synonyms:
    - -j
    - --cores
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: go on with independent jobs if a job fails
    synonyms:
    - -k
    - --keep-going
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: do not execute anything
    synonyms:
    - -n
    - --dryrun
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: print out the shell commands that will be executed
    synonyms:
    - -p
    - --printshellcmds
    args: !EmptyFlagArg {}
    optional: true
  - !Flag
    description: add a timestamp to all logging output
    synonyms:
    - -t
    - --timestamp
    args: !EmptyFlagArg {}
    optional: true
  parent: *id007
  subcommands: []
  usage: []
  help_flag: !Flag
    description: show Snakemake help (or snakemake -h)
    synonyms:
    - --help
    args: !EmptyFlagArg {}
    optional: true
  usage_flag:
  version_flag:
  help_text: "MetaMeta Pipeline (powered by Snakemake)\n\nUsage: /usr/local/bin/metameta\
    \ --configfile FILE [Snakemake options]\n\n Useful Snakemake parameters:\n   --use-conda\
    \            use conda to automatically install pre-configured packages\n   -j,\
    \ --cores            number of cores\n   -k, --keep-going       go on with independent\
    \ jobs if a job fails\n   -n, --dryrun           do not execute anything\n   -p,\
    \ --printshellcmds   print out the shell commands that will be executed\n   -t,\
    \ --timestamp  \t\tadd a timestamp to all logging output\n\n Full list of parameters:\n\
    \   --help                 show Snakemake help (or snakemake -h)\n\n"
  generated_using: *id004
usage: []
help_flag:
usage_flag:
version_flag:
help_text: "usage: snakemake [-h] [--profile PROFILE] [--snakefile FILE] [--gui [PORT]]\n\
  \                 [--cores [N]] [--local-cores N]\n                 [--resources\
  \ [NAME=INT [NAME=INT ...]]]\n                 [--config [KEY=VALUE [KEY=VALUE ...]]]\
  \ [--configfile FILE]\n                 [--list] [--list-target-rules] [--directory\
  \ DIR] [--dryrun]\n                 [--printshellcmds] [--debug-dag] [--dag]\n \
  \                [--force-use-threads] [--rulegraph] [--d3dag] [--summary]\n   \
  \              [--detailed-summary] [--archive FILE] [--touch]\n               \
  \  [--keep-going] [--force] [--forceall]\n                 [--forcerun [TARGET [TARGET\
  \ ...]]]\n                 [--prioritize TARGET [TARGET ...]]\n                \
  \ [--until TARGET [TARGET ...]]\n                 [--omit-from TARGET [TARGET ...]]\
  \ [--allow-ambiguity]\n                 [--cluster CMD | --cluster-sync CMD | --drmaa\
  \ [ARGS]]\n                 [--drmaa-log-dir DIR] [--cluster-config FILE]\n    \
  \             [--immediate-submit] [--jobscript SCRIPT] [--jobname NAME]\n     \
  \            [--cluster-status CLUSTER_STATUS] [--kubernetes [NAMESPACE]]\n    \
  \             [--kubernetes-env ENVVAR [ENVVAR ...]]\n                 [--container-image\
  \ IMAGE] [--reason] [--stats FILE]\n                 [--nocolor] [--quiet] [--nolock]\
  \ [--unlock]\n                 [--cleanup-metadata FILE [FILE ...]] [--rerun-incomplete]\n\
  \                 [--ignore-incomplete] [--list-version-changes]\n             \
  \    [--list-code-changes] [--list-input-changes]\n                 [--list-params-changes]\
  \ [--latency-wait SECONDS]\n                 [--wait-for-files [FILE [FILE ...]]]\
  \ [--benchmark-repeats N]\n                 [--notemp] [--keep-remote] [--keep-target-files]\n\
  \                 [--keep-shadow]\n                 [--allowed-rules ALLOWED_RULES\
  \ [ALLOWED_RULES ...]]\n                 [--max-jobs-per-second MAX_JOBS_PER_SECOND]\n\
  \                 [--max-status-checks-per-second MAX_STATUS_CHECKS_PER_SECOND]\n\
  \                 [--restart-times RESTART_TIMES] [--attempt ATTEMPT]\n        \
  \         [--timestamp] [--greediness GREEDINESS] [--no-hooks]\n               \
  \  [--print-compilation]\n                 [--overwrite-shellcmd OVERWRITE_SHELLCMD]\
  \ [--verbose]\n                 [--debug] [--runtime-profile FILE] [--mode {0,1,2}]\n\
  \                 [--bash-completion] [--use-conda] [--conda-prefix DIR]\n     \
  \            [--create-envs-only] [--use-singularity]\n                 [--singularity-prefix\
  \ DIR] [--singularity-args ARGS]\n                 [--wrapper-prefix WRAPPER_PREFIX]\n\
  \                 [--default-remote-provider {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp}]\n\
  \                 [--default-remote-prefix DEFAULT_REMOTE_PREFIX]\n            \
  \     [--no-shared-fs] [--version]\n                 [target [target ...]]\n\nSnakemake\
  \ is a Python based language and execution environment for GNU Make-\nlike workflows.\n\
  \npositional arguments:\n  target                Targets to build. May be rules\
  \ or files.\n\noptional arguments:\n  -h, --help            show this help message\
  \ and exit\n  --profile PROFILE     Name of profile to use for configuring Snakemake.\n\
  \                        Snakemake will search for a corresponding folder in\n \
  \                       /etc/xdg/snakemake and /root/.config/snakemake.\n      \
  \                  Alternatively, this can be an absolute or relative\n        \
  \                path. The profile folder has to contain a file\n              \
  \          'config.yaml'. This file can be used to set default\n               \
  \         values for command line options in YAML format. For\n                \
  \        example, '--cluster qsub' becomes 'cluster: qsub' in\n                \
  \        the YAML file. Profiles can be obtained from\n                        https://github.com/snakemake-profiles.\n\
  \  --snakefile FILE, -s FILE\n                        The workflow definition in\
  \ a snakefile.\n  --gui [PORT]          Serve an HTML based user interface to the\
  \ given\n                        network and port e.g. 168.129.10.15:8000. By default\n\
  \                        Snakemake is only available in the local network\n    \
  \                    (default port: 8000). To make Snakemake listen to all\n   \
  \                     ip addresses add the special host address 0.0.0.0 to\n   \
  \                     the url (0.0.0.0:8000). This is important if Snakemake\n \
  \                       is used in a virtualised environment like Docker. If\n \
  \                       possible, a browser window is opened.\n  --cores [N], --jobs\
  \ [N], -j [N]\n                        Use at most N cores in parallel (default:\
  \ 1). If N is\n                        omitted, the limit is set to the number of\
  \ available\n                        cores.\n  --local-cores N       In cluster\
  \ mode, use at most N cores of the host\n                        machine in parallel\
  \ (default: number of CPU cores of\n                        the host). The cores\
  \ are used to execute local rules.\n                        This option is ignored\
  \ when not in cluster mode.\n  --resources [NAME=INT [NAME=INT ...]], --res [NAME=INT\
  \ [NAME=INT ...]]\n                        Define additional resources that shall\
  \ constrain the\n                        scheduling analogously to threads (see\
  \ above). A\n                        resource is defined as a name and an integer\
  \ value.\n                        E.g. --resources gpu=1. Rules can use resources\
  \ by\n                        defining the resource keyword, e.g. resources: gpu=1.\n\
  \                        If now two rules require 1 of the resource 'gpu' they\n\
  \                        won't be run in parallel by the scheduler.\n  --config\
  \ [KEY=VALUE [KEY=VALUE ...]], -C [KEY=VALUE [KEY=VALUE ...]]\n                \
  \        Set or overwrite values in the workflow config object.\n              \
  \          The workflow config object is accessible as variable\n              \
  \          config inside the workflow. Default values can be set\n             \
  \           by providing a JSON file (see Documentation).\n  --configfile FILE \
  \    Specify or overwrite the config file of the workflow\n                    \
  \    (see the docs). Values specified in JSON or YAML\n                        format\
  \ are available in the global config dictionary\n                        inside\
  \ the workflow.\n  --list, -l            Show availiable rules in given Snakefile.\n\
  \  --list-target-rules, --lt\n                        Show available target rules\
  \ in given Snakefile.\n  --directory DIR, -d DIR\n                        Specify\
  \ working directory (relative paths in the\n                        snakefile will\
  \ use this as their origin).\n  --dryrun, -n          Do not execute anything.\n\
  \  --printshellcmds, -p  Print out the shell commands that will be executed.\n \
  \ --debug-dag           Print candidate and selected jobs (including their\n   \
  \                     wildcards) while inferring DAG. This can help to debug\n \
  \                       unexpected DAG topology or errors.\n  --dag            \
  \     Do not execute anything and print the directed acyclic\n                 \
  \       graph of jobs in the dot language. Recommended use on\n                \
  \        Unix systems: snakemake --dag | dot | display\n  --force-use-threads  \
  \ Force threads rather than processes. Helpful if shared\n                     \
  \   memory (/dev/shm) is full or unavailable.\n  --rulegraph           Do not execute\
  \ anything and print the dependency graph\n                        of rules in the\
  \ dot language. This will be less\n                        crowded than above DAG\
  \ of jobs, but also show less\n                        information. Note that each\
  \ rule is displayed once,\n                        hence the displayed graph will\
  \ be cyclic if a rule\n                        appears in several steps of the workflow.\
  \ Use this if\n                        above option leads to a DAG that is too large.\n\
  \                        Recommended use on Unix systems: snakemake --rulegraph\n\
  \                        | dot | display\n  --d3dag               Print the DAG\
  \ in D3.js compatible JSON format.\n  --summary, -S         Print a summary of all\
  \ files created by the workflow.\n                        The has the following\
  \ columns: filename, modification\n                        time, rule version, status,\
  \ plan. Thereby rule version\n                        contains the versionthe file\
  \ was created with (see the\n                        version keyword of rules),\
  \ and status denotes whether\n                        the file is missing, its input\
  \ files are newer or if\n                        version or implementation of the\
  \ rule changed since\n                        file creation. Finally the last column\
  \ denotes whether\n                        the file will be updated or created during\
  \ the next\n                        workflow execution.\n  --detailed-summary, -D\n\
  \                        Print a summary of all files created by the workflow.\n\
  \                        The has the following columns: filename, modification\n\
  \                        time, rule version, input file(s), shell command,\n   \
  \                     status, plan. Thereby rule version contains the\n        \
  \                versionthe file was created with (see the version\n           \
  \             keyword of rules), and status denotes whether the file\n         \
  \               is missing, its input files are newer or if version or\n       \
  \                 implementation of the rule changed since file\n              \
  \          creation. The input file and shell command columns are\n            \
  \            selfexplanatory. Finally the last column denotes\n                \
  \        whether the file will be updated or created during the\n              \
  \          next workflow execution.\n  --archive FILE        Archive the workflow\
  \ into the given tar archive FILE.\n                        The archive will be\
  \ created such that the workflow can\n                        be re-executed on\
  \ a vanilla system. The function needs\n                        conda and git to\
  \ be installed. It will archive every\n                        file that is under\
  \ git version control. Note that it\n                        is best practice to\
  \ have the Snakefile, config files,\n                        and scripts under version\
  \ control. Hence, they will be\n                        included in the archive.\
  \ Further, it will add input\n                        files that are not generated\
  \ by by the workflow itself\n                        and conda environments. Note\
  \ that symlinks are\n                        dereferenced. Supported formats are\
  \ .tar, .tar.gz,\n                        .tar.bz2 and .tar.xz.\n  --touch, -t \
  \          Touch output files (mark them up to date without\n                  \
  \      really changing them) instead of running their\n                        commands.\
  \ This is used to pretend that the rules were\n                        executed,\
  \ in order to fool future invocations of\n                        snakemake. Fails\
  \ if a file does not yet exist.\n  --keep-going, -k      Go on with independent\
  \ jobs if a job fails.\n  --force, -f           Force the execution of the selected\
  \ target or the\n                        first rule regardless of already created\
  \ output.\n  --forceall, -F        Force the execution of the selected (or the first)\n\
  \                        rule and all rules it is dependent on regardless of\n \
  \                       already created output.\n  --forcerun [TARGET [TARGET ...]],\
  \ -R [TARGET [TARGET ...]]\n                        Force the re-execution or creation\
  \ of the given rules\n                        or files. Use this option if you changed\
  \ a rule and\n                        want to have all its output in your workflow\
  \ updated.\n  --prioritize TARGET [TARGET ...], -P TARGET [TARGET ...]\n       \
  \                 Tell the scheduler to assign creation of given targets\n     \
  \                   (and all their dependencies) highest priority.\n           \
  \             (EXPERIMENTAL)\n  --until TARGET [TARGET ...], -U TARGET [TARGET ...]\n\
  \                        Runs the pipeline until it reaches the specified rules\n\
  \                        or files. Only runs jobs that are dependencies of the\n\
  \                        specified rule or files, does not run sibling DAGs.\n \
  \ --omit-from TARGET [TARGET ...], -O TARGET [TARGET ...]\n                    \
  \    Prevent the execution or creation of the given rules\n                    \
  \    or files as well as any rules or files that are\n                        downstream\
  \ of these targets in the DAG. Also runs jobs\n                        in sibling\
  \ DAGs that are independent of the rules or\n                        files specified\
  \ here.\n  --allow-ambiguity, -a\n                        Don't check for ambiguous\
  \ rules and simply use the\n                        first if several can produce\
  \ the same file. This\n                        allows the user to prioritize rules\
  \ by their order in\n                        the snakefile.\n  --cluster CMD, -c\
  \ CMD\n                        Execute snakemake rules with the given submit command,\n\
  \                        e.g. qsub. Snakemake compiles jobs into scripts that\n\
  \                        are submitted to the cluster with the given command,\n\
  \                        once all input files for a particular job are present.\n\
  \                        The submit command can be decorated to make it aware\n\
  \                        of certain job properties (input, output, params,\n   \
  \                     wildcards, log, threads and dependencies (see the\n      \
  \                  argument below)), e.g.: $ snakemake --cluster 'qsub\n       \
  \                 -pe threaded {threads}'.\n  --cluster-sync CMD    cluster submission\
  \ command will block, returning the\n                        remote exitstatus upon\
  \ remote termination (for\n                        example, this should be usedif\
  \ the cluster command is\n                        'qsub -sync y' (SGE)\n  --drmaa\
  \ [ARGS]        Execute snakemake on a cluster accessed via DRMAA,\n           \
  \             Snakemake compiles jobs into scripts that are\n                  \
  \      submitted to the cluster with the given command, once\n                 \
  \       all input files for a particular job are present. ARGS\n               \
  \         can be used to specify options of the underlying\n                   \
  \     cluster system, thereby using the job properties\n                       \
  \ input, output, params, wildcards, log, threads and\n                        dependencies,\
  \ e.g.: --drmaa ' -pe threaded {threads}'.\n                        Note that ARGS\
  \ must be given in quotes and with a\n                        leading whitespace.\n\
  \  --drmaa-log-dir DIR   Specify a directory in which stdout and stderr files\n\
  \                        of DRMAA jobs will be written. The value may be given\n\
  \                        as a relative path, in which case Snakemake will use\n\
  \                        the current invocation directory as the origin. If\n  \
  \                      given, this will override any given '-o' and/or '-e'\n  \
  \                      native specification. If not given, all DRMAA stdout\n  \
  \                      and stderr files are written to the current working\n   \
  \                     directory.\n  --cluster-config FILE, -u FILE\n           \
  \             A JSON or YAML file that defines the wildcards used in\n         \
  \               'cluster'for specific rules, instead of having them\n          \
  \              specified in the Snakefile. For example, for rule\n             \
  \           'job' you may define: { 'job' : { 'time' : '24:00:00'\n            \
  \            } } to specify the time for rule 'job'. You can\n                 \
  \       specify more than one file. The configuration files\n                  \
  \      are merged with later values overriding earlier ones.\n  --immediate-submit,\
  \ --is\n                        Immediately submit all jobs to the cluster instead\
  \ of\n                        waiting for present input files. This will fail,\n\
  \                        unless you make the cluster aware of job dependencies,\n\
  \                        e.g. via: $ snakemake --cluster 'sbatch --dependency\n\
  \                        {dependencies}. Assuming that your submit script (here\n\
  \                        sbatch) outputs the generated job id to the first\n   \
  \                     stdout line, {dependencies} will be filled with space\n  \
  \                      separated job ids this job depends on.\n  --jobscript SCRIPT,\
  \ --js SCRIPT\n                        Provide a custom job script for submission\
  \ to the\n                        cluster. The default script resides as 'jobscript.sh'\n\
  \                        in the installation directory.\n  --jobname NAME, --jn\
  \ NAME\n                        Provide a custom name for the jobscript that is\n\
  \                        submitted to the cluster (see --cluster). NAME is\n   \
  \                     \"snakejob.{rulename}.{jobid}.sh\" per default. The\n    \
  \                    wildcard {jobid} has to be present in the name.\n  --cluster-status\
  \ CLUSTER_STATUS\n                        Status command for cluster execution.\
  \ This is only\n                        considered in combination with the --cluster\
  \ flag. If\n                        provided, Snakemake will use the status command\
  \ to\n                        determine if a job has finished successfully or\n\
  \                        failed. For this it is necessary that the submit\n    \
  \                    command provided to --cluster returns the cluster job\n   \
  \                     id. Then, the status command will be invoked with the\n  \
  \                      job id. Snakemake expects it to return 'success' if\n   \
  \                     the job was successfull, 'failed' if the job failed\n    \
  \                    and 'running' if the job still runs.\n  --kubernetes [NAMESPACE]\n\
  \                        Execute workflow in a kubernetes cluster (in the\n    \
  \                    cloud). NAMESPACE is the namespace you want to use for\n  \
  \                      your job (if nothing specified: 'default'). Usually,\n  \
  \                      this requires --default-remote-provider and --default-\n\
  \                        remote-prefix to be set to a S3 or GS bucket where\n  \
  \                      your . data shall be stored. It is further advisable\n  \
  \                      to activate conda integration via --use-conda.\n  --kubernetes-env\
  \ ENVVAR [ENVVAR ...]\n                        Specify environment variables to\
  \ pass to the\n                        kubernetes job.\n  --container-image IMAGE\n\
  \                        Docker image to use, e.g., when submitting jobs to\n  \
  \                      kubernetes. By default, this is\n                       \
  \ 'quay.io/snakemake/snakemake', tagged with the same\n                        version\
  \ as the currently running Snakemake instance.\n                        Note that\
  \ overwriting this value is up to your\n                        responsibility.\
  \ Any used image has to contain a\n                        working snakemake installation\
  \ that is compatible with\n                        (or ideally the same as) the\
  \ currently running\n                        version.\n  --reason, -r          Print\
  \ the reason for each executed rule.\n  --stats FILE          Write stats about\
  \ Snakefile execution in JSON format\n                        to the given file.\n\
  \  --nocolor             Do not use a colored output.\n  --quiet, -q           Do\
  \ not output any progress or rule information.\n  --nolock              Do not lock\
  \ the working directory\n  --unlock              Remove a lock on the working directory.\n\
  \  --cleanup-metadata FILE [FILE ...], --cm FILE [FILE ...]\n                  \
  \      Cleanup the metadata of given files. That means that\n                  \
  \      snakemake removes any tracked version info, and any\n                   \
  \     marks that files are incomplete.\n  --rerun-incomplete, --ri\n           \
  \             Re-run all jobs the output of which is recognized as\n           \
  \             incomplete.\n  --ignore-incomplete, --ii\n                       \
  \ Do not check for incomplete output files.\n  --list-version-changes, --lv\n  \
  \                      List all output files that have been created with a\n   \
  \                     different version (as determined by the version\n        \
  \                keyword).\n  --list-code-changes, --lc\n                      \
  \  List all output files for which the rule body (run or\n                     \
  \   shell) have changed in the Snakefile.\n  --list-input-changes, --li\n      \
  \                  List all output files for which the defined input\n         \
  \               files have changed in the Snakefile (e.g. new input\n          \
  \              files were added in the rule definition or files were\n         \
  \               renamed). For listing input file modification in the\n         \
  \               filesystem, use --summary.\n  --list-params-changes, --lp\n    \
  \                    List all output files for which the defined params\n      \
  \                  have changed in the Snakefile.\n  --latency-wait SECONDS, --output-wait\
  \ SECONDS, -w SECONDS\n                        Wait given seconds if an output file\
  \ of a job is not\n                        present after the job finished. This\
  \ helps if your\n                        filesystem suffers from latency (default\
  \ 5).\n  --wait-for-files [FILE [FILE ...]]\n                        Wait --latency-wait\
  \ seconds for these files to be\n                        present before executing\
  \ the workflow. This option is\n                        used internally to handle\
  \ filesystem latency in\n                        cluster environments.\n  --benchmark-repeats\
  \ N\n                        Repeat a job N times if marked for benchmarking\n \
  \                       (default 1).\n  --notemp, --nt        Ignore temp() declarations.\
  \ This is useful when\n                        running only a part of the workflow,\
  \ since temp()\n                        would lead to deletion of probably needed\
  \ files by\n                        other parts of the workflow.\n  --keep-remote\
  \         Keep local copies of remote input files.\n  --keep-target-files   Do not\
  \ adjust the paths of given target files relative\n                        to the\
  \ working directory.\n  --keep-shadow         Do not delete the shadow directory\
  \ on snakemake\n                        startup.\n  --allowed-rules ALLOWED_RULES\
  \ [ALLOWED_RULES ...]\n                        Only consider given rules. If omitted,\
  \ all rules in\n                        Snakefile are used. Note that this is intended\n\
  \                        primarily for internal use and may lead to unexpected\n\
  \                        results otherwise.\n  --max-jobs-per-second MAX_JOBS_PER_SECOND\n\
  \                        Maximal number of cluster/drmaa jobs per second,\n    \
  \                    default is 10, fractions allowed.\n  --max-status-checks-per-second\
  \ MAX_STATUS_CHECKS_PER_SECOND\n                        Maximal number of job status\
  \ checks per second,\n                        default is 10, fractions allowed.\n\
  \  --restart-times RESTART_TIMES\n                        Number of times to restart\
  \ failing jobs (defaults to\n                        0).\n  --attempt ATTEMPT  \
  \   Internal use only: define the initial value of the\n                       \
  \ attempt parameter (default: 1).\n  --timestamp, -T       Add a timestamp to all\
  \ logging output\n  --greediness GREEDINESS\n                        Set the greediness\
  \ of scheduling. This value between 0\n                        and 1 determines\
  \ how careful jobs are selected for\n                        execution. The default\
  \ value (1.0) provides the best\n                        speed and still acceptable\
  \ scheduling quality.\n  --no-hooks            Do not invoke onstart, onsuccess\
  \ or onerror hooks\n                        after execution.\n  --print-compilation\
  \   Print the python representation of the workflow.\n  --overwrite-shellcmd OVERWRITE_SHELLCMD\n\
  \                        Provide a shell command that shall be executed instead\n\
  \                        of those given in the workflow. This is for debugging\n\
  \                        purposes only.\n  --verbose             Print debugging\
  \ output.\n  --debug               Allow to debug rules with e.g. PDB. This flag\
  \ allows\n                        to set breakpoints in run blocks.\n  --runtime-profile\
  \ FILE\n                        Profile Snakemake and write the output to FILE.\
  \ This\n                        requires yappi to be installed.\n  --mode {0,1,2}\
  \        Set execution mode of Snakemake (internal use only).\n  --bash-completion\
  \     Output code to register bash completion for snakemake.\n                 \
  \       Put the following in your .bashrc (including the\n                     \
  \   accents): `snakemake --bash-completion` or issue it in\n                   \
  \     an open terminal session.\n  --use-conda           If defined in the rule,\
  \ run job in a conda\n                        environment. If this flag is not set,\
  \ the conda\n                        directive is ignored.\n  --conda-prefix DIR\
  \    Specify a directory in which the 'conda' and 'conda-\n                    \
  \    archive' directories are created. These are used to\n                     \
  \   store conda environments and their archives,\n                        respectively.\
  \ If not supplied, the value is set to the\n                        '.snakemake'\
  \ directory relative to the invocation\n                        directory. If supplied,\
  \ the `--use-conda` flag must\n                        also be set. The value may\
  \ be given as a relative\n                        path, which will be extrapolated\
  \ to the invocation\n                        directory, or as an absolute path.\n\
  \  --create-envs-only    If specified, only creates the job-specific conda\n   \
  \                     environments then exits. The `--use-conda` flag must\n   \
  \                     also be set.\n  --use-singularity     If defined in the rule,\
  \ run job within a singularity\n                        container. If this flag\
  \ is not set, the singularity\n                        directive is ignored.\n \
  \ --singularity-prefix DIR\n                        Specify a directory in which\
  \ singularity images will\n                        be stored.If not supplied, the\
  \ value is set to the\n                        '.snakemake' directory relative to\
  \ the invocation\n                        directory. If supplied, the `--use-singularity`\
  \ flag\n                        must also be set. The value may be given as a relative\n\
  \                        path, which will be extrapolated to the invocation\n  \
  \                      directory, or as an absolute path.\n  --singularity-args\
  \ ARGS\n                        Pass additional args to singularity.\n  --wrapper-prefix\
  \ WRAPPER_PREFIX\n                        Prefix for URL created from wrapper directive\n\
  \                        (default: https://bitbucket.org/snakemake/snakemake-\n\
  \                        wrappers/raw/). Set this to a different URL to use\n  \
  \                      your fork or a local clone of the repository.\n  --default-remote-provider\
  \ {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp}\n                        Specify default\
  \ remote provider to be used for all\n                        input and output files\
  \ that don't yet specify one.\n  --default-remote-prefix DEFAULT_REMOTE_PREFIX\n\
  \                        Specify prefix for default remote provider. E.g. a\n  \
  \                      bucket name.\n  --no-shared-fs        Do not assume that\
  \ jobs share a common file system.\n                        When this flag is activated,\
  \ Snakemake will assume\n                        that the filesystem on a cluster\
  \ node is not shared\n                        with other nodes. For example, this\
  \ will lead to\n                        downloading remote files on each cluster\
  \ node\n                        separately. Further, it won't take special measures\
  \ to\n                        deal with filesystem latency issues. This option will\n\
  \                        in most cases only make sense in combination with\n   \
  \                     --default-remote-provider. Further, when using\n         \
  \               --cluster you will have to also provide --cluster-\n           \
  \             status. Only activate this if you know what you are\n            \
  \            doing.\n  --version, -v         show program's version number and exit\n"
generated_using: *id004
